<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
<generator uri="http://jekyllrb.com" version="3.4.3">Jekyll</generator>
<link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" />
<link href="http://localhost:4000/" rel="alternate" type="text/html" />
<updated>2017-04-09T20:44:26+08:00</updated>
<id>http://localhost:4000/</id>
<subtitle>朱智博，朱智博的博客，zhuio,zhuio.github.io,</subtitle>
<entry>
<title>Scrapy爬虫入门教程十二 Link Extractors（链接提取器）</title>
<link href="http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E5%8D%81%E4%BA%8C-link-extractors-%E9%93%BE%E6%8E%A5%E6%8F%90%E5%8F%96%E5%99%A8/" rel="alternate" type="text/html" title="Scrapy爬虫入门教程十二 Link Extractors（链接提取器）" />
<published>2017-04-09T14:34:00+08:00</published>
<updated>2017-04-09T14:34:00+08:00</updated>
<id>http://localhost:4000/scrapy爬虫入门教程十二-link-extractors（链接提取器）</id>
<content type="html" xml:base="http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E5%8D%81%E4%BA%8C-link-extractors-%E9%93%BE%E6%8E%A5%E6%8F%90%E5%8F%96%E5%99%A8/">&lt;h1 id=&quot;scrapy-link-extractors&quot;&gt;Scrapy爬虫入门教程十二 Link Extractors（链接提取器）&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;开发环境：&lt;/strong&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;Python 3.6.0 版本&lt;/code&gt; （当前最新）
&lt;code class=&quot;highlighter-rouge&quot;&gt;Scrapy 1.3.2 版本&lt;/code&gt; （当前最新）&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;链接提取器&lt;/h1&gt;

&lt;p&gt;链接提取器是其唯一目的是从&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy.http.Response&lt;/code&gt;最终将跟随的网页（对象）提取链接的对象。&lt;/p&gt;

&lt;p&gt;有Scrapy，但你可以创建自己的自定义链接提取器，以满足您的需求通​​过实现一个简单的界面。&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy.linkextractors import LinkExtractor&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;每个链接提取器唯一的公共方法是&lt;code class=&quot;highlighter-rouge&quot;&gt;extract_links&lt;/code&gt;接收一个Response对象并返回一个&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy.link.Link&lt;/code&gt;对象列表。链接提取器意在被实例化一次，并且它们的&lt;code class=&quot;highlighter-rouge&quot;&gt;extract_links&lt;/code&gt;方法被调用几次，具有不同的响应以提取跟随的链接。&lt;/p&gt;

&lt;p&gt;链接提取程序&lt;code class=&quot;highlighter-rouge&quot;&gt;CrawlSpider&lt;/code&gt; 通过一组规则在类中使用（可以在Scrapy中使用），但是您也可以在爬虫中使用它，即使不从其中&lt;code class=&quot;highlighter-rouge&quot;&gt;CrawlSpider&lt;/code&gt;提取子类 ，因为其目的非常简单：提取链接。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;内置链接提取器参考&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy.linkextractors&lt;/code&gt;模块中提供了与Scrapy捆绑在一起的链接提取器类 。&lt;/p&gt;

&lt;p&gt;默认的链接提取器是&lt;code class=&quot;highlighter-rouge&quot;&gt;LinkExtractor&lt;/code&gt;，它是相同的 &lt;code class=&quot;highlighter-rouge&quot;&gt;LxmlLinkExtractor&lt;/code&gt;：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from scrapy.linkextractors import LinkExtractor
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;以前的Scrapy版本中曾经有过其他链接提取器类，但现在已经过时了。&lt;/p&gt;

&lt;h3 id=&quot;lxmllinkextractor&quot;&gt;LxmlLinkExtractor&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor(allow=(), deny=(), allow_domains=(), deny_domains=(), deny_extensions=None, restrict_xpaths=(), restrict_css=(), tags=('a', 'area'), attrs=('href', ), canonicalize=True, unique=True, process_value=None)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;LxmlLinkExtractor是推荐的链接提取器与方便的过滤选项。它使用lxml的强大的HTMLParser实现。&lt;/p&gt;

&lt;p&gt;**参数：    **&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;allow（正则表达式（或的列表）） - 一个单一的正则表达式（或正则表达式列表），（绝对）urls必须匹配才能提取。如果没有给出（或为空），它将匹配所有链接。&lt;/li&gt;
  &lt;li&gt;deny（正则表达式或正则表达式列表） - 一个正则表达式（或正则表达式列表），（绝对）urls必须匹配才能排除（即不提取）。它优先于allow参数。如果没有给出（或为空），它不会排除任何链接。&lt;/li&gt;
  &lt;li&gt;allow_domains（str或list） - 单个值或包含将被考虑用于提取链接的域的字符串列表&lt;/li&gt;
  &lt;li&gt;deny_domains（str或list） - 单个值或包含不会被考虑用于提取链接的域的字符串列表&lt;/li&gt;
  &lt;li&gt;deny_extensions（list） - 包含在提取链接时应该忽略的扩展的单个值或字符串列表。如果没有给出，它将默认为IGNORED_EXTENSIONS在&lt;a href=&quot;https://github.com/scrapy/scrapy/blob/master/scrapy/linkextractors/__init__.py&quot;&gt;scrapy.linkextractors&lt;/a&gt;包中定义的 列表 。&lt;/li&gt;
  &lt;li&gt;restrict_xpaths（str或list） - 是一个XPath（或XPath的列表），它定义响应中应从中提取链接的区域。如果给出，只有那些XPath选择的文本将被扫描链接。参见下面的例子。&lt;/li&gt;
  &lt;li&gt;restrict_css（str或list） - 一个CSS选择器（或选择器列表），用于定义响应中应提取链接的区域。有相同的行为restrict_xpaths。
标签（str或list） - 标签或在提取链接时要考虑的标签列表。默认为。(‘a’, ‘area’)&lt;/li&gt;
  &lt;li&gt;attrs（list） - 在查找要提取的链接时应该考虑的属性或属性列表（仅适用于参数中指定的那些标签tags ）。默认为(‘href’,)&lt;/li&gt;
  &lt;li&gt;canonicalize（boolean） - 规范化每个提取的url（使用w3lib.url.canonicalize_url）。默认为True。&lt;/li&gt;
  &lt;li&gt;unique（boolean） - 是否应对提取的链接应用重复过滤。&lt;/li&gt;
  &lt;li&gt;process_value（callable） -
接收从标签提取的每个值和扫描的属性并且可以修改值并返回新值的函数，或者返回None以完全忽略链接。如果没有给出，process_value默认为。lambda x: x&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;例如，要从此代码中提取链接：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;a href=&quot;javascript:goToPage('../other/page.html'); return false&quot;&amp;gt;Link text&amp;lt;/a&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;您可以使用以下功能process_value：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def process_value(value):
    m = re.search(&quot;javascript:goToPage\('(.*?)'&quot;, value)
    if m:
        return m.group(1)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</content>
<category term="Scrapy" />
<summary>Scrapy爬虫入门教程十二 Link Extractors（链接提取器）</summary>
</entry>
<entry>
<title>Scrapy爬虫入门教程十一 Request和Response（请求和响应）</title>
<link href="http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E5%8D%81%E4%B8%80-request%E5%92%8Cresponse-%E8%AF%B7%E6%B1%82%E5%92%8C%E5%93%8D%E5%BA%94/" rel="alternate" type="text/html" title="Scrapy爬虫入门教程十一 Request和Response（请求和响应）" />
<published>2017-04-09T14:33:00+08:00</published>
<updated>2017-04-09T14:33:00+08:00</updated>
<id>http://localhost:4000/scrapy爬虫入门教程十一-request和response（请求和响应）</id>
<content type="html" xml:base="http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E5%8D%81%E4%B8%80-request%E5%92%8Cresponse-%E8%AF%B7%E6%B1%82%E5%92%8C%E5%93%8D%E5%BA%94/">&lt;h1 id=&quot;scrapy-requestresponse&quot;&gt;Scrapy爬虫入门教程十一 Request和Response（请求和响应）&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;开发环境：&lt;/strong&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;Python 3.6.0 版本&lt;/code&gt; （当前最新）
&lt;code class=&quot;highlighter-rouge&quot;&gt;Scrapy 1.3.2 版本&lt;/code&gt; （当前最新）&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;请求和响应&lt;/h1&gt;

&lt;p&gt;Scrapy的Request和Response对象用于爬网网站。&lt;/p&gt;

&lt;p&gt;通常，Request对象在爬虫程序中生成并传递到系统，直到它们到达下载程序，后者执行请求并返回一个Response对象，该对象返回到发出请求的爬虫程序。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;上面一段话比较拗口，有web经验的同学，应该都了解的，不明白看下面的图大概理解下。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;爬虫-&amp;gt;Request:创建
Request-&amp;gt;Response:获取下载数据
Response-&amp;gt;爬虫:数据
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/2880699-f6847c1d6ebf140a.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;两个类Request和Response类都有一些子类，它们添加基类中不需要的功能。这些在下面的请求子类和 响应子类中描述。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;request-objects&quot;&gt;Request objects&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class scrapy.http.Request(url[, callback, method='GET', headers, body, cookies, meta, encoding='utf-8', priority=0, dont_filter=False, errback])
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;一个Request对象表示一个HTTP请求，它通常是在爬虫生成，并由下载执行，从而生成Response。&lt;/p&gt;

&lt;p&gt;-
参数：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;url（string）&lt;/code&gt; - 此请求的网址&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;callback（callable）&lt;/code&gt; - 将使用此请求的响应（一旦下载）作为其第一个参数调用的函数。有关更多信息，请参阅下面的&lt;a href=&quot;https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-response-ref-request-callback-arguments&quot;&gt;将附加数据传递给回调函数&lt;/a&gt;。如果请求没有指定回调，parse()将使用spider的 方法。请注意，如果在处理期间引发异常，则会调用errback。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;method（string）&lt;/code&gt; - 此请求的HTTP方法。默认为’GET’。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;meta（dict）&lt;/code&gt; - 属性的初始值Request.meta。如果给定，在此参数中传递的dict将被浅复制。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;body（str或unicode）&lt;/code&gt; - 请求体。如果unicode传递了a，那么它被编码为 str使用传递的编码（默认为utf-8）。如果 body没有给出，则存储一个空字符串。不管这个参数的类型，存储的最终值将是一个str（不会是unicode或None）。&lt;/li&gt;
  &lt;li&gt;
    &lt;h2 id=&quot;headersdict---dict-nonehttp&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;headers（dict）&lt;/code&gt; - 这个请求的头。dict值可以是字符串（对于单值标头）或列表（对于多值标头）。如果 None作为值传递，则不会发送HTTP头。&lt;/h2&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cookie（dict或list）&lt;/code&gt; - 请求cookie。这些可以以两种形式发送。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;使用dict：&lt;/p&gt;

    &lt;p&gt;request_with_cookies = Request(url=”http://www.example.com”,
                                 cookies={‘currency’: ‘USD’, ‘country’: ‘UY’})&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  * 使用列表：

  ```
  request_with_cookies = Request(url=&quot;http://www.example.com&quot;,
                                 cookies=[{'name': 'currency',
                                          'value': 'USD',
                                          'domain': 'example.com',
                                          'path': '/currency'}])
  ```
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;后一种形式允许定制 cookie的属性domain和path属性。这只有在保存Cookie用于以后的请求时才有用。&lt;/p&gt;

&lt;p&gt;当某些网站返回Cookie（在响应中）时，这些Cookie会存储在该域的Cookie中，并在将来的请求中再次发送。这是任何常规网络浏览器的典型行为。但是，如果由于某种原因，您想要避免与现有Cookie合并，您可以通过将dont_merge_cookies关键字设置为True 来指示Scrapy如此操作 Request.meta。&lt;/p&gt;

&lt;p&gt;不合并Cookie的请求示例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;request_with_cookies = Request(url=&quot;http://www.example.com&quot;,
                               cookies={'currency': 'USD', 'country': 'UY'},
                               meta={'dont_merge_cookies': True})
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;有关详细信息，请参阅&lt;a href=&quot;https://doc.scrapy.org/en/1.3/topics/downloader-middleware.html#cookies-mw&quot;&gt;CookiesMiddleware&lt;/a&gt;。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;encoding（string）&lt;/code&gt; - 此请求的编码（默认为’utf-8’）。此编码将用于对URL进行百分比编码，并将正文转换为str（如果给定unicode）。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;priority（int）&lt;/code&gt; - 此请求的优先级（默认为0）。调度器使用优先级来定义用于处理请求的顺序。具有较高优先级值的请求将较早执行。允许负值以指示相对低优先级。&lt;/li&gt;
  &lt;li&gt;
    &lt;h2 id=&quot;dontfilterboolean---false&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;dont_filter（boolean）&lt;/code&gt; - 表示此请求不应由调度程序过滤。当您想要多次执行相同的请求时忽略重复过滤器时使用。小心使用它，或者你会进入爬行循环。默认为False。&lt;/h2&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;errback（callable）&lt;/code&gt; - 如果在处理请求时引发任何异常，将调用的函数。这包括失败的404 HTTP错误等页面。它接收一个&lt;a href=&quot;https://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html&quot;&gt;Twisted Failure&lt;/a&gt;实例作为第一个参数。有关更多信息，请参阅&lt;a href=&quot;https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-response-ref-errbacks&quot;&gt;使用errbacks在请求处理&lt;/a&gt;中&lt;a href=&quot;https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-response-ref-errbacks&quot;&gt;捕获异常&lt;/a&gt;。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;-
&lt;code class=&quot;highlighter-rouge&quot;&gt;url&lt;/code&gt;
包含此请求的网址的字符串。请记住，此属性包含转义的网址，因此它可能与构造函数中传递的网址不同。&lt;/p&gt;

&lt;p&gt;此属性为只读。更改请求使用的URL &lt;code class=&quot;highlighter-rouge&quot;&gt;replace()&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;-
&lt;code class=&quot;highlighter-rouge&quot;&gt;method&lt;/code&gt;
表示请求中的HTTP方法的字符串。这保证是大写的。例如：&lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;GET&quot;，&quot;POST&quot;，&quot;PUT&quot;&lt;/code&gt;等&lt;/p&gt;

&lt;p&gt;-
&lt;code class=&quot;highlighter-rouge&quot;&gt;headers&lt;/code&gt;
包含请求标头的类似字典的对象。&lt;/p&gt;

&lt;p&gt;-
&lt;code class=&quot;highlighter-rouge&quot;&gt;body&lt;/code&gt;
包含请求正文的str。&lt;/p&gt;

&lt;p&gt;此属性为只读。更改请求使用的正文 &lt;code class=&quot;highlighter-rouge&quot;&gt;replace()&lt;/code&gt;。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;meta&lt;/code&gt;
包含此请求的任意元数据的字典。此dict对于新请求为空，通常由不同的Scrapy组件（扩展程序，中间件等）填充。因此，此dict中包含的数据取决于您启用的扩展。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;有关&lt;a href=&quot;https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-meta&quot;&gt;Scrapy识别&lt;/a&gt;的特殊元键列表，请参阅&lt;a href=&quot;https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-meta&quot;&gt;Request.meta特殊键&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;当使用or 方法克隆请求时，此dict是&lt;a href=&quot;https://docs.python.org/2/library/copy.html&quot;&gt;浅复制&lt;/a&gt;的 ，并且也可以在您的爬虫中从属性访问。copy()replace()response.meta&lt;/p&gt;

&lt;p&gt;-
&lt;code class=&quot;highlighter-rouge&quot;&gt;copy（）&lt;/code&gt;
返回一个新的请求，它是这个请求的副本。另请参见： &lt;a href=&quot;https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-response-ref-request-callback-arguments&quot;&gt;将附加数据传递到回调函数&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;-
&lt;code class=&quot;highlighter-rouge&quot;&gt;replace([url, method, headers, body, cookies, meta, encoding, dont_filter, callback, errback])&lt;/code&gt;
返回具有相同成员的Request对象，但通过指定的任何关键字参数赋予新值的成员除外。该属性Request.meta是默认复制（除非新的值在给定的meta参数）。另请参见 &lt;a href=&quot;https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-response-ref-request-callback-arguments&quot;&gt;将附加数据传递给回调函数&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;将附加数据传递给回调函数&lt;/h3&gt;

&lt;p&gt;请求的回调是当下载该请求的响应时将被调用的函数。将使用下载的Response对象作为其第一个参数来调用回调函数。&lt;/p&gt;

&lt;p&gt;例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def parse_page1(self, response):
    return scrapy.Request(&quot;http://www.example.com/some_page.html&quot;,
                          callback=self.parse_page2)

def parse_page2(self, response):
    # this would log http://www.example.com/some_page.html
    self.logger.info(&quot;Visited %s&quot;, response.url)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;在某些情况下，您可能有兴趣向这些回调函数传递参数，以便稍后在第二个回调中接收参数。您可以使用该&lt;code class=&quot;highlighter-rouge&quot;&gt;Request.meta&lt;/code&gt;属性。&lt;/p&gt;

&lt;p&gt;以下是使用此机制传递项目以填充来自不同页面的不同字段的示例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def parse_page1(self, response):
    item = MyItem()
    item['main_url'] = response.url
    request = scrapy.Request(&quot;http://www.example.com/some_page.html&quot;,
                             callback=self.parse_page2)
    request.meta['item'] = item
    yield request

def parse_page2(self, response):
    item = response.meta['item']
    item['other_url'] = response.url
    yield item
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;errbacks&quot;&gt;使用errbacks在请求处理中捕获异常&lt;/h3&gt;

&lt;p&gt;请求的errback是在处理异常时被调用的函数。&lt;/p&gt;

&lt;p&gt;它接收一个&lt;a href=&quot;https://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html&quot;&gt;Twisted Failure&lt;/a&gt;实例作为第一个参数，并可用于跟踪连接建立超时，DNS错误等。&lt;/p&gt;

&lt;p&gt;这里有一个示例爬虫记录所有错误，并捕获一些特定的错误，如果需要：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import scrapy

from scrapy.spidermiddlewares.httperror import HttpError
from twisted.internet.error import DNSLookupError
from twisted.internet.error import TimeoutError, TCPTimedOutError

class ErrbackSpider(scrapy.Spider):
    name = &quot;errback_example&quot;
    start_urls = [
        &quot;http://www.httpbin.org/&quot;,              # HTTP 200 expected
        &quot;http://www.httpbin.org/status/404&quot;,    # Not found error
        &quot;http://www.httpbin.org/status/500&quot;,    # server issue
        &quot;http://www.httpbin.org:12345/&quot;,        # non-responding host, timeout expected
        &quot;http://www.httphttpbinbin.org/&quot;,       # DNS error expected
    ]

    def start_requests(self):
        for u in self.start_urls:
            yield scrapy.Request(u, callback=self.parse_httpbin,
                                    errback=self.errback_httpbin,
                                    dont_filter=True)

    def parse_httpbin(self, response):
        self.logger.info('Got successful response from {}'.format(response.url))
        # do something useful here...

    def errback_httpbin(self, failure):
        # log all failures
        self.logger.error(repr(failure))

        # in case you want to do something special for some errors,
        # you may need the failure's type:

        if failure.check(HttpError):
            # these exceptions come from HttpError spider middleware
            # you can get the non-200 response
            response = failure.value.response
            self.logger.error('HttpError on %s', response.url)

        elif failure.check(DNSLookupError):
            # this is the original request
            request = failure.request
            self.logger.error('DNSLookupError on %s', request.url)

        elif failure.check(TimeoutError, TCPTimedOutError):
            request = failure.request
            self.logger.error('TimeoutError on %s', request.url)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;requestmeta&quot;&gt;Request.meta特殊键&lt;/h2&gt;

&lt;p&gt;该&lt;code class=&quot;highlighter-rouge&quot;&gt;Request.meta&lt;/code&gt;属性可以包含任何任意数据，但有一些特殊的键由Scrapy及其内置扩展识别。&lt;/p&gt;

&lt;p&gt;那些是：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dont_redirect
dont_retry
handle_httpstatus_list
handle_httpstatus_all
dont_merge_cookies（参见cookies构造函数的Request参数）
cookiejar
dont_cache
redirect_urls
bindaddress
dont_obey_robotstxt
download_timeout
download_maxsize
download_latency
proxy
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;bindaddress&quot;&gt;bindaddress&lt;/h4&gt;

&lt;p&gt;用于执行请求的出站IP地址的IP。&lt;/p&gt;

&lt;h4 id=&quot;downloadtimeout&quot;&gt;download_timeout&lt;/h4&gt;

&lt;p&gt;下载器在超时前等待的时间量（以秒为单位）。参见：&lt;code class=&quot;highlighter-rouge&quot;&gt;DOWNLOAD_TIMEOUT&lt;/code&gt;。&lt;/p&gt;

&lt;h4 id=&quot;downloadlatency&quot;&gt;download_latency&lt;/h4&gt;

&lt;p&gt;自请求已启动以来，用于获取响应的时间量，即通过网络发送的HTTP消息。此元键仅在响应已下载时可用。虽然大多数其他元键用于控制Scrapy行为，但这应该是只读的。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;请求子类&lt;/h2&gt;

&lt;p&gt;这里是内置子类的Request列表。您还可以将其子类化以实现您自己的自定义功能。&lt;/p&gt;

&lt;p&gt;FormRequest对象
FormRequest类扩展了Request具有处理HTML表单的功能的基础。它使用lxml.html表单 从Response对象的表单数据预填充表单字段。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;class scrapy.http.FormRequest(url[, formdata, ...])&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;本&lt;code class=&quot;highlighter-rouge&quot;&gt;FormRequest&lt;/code&gt;类增加了新的构造函数的参数。其余的参数与&lt;code class=&quot;highlighter-rouge&quot;&gt;Request&lt;/code&gt;类相同，这里没有记录。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;参数：&lt;strong&gt;formdata&lt;/strong&gt;（元组的dict或iterable） - 是一个包含HTML Form数据的字典（或（key，value）元组的迭代），它将被url编码并分配给请求的主体。
该&lt;code class=&quot;highlighter-rouge&quot;&gt;FormRequest&lt;/code&gt;对象支持除标准以下类方法Request的方法：&lt;/p&gt;

    &lt;p&gt;classmethod from_response(response[, formname=None, formid=None, formnumber=0, formdata=None, formxpath=None, formcss=None, clickdata=None, dont_click=False, …])&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;返回一个新&lt;code class=&quot;highlighter-rouge&quot;&gt;FormRequest&lt;/code&gt;对象，其中的表单字段值已预先&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;form&amp;gt;&lt;/code&gt;填充在给定响应中包含的HTML 元素中。有关示例，请参阅 &lt;a href=&quot;https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-response-ref-request-userlogin&quot;&gt;使用FormRequest.from_response（）来模拟用户登录&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;该策略是在任何可查看的表单控件上默认自动模拟点击，如a 。即使这是相当方便，并且经常想要的行为，有时它可能导致难以调试的问题。例如，当使用使用javascript填充和/或提交的表单时，默认行为可能不是最合适的。要禁用此行为，您可以将参数设置 为。此外，如果要更改单击的控件（而不是禁用它），您还可以使用 参数。&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;input type=&quot;submit&quot;&amp;gt; from_response() dont_click True clickdata&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;参数：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;response（Responseobject） - 包含将用于预填充表单字段的HTML表单的响应&lt;/li&gt;
  &lt;li&gt;formname（string） - 如果给定，将使用name属性设置为此值的形式。&lt;/li&gt;
  &lt;li&gt;formid（string） - 如果给定，将使用id属性设置为此值的形式。&lt;/li&gt;
  &lt;li&gt;formxpath（string） - 如果给定，将使用匹配xpath的第一个表单。&lt;/li&gt;
  &lt;li&gt;formcss（string） - 如果给定，将使用匹配css选择器的第一个形式。&lt;/li&gt;
  &lt;li&gt;formnumber（integer） - 当响应包含多个表单时要使用的表单的数量。第一个（也是默认）是0。&lt;/li&gt;
  &lt;li&gt;formdata（dict） - 要在表单数据中覆盖的字段。如果响应&amp;lt;form&amp;gt;元素中已存在字段，则其值将被在此参数中传递的值覆盖。&lt;/li&gt;
  &lt;li&gt;clickdata（dict） - 查找控件被点击的属性。如果没有提供，表单数据将被提交，模拟第一个可点击元素的点击。除了html属性，控件可以通过其相对于表单中其他提交表输入的基于零的索引，通过nr属性来标识。&lt;/li&gt;
  &lt;li&gt;dont_click（boolean） - 如果为True，表单数据将在不点击任何元素的情况下提交。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;这个类方法的其他参数直接传递给 FormRequest构造函数。
在新版本0.10.3：该formname参数。
在新版本0.17：该formxpath参数。
新的版本1.1.0：该formcss参数。
新的版本1.1.0：该formid参数。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;section-3&quot;&gt;请求使用示例&lt;/h3&gt;

&lt;h4 id=&quot;formrequesthttp-post&quot;&gt;使用FormRequest通过HTTP POST发送数据&lt;/h4&gt;

&lt;p&gt;如果你想在你的爬虫中模拟HTML表单POST并发送几个键值字段，你可以返回一个FormRequest对象（从你的爬虫）像这样：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;return [FormRequest(url=&quot;http://www.example.com/post/action&quot;,
                    formdata={'name': 'John Doe', 'age': '27'},
                    callback=self.after_post)]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;formrequestfromresponse&quot;&gt;使用FormRequest.from_response（）来模拟用户登录&lt;/h4&gt;

&lt;p&gt;网站通常通过元素（例如会话相关数据或认证令牌（用于登录页面））提供预填充的表单字段。进行剪贴时，您需要自动预填充这些字段，并且只覆盖其中的一些，例如用户名和密码。您可以使用 此作业的方法。这里有一个使用它的爬虫示例：&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;input type=&quot;hidden&quot;&amp;gt; FormRequest.from_response()&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import scrapy

class LoginSpider(scrapy.Spider):
    name = 'example.com'
    start_urls = ['http://www.example.com/users/login.php']

    def parse(self, response):
        return scrapy.FormRequest.from_response(
            response,
            formdata={'username': 'john', 'password': 'secret'},
            callback=self.after_login
        )

    def after_login(self, response):
        # check login succeed before going on
        if &quot;authentication failed&quot; in response.body:
            self.logger.error(&quot;Login failed&quot;)
            return

        # continue scraping with authenticated session...
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h1 id=&quot;section-4&quot;&gt;响应对象&lt;/h1&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;class scrapy.http.Response(url[, status=200, headers=None, body=b'', flags=None, request=None])&lt;/code&gt;
一个Response对象表示的HTTP响应，这通常是下载（由下载），并供给到爬虫进行处理。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;参数：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;url（string） - 此响应的URL&lt;/li&gt;
  &lt;li&gt;status（integer） - 响应的HTTP状态。默认为&lt;strong&gt;200&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;headers（dict） - 这个响应的头。dict值可以是字符串（对于单值标头）或列表（对于多值标头）。&lt;/li&gt;
  &lt;li&gt;body（str） - 响应体。它必须是str，而不是unicode，除非你使用一个编码感知&lt;a href=&quot;https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-response-ref-response-subclasses&quot;&gt;响应子类&lt;/a&gt;，如 &lt;code class=&quot;highlighter-rouge&quot;&gt;TextResponse&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;flags（&lt;a href=&quot;https://doc.scrapy.org/en/1.3/topics/api.html#scrapy.loader.SpiderLoader.list&quot;&gt;list&lt;/a&gt;） - 是一个包含属性初始值的 &lt;code class=&quot;highlighter-rouge&quot;&gt;Response.flags&lt;/code&gt;列表。如果给定，列表将被浅复制。&lt;/li&gt;
  &lt;li&gt;request（Requestobject） - 属性的初始值&lt;code class=&quot;highlighter-rouge&quot;&gt;Response.request&lt;/code&gt;。这代表Request生成此响应。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;url&lt;/strong&gt;
包含响应的URL的字符串。&lt;/p&gt;

&lt;p&gt;此属性为只读。更改响应使用的URL replace()。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;status&lt;/strong&gt;
表示响应的HTTP状态的整数。示例：200， 404。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;headers&lt;/strong&gt;
包含响应标题的类字典对象。可以使用get()返回具有指定名称的第一个标头值或getlist()返回具有指定名称的所有标头值来访问值。例如，此调用会为您提供标题中的所有Cookie：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;response.headers.getlist('Set-Cookie')
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;body&lt;/strong&gt;
本回复的正文。记住Response.body总是一个字节对象。如果你想unicode版本使用 TextResponse.text（只在TextResponse 和子类中可用）。&lt;/p&gt;

&lt;p&gt;此属性为只读。更改响应使用的主体 replace()。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;request&lt;/strong&gt;
Request生成此响应的对象。在响应和请求通过所有&lt;a href=&quot;https://doc.scrapy.org/en/1.3/topics/downloader-middleware.html#topics-downloader-middleware&quot;&gt;下载中间件&lt;/a&gt;后，此属性在Scrapy引擎中分配。特别地，这意味着：&lt;/p&gt;

&lt;p&gt;HTTP重定向将导致将原始请求（重定向之前的URL）分配给重定向响应（重定向后具有最终URL）。
Response.request.url并不总是等于Response.url
此属性仅在爬虫程序代码和 &lt;a href=&quot;https://doc.scrapy.org/en/1.3/topics/spider-middleware.html#topics-spider-middleware&quot;&gt;Spider Middleware&lt;/a&gt;中可用，但不能在Downloader Middleware中使用（尽管您有通过其他方式可用的请求）和处理程序response_downloaded。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;meta&lt;/strong&gt;
的快捷方式Request.meta的属性 Response.request对象（即self.request.meta）。&lt;/p&gt;

&lt;p&gt;与Response.request属性不同，Response.meta 属性沿重定向和重试传播，因此您将获得Request.meta从您的爬虫发送的原始属性。&lt;/p&gt;

&lt;p&gt;也可以看看&lt;/p&gt;

&lt;p&gt;Request.meta 属性&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;flags&lt;/strong&gt;
包含此响应的标志的列表。标志是用于标记响应的标签。例如：’cached’，’redirected ‘等等。它们显示在Response（** str** 方法）的字符串表示上，它被引擎用于日志记录。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;copy（）&lt;/strong&gt;
返回一个新的响应，它是此响应的副本。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;replace（[ url，status，headers，body，request，flags，cls ] ）&lt;/strong&gt;
返回具有相同成员的Response对象，但通过指定的任何关键字参数赋予新值的成员除外。该属性Response.meta是默认复制。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;urljoin（url ）&lt;/strong&gt;
通过将响应url与可能的相对URL 组合构造绝对url。&lt;/p&gt;

&lt;p&gt;这是一个包装在&lt;a href=&quot;https://docs.python.org/2/library/urlparse.html#urlparse.urljoin&quot;&gt;urlparse.urljoin&lt;/a&gt;，它只是一个别名，使这个调用：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;urlparse.urljoin(response.url, url)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;section-5&quot;&gt;响应子类&lt;/h3&gt;

&lt;p&gt;这里是可用的内置Response子类的列表。您还可以将Response类子类化以实现您自己的功能。&lt;/p&gt;

&lt;h4 id=&quot;textresponse&quot;&gt;TextResponse对象&lt;/h4&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;class scrapy.http.TextResponse(url[, encoding[, ...]])&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;TextResponse对象向基Response类添加编码能力 ，这意味着仅用于二进制数据，例如图像，声音或任何媒体文件。&lt;/p&gt;

&lt;p&gt;TextResponse对象支持一个新的构造函数参数，除了基础Response对象。其余的功能与Response类相同，这里没有记录。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;参数：    encoding（string）&lt;/strong&gt; - 是一个字符串，包含用于此响应的编码。如果你创建一个TextResponse具有unicode主体的对象，它将使用这个编码进行编码（记住body属性总是一个字符串）。如果encoding是None（默认值），则将在响应标头和正文中查找编码。
TextResponse除了标准对象之外，对象还支持以下属性Response&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;text&lt;/strong&gt;
响应体，如unicode。&lt;/p&gt;

&lt;p&gt;同样&lt;code class=&quot;highlighter-rouge&quot;&gt;response.body.decode(response.encoding)&lt;/code&gt;，但结果是在第一次调用后缓存，因此您可以访问 &lt;code class=&quot;highlighter-rouge&quot;&gt;response.text&lt;/code&gt;多次，无需额外的开销。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;
unicode(response.body)不是一个正确的方法来将响应身体转换为unicode：您将使用系统默认编码（通常为ascii）而不是响应编码。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;encoding&lt;/strong&gt;
包含此响应的编码的字符串。编码通过尝试以下机制按顺序解决：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;在构造函数编码参数中传递的编码&lt;/li&gt;
  &lt;li&gt;在Content-Type HTTP头中声明的编码。如果此编码无效（即未知），则会被忽略，并尝试下一个解析机制。&lt;/li&gt;
  &lt;li&gt;在响应主体中声明的编码。TextResponse类不提供任何特殊功能。然而， HtmlResponse和XmlResponse类做。&lt;/li&gt;
  &lt;li&gt;通过查看响应体来推断的编码。这是更脆弱的方法，但也是最后一个尝试。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;selector&lt;/strong&gt;
一个Selector使用响应为目标实例。选择器在第一次访问时被延迟实例化。&lt;/p&gt;

&lt;p&gt;TextResponse对象除了标准对象外还支持以下方法Response：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;xpath（查询）&lt;/strong&gt;
快捷方式&lt;code class=&quot;highlighter-rouge&quot;&gt;TextResponse.selector.xpath(query)&lt;/code&gt;：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;response.xpath('//p')
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;css(query)&lt;/strong&gt;
快捷方式 &lt;code class=&quot;highlighter-rouge&quot;&gt;TextResponse.selector.css(query)&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;response.css('p')
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;body_as_unicode()&lt;/strong&gt;
同样text，但可用作方法。保留此方法以实现向后兼容; 请喜欢response.text。&lt;/p&gt;

&lt;h4 id=&quot;htmlresponse&quot;&gt;HtmlResponse对象&lt;/h4&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;class scrapy.http.HtmlResponse（url [，... ] ）&lt;/code&gt;
本HtmlResponse类的子类，TextResponse 这增加了通过查看HTML编码自动发现支持META HTTP-EQUIV属性。见TextResponse.encoding。&lt;/p&gt;

&lt;h4 id=&quot;xmlresponse&quot;&gt;XmlResponse对象&lt;/h4&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;class scrapy.http.XmlResponse（url [，... ] ）&lt;/code&gt;
本XmlResponse类的子类，TextResponse这增加了通过查看XML声明线路编码自动发现支持。见TextResponse.encoding。&lt;/p&gt;
</content>
<category term="Scrapy" />
<summary>Scrapy爬虫入门教程十一 Request和Response（请求和响应）</summary>
</entry>
<entry>
<title>Scrapy爬虫入门教程十 Feed exports（导出文件）</title>
<link href="http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E5%8D%81-feed-exports-%E5%AF%BC%E5%87%BA%E6%96%87%E4%BB%B6/" rel="alternate" type="text/html" title="Scrapy爬虫入门教程十 Feed exports（导出文件）" />
<published>2017-04-09T14:33:00+08:00</published>
<updated>2017-04-09T14:33:00+08:00</updated>
<id>http://localhost:4000/scrapy爬虫入门教程十-feed-exports（导出文件）</id>
<content type="html" xml:base="http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E5%8D%81-feed-exports-%E5%AF%BC%E5%87%BA%E6%96%87%E4%BB%B6/">&lt;h1 id=&quot;scrapy-feed-exports&quot;&gt;Scrapy爬虫入门教程十 Feed exports（导出文件）&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;开发环境：&lt;/strong&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;Python 3.6.0 版本&lt;/code&gt; （当前最新）
&lt;code class=&quot;highlighter-rouge&quot;&gt;Scrapy 1.3.2 版本&lt;/code&gt; （当前最新）&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;导出文件&lt;/h1&gt;

&lt;p&gt;新版本0.10。&lt;/p&gt;

&lt;p&gt;实现爬虫时最常需要的特征之一是能够正确地存储所过滤的数据，并且经常意味着使用被过滤的数据（通常称为“export feed”）生成要由其他系统消耗的“导出文件” 。&lt;/p&gt;

&lt;p&gt;Scrapy使用Feed导出功能即时提供此功能，这允许您使用多个序列化格式和存储后端来生成包含已抓取项目的Feed。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;序列化格式&lt;/h2&gt;

&lt;p&gt;为了序列化抓取的数据，Feed导出使用项导出器。这些格式是开箱即用的：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/feed-exports.html#topics-feed-format-json&quot;&gt;JSON&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/feed-exports.html#topics-feed-format-jsonlines&quot;&gt;JSON lines&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/feed-exports.html#topics-feed-format-csv&quot;&gt;CSV&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/feed-exports.html#topics-feed-format-xml&quot;&gt;XML&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;但您也可以通过FEED_EXPORTERS设置扩展支持的格式 。&lt;/p&gt;

&lt;h3 id=&quot;json&quot;&gt;JSON&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;FEED_FORMAT： json&lt;/li&gt;
  &lt;li&gt;使用出口： JsonItemExporter&lt;/li&gt;
  &lt;li&gt;如果您对大型Feed使用JSON，请参阅&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/exporters.html#json-with-large-data&quot;&gt;此警告&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;json-lines&quot;&gt;JSON lines&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;FEED_FORMAT： jsonlines&lt;/li&gt;
  &lt;li&gt;使用出口： JsonLinesItemExporter&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;csv&quot;&gt;CSV&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;FEED_FORMAT： csv&lt;/li&gt;
  &lt;li&gt;使用出口： CsvItemExporter&lt;/li&gt;
  &lt;li&gt;指定要导出的列及其顺序使用 FEED_EXPORT_FIELDS。其他Feed导出程序也可以使用此选项，但它对CSV很重要，因为与许多其他导出格式不同，CSV使用固定标头。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;xml&quot;&gt;XML&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;FEED_FORMAT： xml&lt;/li&gt;
  &lt;li&gt;使用出口： XmlItemExporter&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pickle&quot;&gt;Pickle&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;FEED_FORMAT： pickle&lt;/li&gt;
  &lt;li&gt;使用出口： PickleItemExporter&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;marshal&quot;&gt;Marshal&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;FEED_FORMAT： marshal&lt;/li&gt;
  &lt;li&gt;使用出口： MarshalItemExporter&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;section-2&quot;&gt;存储&lt;/h2&gt;

&lt;p&gt;使用Feed导出时，您可以使用&lt;a href=&quot;https://en.wikipedia.org/wiki/Uniform_Resource_Identifier&quot;&gt;URI&lt;/a&gt;（通过FEED_URI设置）定义在哪里存储Feed 。Feed导出支持由URI方案定义的多个存储后端类型。&lt;/p&gt;

&lt;p&gt;支持开箱即用的存储后端包括：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/feed-exports.html#topics-feed-storage-fs&quot;&gt;本地文件系统&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/feed-exports.html#topics-feed-storage-ftp&quot;&gt;FTP&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/feed-exports.html#topics-feed-storage-s3&quot;&gt;S3&lt;/a&gt;（需要 &lt;a href=&quot;https://github.com/boto/botocore&quot;&gt;botocore&lt;/a&gt;或 &lt;a href=&quot;https://github.com/boto/boto&quot;&gt;boto&lt;/a&gt;）&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/feed-exports.html#topics-feed-storage-stdout&quot;&gt;标准输出&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果所需的外部库不可用，则某些存储后端可能无法使用。例如，S3后端仅在安装了&lt;a href=&quot;https://github.com/boto/botocore&quot;&gt;botocore&lt;/a&gt; 或&lt;a href=&quot;https://github.com/boto/boto&quot;&gt;boto&lt;/a&gt;库时可用（Scrapy仅支持&lt;a href=&quot;https://github.com/boto/boto&quot;&gt;boto&lt;/a&gt;到Python 2）。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;uri&quot;&gt;存储URI参数&lt;/h2&gt;

&lt;p&gt;存储URI还可以包含在创建订阅源时被替换的参数。这些参数是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;%(time)s - 在创建订阅源时由时间戳替换&lt;/li&gt;
  &lt;li&gt;%(name)s - 被蜘蛛名替换&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;任何其他命名参数将替换为同名的spider属性。例如， 在创建订阅源的那一刻，&lt;code class=&quot;highlighter-rouge&quot;&gt;%(site_id)s&lt;/code&gt;将被&lt;code class=&quot;highlighter-rouge&quot;&gt;spider.site_id&lt;/code&gt;属性替换。&lt;/p&gt;

&lt;p&gt;这里有一些例子来说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;存储在FTP中使用每个蜘蛛一个目录：&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ftp://user:password@ftp.example.com/scraping/feeds/%(name)s/%(time)s.json&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;存储在S3使用每个蜘蛛一个目录：&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;s3://mybucket/scraping/feeds/%(name)s/%(time)s.json&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-3&quot;&gt;存储后端&lt;/h2&gt;

&lt;h3 id=&quot;section-4&quot;&gt;本地文件系统&lt;/h3&gt;

&lt;p&gt;订阅源存储在本地文件系统中。&lt;/p&gt;

&lt;p&gt;URI方案： &lt;code class=&quot;highlighter-rouge&quot;&gt;file&lt;/code&gt;
示例URI： &lt;code class=&quot;highlighter-rouge&quot;&gt;file:///tmp/export.csv&lt;/code&gt;
所需的外部库：none
&lt;strong&gt;请注意&lt;/strong&gt;，（仅）对于本地文件系统存储，如果指定绝对路径，则可以省略该方案&lt;code class=&quot;highlighter-rouge&quot;&gt;/tmp/export.csv&lt;/code&gt;。这只适用于Unix系统。&lt;/p&gt;

&lt;h3 id=&quot;ftp&quot;&gt;FTP&lt;/h3&gt;

&lt;p&gt;订阅源存储在FTP服务器中。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;URI方案： &lt;code class=&quot;highlighter-rouge&quot;&gt;ftp&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;示例URI： &lt;code class=&quot;highlighter-rouge&quot;&gt;ftp://user:pass@ftp.example.com/&lt;/code&gt;path/to/export.csv&lt;/li&gt;
  &lt;li&gt;所需的外部库：none&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;s3&quot;&gt;S3&lt;/h3&gt;

&lt;p&gt;订阅源存储在&lt;a href=&quot;https://aws.amazon.com/s3/&quot;&gt;Amazon S3&lt;/a&gt;上。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;URI方案： &lt;code class=&quot;highlighter-rouge&quot;&gt;s3&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;示例URI：&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;s3://mybucket/path/to/export.csv&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;s3://aws_key:aws_secret@mybucket/path/to/export.csv&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;所需的外部库：botocore或boto&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;AWS凭证可以作为URI中的用户/密码传递，也可以通过以下设置传递：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;AWS_ACCESS_KEY_ID&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;AWS_SECRET_ACCESS_KEY&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-5&quot;&gt;标准输出&lt;/h3&gt;

&lt;p&gt;Feed被写入Scrapy进程的标准输出。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;URI方案： &lt;code class=&quot;highlighter-rouge&quot;&gt;stdout&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;示例URI： &lt;code class=&quot;highlighter-rouge&quot;&gt;stdout:&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;所需的外部库：none&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-6&quot;&gt;设置&lt;/h2&gt;

&lt;p&gt;这些是用于配置Feed导出的设置：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;FEED_URI （强制性）&lt;/li&gt;
  &lt;li&gt;FEED_FORMAT&lt;/li&gt;
  &lt;li&gt;FEED_STORAGES&lt;/li&gt;
  &lt;li&gt;FEED_EXPORTERS&lt;/li&gt;
  &lt;li&gt;FEED_STORE_EMPTY&lt;/li&gt;
  &lt;li&gt;FEED_EXPORT_ENCODING&lt;/li&gt;
  &lt;li&gt;FEED_EXPORT_FIELDS&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;feeduri&quot;&gt;FEED_URI&lt;/h3&gt;

&lt;p&gt;默认： None&lt;/p&gt;

&lt;p&gt;导出Feed的URI。请参阅支持的URI方案的存储后端。&lt;/p&gt;

&lt;p&gt;启用Feed导出时需要此设置。&lt;/p&gt;

&lt;h3 id=&quot;feedformat&quot;&gt;FEED_FORMAT&lt;/h3&gt;

&lt;p&gt;要用于Feed的序列化格式。有关可能的值，请参阅 序列化格式。&lt;/p&gt;

&lt;h3 id=&quot;feedexportencoding&quot;&gt;FEED_EXPORT_ENCODING&lt;/h3&gt;

&lt;p&gt;默认： None&lt;/p&gt;

&lt;p&gt;要用于Feed的编码。&lt;/p&gt;

&lt;p&gt;如果取消设置或设置为None（默认），它使用UTF-8除了JSON输出，\uXXXX由于历史原因使用安全的数字编码（序列）。&lt;/p&gt;

&lt;p&gt;使用utf-8，如果你想UTF-8 JSON了。&lt;/p&gt;

&lt;h3 id=&quot;feedexportfields&quot;&gt;FEED_EXPORT_FIELDS&lt;/h3&gt;

&lt;p&gt;默认： None&lt;/p&gt;

&lt;p&gt;要导出的字段的列表，可选。示例：。FEED_EXPORT_FIELDS = [“foo”, “bar”, “baz”]&lt;/p&gt;

&lt;p&gt;使用FEED_EXPORT_FIELDS选项定义要导出的字段及其顺序。&lt;/p&gt;

&lt;p&gt;当FEED_EXPORT_FIELDS为空或无（默认）时，Scrapy使用在Item蜘蛛正在产生的dicts 或子类中定义的字段。&lt;/p&gt;

&lt;p&gt;如果导出器需要一组固定的字段（CSV导出格式为这种情况 ），并且FEED_EXPORT_FIELDS为空或无，则Scrapy会尝试从导出的​​数据中推断字段名称 - 当前它使用第一个项目中的字段名称。&lt;/p&gt;

&lt;h3 id=&quot;feedstoreempty&quot;&gt;FEED_STORE_EMPTY&lt;/h3&gt;

&lt;p&gt;默认： False&lt;/p&gt;

&lt;p&gt;是否导出空Feed（即，没有项目的Feed）。&lt;/p&gt;

&lt;p&gt;FEED_STORAGES
默认： {}&lt;/p&gt;

&lt;p&gt;包含您的项目支持的其他Feed存储后端的字典。键是URI方案，值是存储类的路径。&lt;/p&gt;

&lt;h3 id=&quot;feedstoragesbase&quot;&gt;FEED_STORAGES_BASE&lt;/h3&gt;

&lt;p&gt;默认：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'scrapy.extensions.feedexport.FileFeedStorage',&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'file':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'scrapy.extensions.feedexport.FileFeedStorage',&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'stdout':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'scrapy.extensions.feedexport.StdoutFeedStorage',&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'s3':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'scrapy.extensions.feedexport.S3FeedStorage',&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'ftp':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'scrapy.extensions.feedexport.FTPFeedStorage',&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;包含Scrapy支持的内置Feed存储后端的字典。您可以通过分配其中None的URI方案 来禁用这些后端FEED_STORAGES。例如，要禁用内置FTP存储后端（无替换），请将其放置在settings.py：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FEED_STORAGES = {
    'ftp': None,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;feedexporters&quot;&gt;FEED_EXPORTERS&lt;/h3&gt;

&lt;p&gt;默认： {}&lt;/p&gt;

&lt;p&gt;包含您的项目支持的其他导出器的字典。键是序列化格式，值是Item exporter类的路径。&lt;/p&gt;

&lt;h3 id=&quot;feedexportersbase&quot;&gt;FEED_EXPORTERS_BASE&lt;/h3&gt;

&lt;p&gt;默认：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'json':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'scrapy.exporters.JsonItemExporter',&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'jsonlines':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'scrapy.exporters.JsonLinesItemExporter',&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'jl':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'scrapy.exporters.JsonLinesItemExporter',&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'csv':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'scrapy.exporters.CsvItemExporter',&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'xml':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'scrapy.exporters.XmlItemExporter',&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'marshal':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'scrapy.exporters.MarshalItemExporter',&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'pickle':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'scrapy.exporters.PickleItemExporter',&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;一个包含Scrapy支持的内置feed导出器的dict。您可以通过分配其中None的序列化格式来禁用任何这些导出器FEED_EXPORTERS。例如，要禁用内置的CSV导出器（无替换），请将其放置在settings.py：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FEED_EXPORTERS = {
    'csv': None,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</content>
<category term="Scrapy" />
<summary>Scrapy爬虫入门教程十 Feed exports（导出文件）</summary>
</entry>
<entry>
<title>Scrapy爬虫入门教程九 Item Pipeline（项目管道）</title>
<link href="http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E4%B9%9D-item-pipeline-%E9%A1%B9%E7%9B%AE%E7%AE%A1%E9%81%93/" rel="alternate" type="text/html" title="Scrapy爬虫入门教程九 Item Pipeline（项目管道）" />
<published>2017-04-09T14:32:00+08:00</published>
<updated>2017-04-09T14:32:00+08:00</updated>
<id>http://localhost:4000/scrapy爬虫入门教程九-item-pipeline（项目管道）</id>
<content type="html" xml:base="http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E4%B9%9D-item-pipeline-%E9%A1%B9%E7%9B%AE%E7%AE%A1%E9%81%93/">&lt;h1 id=&quot;scrapy-item-pipeline&quot;&gt;Scrapy爬虫入门教程九 Item Pipeline（项目管道）&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;开发环境：&lt;/strong&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;Python 3.6.0 版本&lt;/code&gt; （当前最新）
&lt;code class=&quot;highlighter-rouge&quot;&gt;Scrapy 1.3.2 版本&lt;/code&gt; （当前最新）&lt;/p&gt;

&lt;h1 id=&quot;item-pipeline&quot;&gt;Item Pipeline（项目管道）&lt;/h1&gt;

&lt;p&gt;在项目被蜘蛛抓取后，它被发送到项目管道，它通过顺序执行的几个组件来处理它。&lt;/p&gt;

&lt;p&gt;每个项目管道组件（有时称为“Item Pipeline”）是一个实现简单方法的Python类。他们接收一个项目并对其执行操作，还决定该项目是否应该继续通过流水线或被丢弃并且不再被处理。&lt;/p&gt;

&lt;p&gt;项目管道的典型用途是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;清理HTML数据&lt;/li&gt;
  &lt;li&gt;验证抓取的数据（检查项目是否包含特定字段）&lt;/li&gt;
  &lt;li&gt;检查重复（并删除）&lt;/li&gt;
  &lt;li&gt;将刮取的项目存储在数据库中&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section&quot;&gt;编写自己的项目管道&lt;/h2&gt;

&lt;p&gt;每个项目管道组件是一个Python类，必须实现以下方法：
&lt;code class=&quot;highlighter-rouge&quot;&gt;process_item(self, item, spider)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;对于每个项目管道组件调用此方法。process_item() 必须：返回一个带数据的dict，返回一个Item （或任何后代类）对象，返回一个Twisted Deferred或者raise DropItemexception。丢弃的项目不再由其他管道组件处理。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;参数：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;item（Itemobject或dict） - 剪切的项目&lt;/li&gt;
  &lt;li&gt;Spider（Spider对象） - 抓取物品的蜘蛛&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;另外，它们还可以实现以下方法：&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;open_spider(self, spider)&lt;/code&gt;
当蜘蛛打开时调用此方法。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;参数：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;蜘蛛（Spider对象） - 打开的蜘蛛&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;close_spider(self, spider)&lt;/code&gt;
当蜘蛛关闭时调用此方法。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;参数：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;蜘蛛（Spider对象） - 被关闭的蜘蛛&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;from_crawler(cls, crawler)&lt;/code&gt;
如果存在，则调用此类方法以从a创建流水线实例Crawler。它必须返回管道的新实例。Crawler对象提供对所有Scrapy核心组件（如设置和信号）的访问; 它是管道访问它们并将其功能挂钩到Scrapy中的一种方式。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;参数：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;crawler（Crawlerobject） - 使用此管道的crawler&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;section-1&quot;&gt;项目管道示例&lt;/h2&gt;

&lt;h3 id=&quot;section-2&quot;&gt;价格验证和丢弃项目没有价格&lt;/h3&gt;

&lt;p&gt;让我们来看看以下假设的管道，它调整 price那些不包括增值税（price_excludes_vat属性）的项目的属性，并删除那些不包含价格的项目：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from scrapy.exceptions import DropItem

class PricePipeline(object):

    vat_factor = 1.15

    def process_item(self, item, spider):
        if item['price']:
            if item['price_excludes_vat']:
                item['price'] = item['price'] * self.vat_factor
            return item
        else:
            raise DropItem(&quot;Missing price in %s&quot; % item)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;将项目写入JSON文件
以下管道将所有抓取的项目（来自所有蜘蛛）存储到单个items.jl文件中，每行包含一个项目，以JSON格式序列化：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import json

class JsonWriterPipeline(object):

    def open_spider(self, spider):
        self.file = open('items.jl', 'wb')

    def close_spider(self, spider):
        self.file.close()

    def process_item(self, item, spider):
        line = json.dumps(dict(item)) + &quot;\n&quot;
        self.file.write(line)
        return item
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;JsonWriterPipeline的目的只是介绍如何编写项目管道。如果您真的想要将所有抓取的项目存储到JSON文件中，则应使用Feed导出。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;mongodb&quot;&gt;将项目写入MongoDB&lt;/h3&gt;

&lt;p&gt;在这个例子中，我们使用&lt;a href=&quot;https://api.mongodb.org/python/current/&quot;&gt;pymongo&lt;/a&gt;将项目写入&lt;a href=&quot;https://www.mongodb.org/&quot;&gt;MongoDB&lt;/a&gt;。MongoDB地址和数据库名称在Scrapy设置中指定; MongoDB集合以item类命名。&lt;/p&gt;

&lt;p&gt;这个例子的要点是显示如何使用&lt;code class=&quot;highlighter-rouge&quot;&gt;from_crawler()&lt;/code&gt;方法和如何正确清理资源：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import pymongo

class MongoPipeline(object):

    collection_name = 'scrapy_items'

    def __init__(self, mongo_uri, mongo_db):
        self.mongo_uri = mongo_uri
        self.mongo_db = mongo_db

    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            mongo_uri=crawler.settings.get('MONGO_URI'),
            mongo_db=crawler.settings.get('MONGO_DATABASE', 'items')
        )

    def open_spider(self, spider):
        self.client = pymongo.MongoClient(self.mongo_uri)
        self.db = self.client[self.mongo_db]

    def close_spider(self, spider):
        self.client.close()

    def process_item(self, item, spider):
        self.db[self.collection_name].insert(dict(item))
        return item
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;section-3&quot;&gt;拍摄项目的屏幕截图&lt;/h3&gt;

&lt;p&gt;此示例演示如何从方法返回Deferredprocess_item()。它使用Splash来呈现项目网址的屏幕截图。Pipeline请求本地运行的Splash实例。在请求被下载并且Deferred回调触发后，它将项目保存到一个文件并将文件名添加到项目。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import scrapy
import hashlib
from urllib.parse import quote


class ScreenshotPipeline(object):
    &quot;&quot;&quot;Pipeline that uses Splash to render screenshot of
    every Scrapy item.&quot;&quot;&quot;

    SPLASH_URL = &quot;http://localhost:8050/render.png?url={}&quot;

    def process_item(self, item, spider):
        encoded_item_url = quote(item[&quot;url&quot;])
        screenshot_url = self.SPLASH_URL.format(encoded_item_url)
        request = scrapy.Request(screenshot_url)
        dfd = spider.crawler.engine.download(request, spider)
        dfd.addBoth(self.return_item, item)
        return dfd

    def return_item(self, response, item):
        if response.status != 200:
            # Error happened, return item.
            return item

        # Save screenshot to file, filename will be hash of url.
        url = item[&quot;url&quot;]
        url_hash = hashlib.md5(url.encode(&quot;utf8&quot;)).hexdigest()
        filename = &quot;{}.png&quot;.format(url_hash)
        with open(filename, &quot;wb&quot;) as f:
            f.write(response.body)

        # Store filename in item.
        item[&quot;screenshot_filename&quot;] = filename
        return item
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;section-4&quot;&gt;复制过滤器&lt;/h3&gt;

&lt;p&gt;用于查找重复项目并删除已处理的项目的过滤器。假设我们的项目具有唯一的ID，但是我们的蜘蛛会返回具有相同id的多个项目：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from scrapy.exceptions import DropItem

class DuplicatesPipeline(object):

    def __init__(self):
        self.ids_seen = set()

    def process_item(self, item, spider):
        if item['id'] in self.ids_seen:
            raise DropItem(&quot;Duplicate item found: %s&quot; % item)
        else:
            self.ids_seen.add(item['id'])
            return item
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;section-5&quot;&gt;激活项目管道组件&lt;/h2&gt;

&lt;p&gt;要激活项目管道组件，必须将其类添加到 ITEM_PIPELINES设置，类似于以下示例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ITEM_PIPELINES = {
    'myproject.pipelines.PricePipeline': 300,
    'myproject.pipelines.JsonWriterPipeline': 800,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;您在此设置中分配给类的整数值确定它们运行的​​顺序：项目从较低值到较高值类。通常将这些数字定义在0-1000范围内。&lt;/p&gt;
</content>
<category term="Scrapy" />
<summary>Scrapy爬虫入门教程九 Item Pipeline（项目管道）</summary>
</entry>
<entry>
<title>Scrapy爬虫入门教程八 交互式 shell 方便调试</title>
<link href="http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E5%85%AB-%E4%BA%A4%E4%BA%92%E5%BC%8F-shell-%E6%96%B9%E4%BE%BF%E8%B0%83%E8%AF%95/" rel="alternate" type="text/html" title="Scrapy爬虫入门教程八 交互式 shell 方便调试" />
<published>2017-04-09T14:31:00+08:00</published>
<updated>2017-04-09T14:31:00+08:00</updated>
<id>http://localhost:4000/scrapy爬虫入门教程八-交互式-shell-方便调试</id>
<content type="html" xml:base="http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E5%85%AB-%E4%BA%A4%E4%BA%92%E5%BC%8F-shell-%E6%96%B9%E4%BE%BF%E8%B0%83%E8%AF%95/">&lt;h1 id=&quot;scrapy--shell-&quot;&gt;Scrapy爬虫入门教程八 交互式 shell 方便调试&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;开发环境：&lt;/strong&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;Python 3.6.0 版本&lt;/code&gt; （当前最新）
&lt;code class=&quot;highlighter-rouge&quot;&gt;Scrapy 1.3.2 版本&lt;/code&gt; （当前最新）&lt;/p&gt;

&lt;h1 id=&quot;scrapy-shell&quot;&gt;Scrapy shell&lt;/h1&gt;

&lt;p&gt;Scrapy shell是一个交互式shell，您可以在此快速尝试和调试您的抓取代码，而无需运行爬虫程序。它用于测试数据提取代码，但实际上可以使用它来测试任何类型的代码，因为它也是一个常规的Python shell。&lt;/p&gt;

&lt;p&gt;shell用于测试XPath或CSS表达式，并查看它们如何工作，以及他们从您要尝试抓取的网页中提取的数据。它允许您在编写爬虫时交互式测试表达式，而无需运行爬虫来测试每个更改。&lt;/p&gt;

&lt;p&gt;一旦你熟悉了Scrapy shell，你会发现它是开发和调试你的爬虫的一个非常宝贵的工具。&lt;/p&gt;

&lt;h2 id=&quot;shell&quot;&gt;配置shell&lt;/h2&gt;

&lt;p&gt;如果你安装了&lt;a href=&quot;http://ipython.org/&quot;&gt;IPython&lt;/a&gt;，Scrapy shell会使用它（而不是标准的Python控制台）。该IPython的控制台功能更强大，并提供智能自动完成和彩色输出，等等。&lt;/p&gt;

&lt;p&gt;我们强烈建议您安装IPython，特别是如果你正在使用Unix系统（IPython擅长）。有关 详细信息，请参阅&lt;a href=&quot;http://ipython.org/install.html&quot;&gt;IPython安装指南&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;Scrapy还支持&lt;a href=&quot;http://www.bpython-interpreter.org/&quot;&gt;bpython&lt;/a&gt;，并且将尝试在IPython 不可用的地方使用它。&lt;/p&gt;

&lt;p&gt;通过scrapy的设置，您可以配置为使用中的任何一个 &lt;code class=&quot;highlighter-rouge&quot;&gt;ipython&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;bpython&lt;/code&gt;或标准&lt;code class=&quot;highlighter-rouge&quot;&gt;python&lt;/code&gt;外壳，安装无论哪个。这是通过设置&lt;code class=&quot;highlighter-rouge&quot;&gt;SCRAPY_PYTHON_SHELL&lt;/code&gt;环境变量来完成的; 或通过在scrapy.cfg中定义它：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[settings]
shell = bpython
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;shell-1&quot;&gt;启动shell&lt;/h2&gt;

&lt;p&gt;要启动Scrapy shell，可以使用如下shell命令：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scrapy shell &amp;lt;url&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;其中，&lt;url&gt;是您要抓取的网址。&lt;/url&gt;&lt;/p&gt;

&lt;p&gt;shell也适用于本地文件。如果你想玩一个网页的本地副本，这可以很方便。shell了解本地文件的以下语法：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# UNIX-style
scrapy shell ./path/to/file.html
scrapy shell ../other/path/to/file.html
scrapy shell /absolute/path/to/file.html

# File URI
scrapy shell file:///absolute/path/to/file.html
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;当使用相对文件路径时，是显式的，并在它们前面./（或../相关时）。 将不会像一个人所期望的那样工作（这是设计，而不是一个错误）。scrapy shell index.html
因为shell喜欢文件URI上的HTTP URL，并且index.html在语法上类似example.com， shell会将其视为index.html域名并触发DNS查找错误：&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ scrapy shell index.html
[ ... scrapy shell starts ... ]
[ ... traceback ... ]
twisted.internet.error.DNSLookupError: DNS lookup failed:
address 'index.html' not found: [Errno -5] No address associated with hostname.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;shell将不会预先测试index.html 当前目录中是否存在调用的文件。再次，明确。&lt;/p&gt;

&lt;h2 id=&quot;shell-2&quot;&gt;使用shell&lt;/h2&gt;

&lt;p&gt;Scrapy shell只是一个普通的Python控制台（或IPython控制台，如果你有它），为方便起见，它提供了一些额外的快捷方式功能。&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;可用快捷键&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;shelp()&lt;/code&gt; - 打印有可用对象和快捷方式列表的帮助
*&lt;code class=&quot;highlighter-rouge&quot;&gt;fetch(url[, redirect=True])&lt;/code&gt; - 从给定的URL获取新的响应，并相应地更新所有相关对象。你可以选择要求HTTP 3xx重定向，不要通过redirect=False&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;fetch(request)&lt;/code&gt; - 从给定请求获取新响应，并相应地更新所有相关对象。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;view(response)&lt;/code&gt; - 在本地Web浏览器中打开给定的响应，以进行检查。这将向响应正文添加一个&lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/HTML/Element/base&quot;&gt;&lt;base /&gt;标记&lt;/a&gt;，以便正确显示外部链接（如图片和样式表）。但请注意，这将在您的计算机中创建一个临时文件，不会自动删除。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;scrapy&quot;&gt;可用Scrapy对象&lt;/h3&gt;

&lt;p&gt;Scrapy shell自动从下载的页面创建一些方便的对象，如Response对象和 Selector对象（对于HTML和XML内容）。&lt;/p&gt;

&lt;p&gt;这些对象是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;crawler- 当前Crawler对象。&lt;/li&gt;
  &lt;li&gt;spider- 已知处理URL的Spider，或者Spider如果没有为当前URL找到的爬虫，则为 对象&lt;/li&gt;
  &lt;li&gt;request- Request最后一个抓取页面的对象。您可以replace() 使用fetch 快捷方式或使用快捷方式获取新请求（而不离开shell）来修改此请求。&lt;/li&gt;
  &lt;li&gt;response- 包含Response最后一个抓取页面的对象&lt;/li&gt;
  &lt;li&gt;settings- 当前&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/settings.html#topics-settings&quot;&gt;Scrapy设置&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;shell-3&quot;&gt;shell会话的示例&lt;/h2&gt;

&lt;p&gt;下面是一个典型的shell会话示例，我们首先抓取 &lt;a href=&quot;http://scrapy.org%E9%A1%B5%E9%9D%A2%EF%BC%8C%E7%84%B6%E5%90%8E%E7%BB%A7%E7%BB%AD%E6%8A%93%E5%8F%96https://reddit.com&quot;&gt;http://scrapy.org页面，然后继续抓取https://reddit.com&lt;/a&gt; 页面。最后，我们将（Reddit）请求方法修改为POST并重新获取它获取错误。我们通过在Windows中键入Ctrl-D（在Unix系统中）或Ctrl-Z结束会话。&lt;/p&gt;

&lt;p&gt;请记住，在这里提取的数据可能不一样，当你尝试它，因为那些网页不是静态的，可能已经改变了你测试这个。这个例子的唯一目的是让你熟悉Scrapy shell的工作原理。&lt;/p&gt;

&lt;p&gt;首先，我们启动shell：
&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy shell 'http://scrapy.org' --nolog&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;然后，shell获取URL（使用Scrapy下载器）并打印可用对象和有用的快捷方式列表（您会注意到这些行都以[s]前缀开头）：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[s] Available Scrapy objects:
[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
[s]   crawler    &amp;lt;scrapy.crawler.Crawler object at 0x7f07395dd690&amp;gt;
[s]   item       {}
[s]   request    &amp;lt;GET http://scrapy.org&amp;gt;
[s]   response   &amp;lt;200 https://scrapy.org/&amp;gt;
[s]   settings   &amp;lt;scrapy.settings.Settings object at 0x7f07395dd710&amp;gt;
[s]   spider     &amp;lt;DefaultSpider 'default' at 0x7f0735891690&amp;gt;
[s] Useful shortcuts:
[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)
[s]   fetch(req)                  Fetch a scrapy.Request and update local objects
[s]   shelp()           Shell help (print this help)
[s]   view(response)    View response in a browser

&amp;gt;&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;之后，我们可以开始使用对象：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; response.xpath('//title/text()').extract_first()
'Scrapy | A Fast and Powerful Scraping and Web Crawling Framework'

&amp;gt;&amp;gt;&amp;gt; fetch(&quot;http://reddit.com&quot;)

&amp;gt;&amp;gt;&amp;gt; response.xpath('//title/text()').extract()
['reddit: the front page of the internet']

&amp;gt;&amp;gt;&amp;gt; request = request.replace(method=&quot;POST&quot;)

&amp;gt;&amp;gt;&amp;gt; fetch(request)

&amp;gt;&amp;gt;&amp;gt; response.status
404

&amp;gt;&amp;gt;&amp;gt; from pprint import pprint

&amp;gt;&amp;gt;&amp;gt; pprint(response.headers)
{'Accept-Ranges': ['bytes'],
 'Cache-Control': ['max-age=0, must-revalidate'],
 'Content-Type': ['text/html; charset=UTF-8'],
 'Date': ['Thu, 08 Dec 2016 16:21:19 GMT'],
 'Server': ['snooserv'],
 'Set-Cookie': ['loid=KqNLou0V9SKMX4qb4n; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure',
                'loidcreated=2016-12-08T16%3A21%3A19.445Z; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure',
                'loid=vi0ZVe4NkxNWdlH7r7; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure',
                'loidcreated=2016-12-08T16%3A21%3A19.459Z; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure'],
 'Vary': ['accept-encoding'],
 'Via': ['1.1 varnish'],
 'X-Cache': ['MISS'],
 'X-Cache-Hits': ['0'],
 'X-Content-Type-Options': ['nosniff'],
 'X-Frame-Options': ['SAMEORIGIN'],
 'X-Moose': ['majestic'],
 'X-Served-By': ['cache-cdg8730-CDG'],
 'X-Timer': ['S1481214079.394283,VS0,VE159'],
 'X-Ua-Compatible': ['IE=edge'],
 'X-Xss-Protection': ['1; mode=block']}
&amp;gt;&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;shell-4&quot;&gt;从爬虫调用shell检查响应&lt;/h2&gt;

&lt;p&gt;有时候，你想检查在爬虫的某一点被处理的响应，如果只检查你期望的响应到达那里。&lt;/p&gt;

&lt;p&gt;这可以通过使用该&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy.shell.inspect_response&lt;/code&gt;功能来实现。&lt;/p&gt;

&lt;p&gt;下面是一个如何从爬虫调用它的例子：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import scrapy


class MySpider(scrapy.Spider):
    name = &quot;myspider&quot;
    start_urls = [
        &quot;http://example.com&quot;,
        &quot;http://example.org&quot;,
        &quot;http://example.net&quot;,
    ]

    def parse(self, response):
        # We want to inspect one specific response.
        if &quot;.org&quot; in response.url:
            from scrapy.shell import inspect_response
            inspect_response(response, self)

        # Rest of parsing code.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;当你运行爬虫，你会得到类似的东西：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;2014-01-23 17:48:31-0400 [scrapy.core.engine] DEBUG: Crawled (200) &amp;lt;GET http://example.com&amp;gt; (referer: None)
2014-01-23 17:48:31-0400 [scrapy.core.engine] DEBUG: Crawled (200) &amp;lt;GET http://example.org&amp;gt; (referer: None)
[s] Available Scrapy objects:
[s]   crawler    &amp;lt;scrapy.crawler.Crawler object at 0x1e16b50&amp;gt;
...

&amp;gt;&amp;gt;&amp;gt; response.url
'http://example.org'
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;然后，您可以检查提取代码是否正常工作：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; response.xpath('//h1[@class=&quot;fn&quot;]')
[]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;不，不是。因此，您可以在Web浏览器中打开响应，看看它是否是您期望的响应：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; view(response)
True
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;最后，您按Ctrl-D（或Windows中的Ctrl-Z）退出外壳并继续抓取：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; ^D
2014-01-23 17:50:03-0400 [scrapy.core.engine] DEBUG: Crawled (200) &amp;lt;GET http://example.net&amp;gt; (referer: None)
...
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;请注意，您不能使用fetch此处的快捷方式，因为Scrapy引擎被shell阻止。然而，在你离开shell之后，爬虫会继续爬到它停止的地方，如上图所示。&lt;/p&gt;
</content>
<category term="Scrapy" />
<summary>Scrapy爬虫入门教程八 交互式 shell 方便调试</summary>
</entry>
<entry>
<title>Scrapy爬虫入门教程七 Item Loaders（项目加载器）</title>
<link href="http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E4%B8%83-item-loaders-%E9%A1%B9%E7%9B%AE%E5%8A%A0%E8%BD%BD%E5%99%A8/" rel="alternate" type="text/html" title="Scrapy爬虫入门教程七 Item Loaders（项目加载器）" />
<published>2017-04-09T14:31:00+08:00</published>
<updated>2017-04-09T14:31:00+08:00</updated>
<id>http://localhost:4000/scrapy爬虫入门教程七-item-loaders（项目加载器）</id>
<content type="html" xml:base="http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E4%B8%83-item-loaders-%E9%A1%B9%E7%9B%AE%E5%8A%A0%E8%BD%BD%E5%99%A8/">&lt;h1 id=&quot;scrapy-item-loaders&quot;&gt;Scrapy爬虫入门教程七 Item Loaders（项目加载器）&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;开发环境：&lt;/strong&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;Python 3.6.0 版本&lt;/code&gt; （当前最新）
&lt;code class=&quot;highlighter-rouge&quot;&gt;Scrapy 1.3.2 版本&lt;/code&gt; （当前最新）&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;项目加载器&lt;/h1&gt;

&lt;p&gt;项目加载器提供了一种方便的机制来填充抓取的&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/items.html#topics-items&quot;&gt;项目&lt;/a&gt;。即使可以使用自己的类似字典的API填充项目，项目加载器提供了一个更方便的API，通过自动化一些常见的任务，如解析原始提取的数据，然后分配它从剪贴过程中填充他们。&lt;/p&gt;

&lt;p&gt;换句话说，&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/items.html#topics-items&quot;&gt;Items&lt;/a&gt;提供了抓取数据的容器，而Item Loader提供了填充该容器的机制。&lt;/p&gt;

&lt;p&gt;项目加载器旨在提供一种灵活，高效和容易的机制，通过爬虫或源格式（HTML，XML等）扩展和覆盖不同的字段解析规则，而不会成为维护的噩梦。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;使用装载机项目来填充的项目&lt;/h2&gt;

&lt;p&gt;要使用项目加载器，您必须首先实例化它。您可以使用类似dict的对象（例如Item或dict）实例化它，也可以不使用它，在这种情况下，项目将在Item Loader构造函数中使用属性中指定的Item类自动&lt;code class=&quot;highlighter-rouge&quot;&gt;ItemLoader.default_item_class&lt;/code&gt; 实例化。&lt;/p&gt;

&lt;p&gt;然后，您开始收集值到项装载程序，通常使用&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/selectors.html#topics-selectors&quot;&gt;选择器&lt;/a&gt;。您可以向同一项目字段添加多个值; 项目加载器将知道如何使用适当的处理函数“加入”这些值。&lt;/p&gt;

&lt;p&gt;这里是&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/spiders.html#topics-spiders&quot;&gt;Spider&lt;/a&gt;中典型的Item Loader用法，使用&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/items.html#topics-items&quot;&gt;Items部分&lt;/a&gt;中声明的&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/items.html#topics-items-declaring&quot;&gt;Product&lt;/a&gt;项：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from scrapy.loader import ItemLoader
from myproject.items import Product

def parse(self, response):
    l = ItemLoader(item=Product(), response=response)
    l.add_xpath('name', '//div[@class=&quot;product_name&quot;]')
    l.add_xpath('name', '//div[@class=&quot;product_title&quot;]')
    l.add_xpath('price', '//p[@id=&quot;price&quot;]')
    l.add_css('stock', 'p#stock]')
    l.add_value('last_updated', 'today') # you can also use literal values
    return l.load_item()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;通过快速查看该代码，我们可以看到该&lt;code class=&quot;highlighter-rouge&quot;&gt;name&lt;/code&gt;字段正从页面中两个不同的XPath位置提取：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;//div[@class=&quot;product_name&quot;]&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;//div[@class=&quot;product_title&quot;]&lt;/code&gt;
换句话说，通过使用&lt;code class=&quot;highlighter-rouge&quot;&gt;add_xpath()&lt;/code&gt;方法从两个&lt;code class=&quot;highlighter-rouge&quot;&gt;XPath&lt;/code&gt;位置提取数据来收集数据。这是稍后将分配给name字段的数据。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;之后，类似的调用用于&lt;code class=&quot;highlighter-rouge&quot;&gt;price&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;stock&lt;/code&gt;字段（后者使用带有&lt;code class=&quot;highlighter-rouge&quot;&gt;add_css()&lt;/code&gt;方法的CSS选择器），最后使用不同的方法last_update直接使用文字值（today）填充字段add_value()。&lt;/p&gt;

&lt;p&gt;最后，收集的所有数据时，该&lt;code class=&quot;highlighter-rouge&quot;&gt;ItemLoader.load_item()&lt;/code&gt;方法被称为实际上返回填充先前提取并与收集到的数据的项目&lt;code class=&quot;highlighter-rouge&quot;&gt;add_xpath()&lt;/code&gt;， &lt;code class=&quot;highlighter-rouge&quot;&gt;add_css()&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;add_value()&lt;/code&gt;调用。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;输入和输出处理器&lt;/h2&gt;

&lt;p&gt;项目加载器对于每个（项目）字段包含一个输入处理器和一个输出处理器。输入处理器只要它的接收处理所提取的数据（通过add_xpath()，add_css()或 add_value()方法）和输入处理器的结果被收集并保持ItemLoader内部。收集所有数据后，ItemLoader.load_item()调用该 方法来填充和获取填充 Item对象。这是当输出处理器使用先前收集的数据（并使用输入处理器处理）调用时。输出处理器的结果是分配给项目的最终值。&lt;/p&gt;

&lt;p&gt;让我们看一个例子来说明如何为特定字段调用输入和输出处理器（同样适用于任何其他字段）：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;l = ItemLoader(Product(), some_selector)
l.add_xpath('name', xpath1) # (1)
l.add_xpath('name', xpath2) # (2)
l.add_css('name', css) # (3)
l.add_value('name', 'test') # (4)
return l.load_item() # (5)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;所以会发生什么：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;从数据xpath1提取出来，并通过所传递的输入处理器的的name字段。输入处理器的结果被收集并保存在项目加载器中（但尚未分配给项目）。&lt;/li&gt;
  &lt;li&gt;从中xpath2提取数据，并通过（1）中使用的同一输入处理器。输入处理器的结果附加到（1）中收集的数据（如果有）。&lt;/li&gt;
  &lt;li&gt;这种情况类似于先前的情况，除了数据从cssCSS选择器提取，并且通过在（1）和（2）中使用的相同的输入处理器。输入处理器的结果附加到在（1）和（2）中收集的数据（如果有的话）。&lt;/li&gt;
  &lt;li&gt;这种情况也与之前的类似，除了要收集的值直接分配，而不是从XPath表达式或CSS选择器中提取。但是，该值仍然通过输入处理器。在这种情况下，由于该值不可迭代，因此在将其传递给输入处理器之前，它将转换为单个元素的可迭代，因为输入处理器总是接收迭代。&lt;/li&gt;
  &lt;li&gt;在步骤（1），（2），（3）和（4）中收集的数据通过name字段的输出处理器。输出处理器的结果是分配给name 项目中字段的值。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;值得注意的是，处理器只是可调用对象，它们使用要解析的数据调用，并返回解析的值。所以你可以使用任何功能作为输入或输出处理器。唯一的要求是它们必须接受一个（也只有一个）位置参数，这将是一个迭代器。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;输入和输出处理器都必须接收一个迭代器作为它们的第一个参数。这些函数的输出可以是任何东西。输入处理器的结果将附加到包含收集的值（对于该字段）的内部列表（在加载程序中）。输出处理器的结果是最终分配给项目的值。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;另一件需要记住的事情是，输入处理器返回的值在内部（在列表中）收集，然后传递到输出处理器以填充字段。&lt;/p&gt;

&lt;p&gt;最后，但并非最不重要的是，Scrapy自带一些常用的处理器内置的方便。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;声明项目加载器&lt;/h2&gt;

&lt;p&gt;项目加载器通过使用类定义语法声明为Items。这里是一个例子：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from scrapy.loader import ItemLoader
from scrapy.loader.processors import TakeFirst, MapCompose, Join

class ProductLoader(ItemLoader):

    default_output_processor = TakeFirst()

    name_in = MapCompose(unicode.title)
    name_out = Join()

    price_in = MapCompose(unicode.strip)

    # ...
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;可以看到，输入处理器使用_in后缀声明，而输出处理器使用_out后缀声明。您还可以使用ItemLoader.default_input_processor和 ItemLoader.default_output_processor属性声明默认输入/输出 处理器。&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;声明输入和输出处理器&lt;/h2&gt;

&lt;p&gt;如上一节所述，输入和输出处理器可以在Item Loader定义中声明，这种方式声明输入处理器是很常见的。但是，还有一个地方可以指定要使用的输入和输出处理器：在项目字段 元数据中。这里是一个例子：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import scrapy
from scrapy.loader.processors import Join, MapCompose, TakeFirst
from w3lib.html import remove_tags

def filter_price(value):
    if value.isdigit():
        return value

class Product(scrapy.Item):
    name = scrapy.Field(
        input_processor=MapCompose(remove_tags),
        output_processor=Join(),
    )
    price = scrapy.Field(
        input_processor=MapCompose(remove_tags, filter_price),
        output_processor=TakeFirst(),
    )

&amp;gt;&amp;gt;&amp;gt; from scrapy.loader import ItemLoader
&amp;gt;&amp;gt;&amp;gt; il = ItemLoader(item=Product())
&amp;gt;&amp;gt;&amp;gt; il.add_value('name', [u'Welcome to my', u'&amp;lt;strong&amp;gt;website&amp;lt;/strong&amp;gt;'])
&amp;gt;&amp;gt;&amp;gt; il.add_value('price', [u'€', u'&amp;lt;span&amp;gt;1000&amp;lt;/span&amp;gt;'])
&amp;gt;&amp;gt;&amp;gt; il.load_item()
{'name': u'Welcome to my website', 'price': u'1000'}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;输入和输出处理器的优先级顺序如下：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;项目加载程序字段特定属性：field_in和field_out（最高优先级）&lt;/li&gt;
  &lt;li&gt;字段元数据（input_processor和output_processor键）&lt;/li&gt;
  &lt;li&gt;项目加载器默认值：ItemLoader.default_input_processor()和 ItemLoader.default_output_processor()（最低优先级）&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;参见：&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/loaders.html#topics-loaders-extending&quot;&gt;重用和扩展项目加载器&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&quot;section-5&quot;&gt;项目加载器上下文&lt;/h2&gt;

&lt;p&gt;项目加载器上下文是在项目加载器中的所有输入和输出处理器之间共享的任意键/值的dict。它可以在声明，实例化或使用Item Loader时传递。它们用于修改输入/输出处理器的行为。&lt;/p&gt;

&lt;p&gt;例如，假设您有一个parse_length接收文本值并从中提取长度的函数：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def  parse_length （text ， loader_context ）：
    unit  =  loader_context 。get （'unit' ， 'm' ）
    ＃...长度解析代码在这里...
    return  parsed_length
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;通过接受一个loader_context参数，该函数显式地告诉Item Loader它能够接收一个Item Loader上下文，因此Item Loader在调用它时传递当前活动的上下文，因此处理器功能（parse_length在这种情况下）可以使用它们。&lt;/p&gt;

&lt;p&gt;有几种方法可以修改Item Loader上下文值：&lt;/p&gt;

&lt;p&gt;1.
通过修改当前活动的Item Loader上下文（context属性）：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; loader = ItemLoader(product)
 loader.context['unit'] = 'cm'
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;2.
On Item Loader实例化（Item Loader构造函数的关键字参数存储在Item Loader上下文中）：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; loader = ItemLoader(product, unit='cm')
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;3.
On Item Loader声明，对于那些支持使用Item Loader上下文实例化的输入/输出处理器。MapCompose是其中之一：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; class ProductLoader(ItemLoader):
     length_out = MapCompose(parse_length, unit='cm')
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;itemloader&quot;&gt;ItemLoader对象&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class scrapy.loader.ItemLoader([item, selector, response, ]**kwargs)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;返回一个新的Item Loader来填充给定的Item。如果没有给出项目，则使用中的类自动实例化 default_item_class。&lt;/p&gt;

&lt;p&gt;当使用选择器或响应参数实例化时，ItemLoader类提供了使用选择器从网页提取数据的方便的机制。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;参数：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;item（Item对象）-项目实例来填充用以后调用 add_xpath()，add_css()或add_value()。&lt;/li&gt;
  &lt;li&gt;selector（Selectorobject） - 当使用add_xpath()（或。add_css()）或replace_xpath() （或replace_css()）方法时，从中提取数据的选择器 。&lt;/li&gt;
  &lt;li&gt;response（Responseobject） - 用于使用构造选择器的响应 default_selector_class，除非给出选择器参数，在这种情况下，将忽略此参数。
项目，选择器，响应和剩余的关键字参数被分配给Loader上下文（可通过context属性访问）。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;itemloader-&quot;&gt;ItemLoader 实例有以下方法：&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;get_value（value，* processors，** kwargs ）&lt;/code&gt;
处理给定value的给定processors和关键字参数。&lt;/p&gt;

&lt;p&gt;可用的关键字参数：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;参数：    re（str 或compiled regex）&lt;/strong&gt;
一个正则表达式extract_regex()，用于使用方法从给定值提取数据，在处理器之前应用
例子：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from scrapy.loader.processors import TakeFirst
&amp;gt;&amp;gt;&amp;gt; loader.get_value(u'name: foo', TakeFirst(), unicode.upper, re='name: (.+)')
'FOO`
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;add_value（field_name，value，* processors，** kwargs ）&lt;/code&gt;
处理，然后添加给value定字段的给定。&lt;/p&gt;

&lt;p&gt;该值首先通过get_value()赋予 processors和kwargs，然后通过 字段输入处理器及其结果追加到为该字段收集的数据。如果字段已包含收集的数据，则会添加新数据。&lt;/p&gt;

&lt;p&gt;给定field_name可以是None，在这种情况下可以添加多个字段的值。并且已处理的值应为一个字段，其中field_name映射到值。&lt;/p&gt;

&lt;p&gt;例子：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;loader.add_value('name', u'Color TV')
loader.add_value('colours', [u'white', u'blue'])
loader.add_value('length', u'100')
loader.add_value('name', u'name: foo', TakeFirst(), re='name: (.+)')
loader.add_value(None, {'name': u'foo', 'sex': u'male'})
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;replace_value（field_name，value，* processors，** kwargs ）&lt;/code&gt;
类似于add_value()但是用新值替换收集的数据，而不是添加它。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;get_xpath（xpath，* processors，** kwargs ）&lt;/code&gt;
类似于ItemLoader.get_value()但接收XPath而不是值，用于从与此相关联的选择器提取unicode字符串的列表ItemLoader。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;参数：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;xpath（str） - 从中​​提取数据的XPath&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;re（str 或compiled regex） - 用于从所选XPath区域提取数据的正则表达式
例子：&lt;/p&gt;

    &lt;p&gt;# HTML snippet: &amp;lt;p class=&quot;product-name&quot;&amp;gt;Color TV&amp;lt;/p&amp;gt;
  loader.get_xpath(‘//p[@class=”product-name”]’)
  # HTML snippet: &amp;lt;p id=&quot;price&quot;&amp;gt;the price is $1200&amp;lt;/p&amp;gt;
  loader.get_xpath(‘//p[@id=”price”]’, TakeFirst(), re=’the price is (.*)’)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;add_xpath（field_name，xpath，* processor，** kwargs ）&lt;/code&gt;
类似于ItemLoader.add_value()但接收XPath而不是值，用于从与此相关联的选择器提取unicode字符串的列表ItemLoader。&lt;/p&gt;

&lt;p&gt;见get_xpath()的kwargs。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;参数：&lt;/strong&gt;
xpath（str） - 从中​​提取数据的XPath&lt;/p&gt;

&lt;p&gt;例子：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# HTML snippet: &amp;lt;p class=&quot;product-name&quot;&amp;gt;Color TV&amp;lt;/p&amp;gt;
loader.add_xpath('name', '//p[@class=&quot;product-name&quot;]')
# HTML snippet: &amp;lt;p id=&quot;price&quot;&amp;gt;the price is $1200&amp;lt;/p&amp;gt;
loader.add_xpath('price', '//p[@id=&quot;price&quot;]', re='the price is (.*)')
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;replace_xpath（field_name，xpath，* processor，** kwargs ）&lt;/code&gt;
类似于add_xpath()但替换收集的数据，而不是添加它。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;get_css（css，* processors，** kwargs ）&lt;/code&gt;
类似于ItemLoader.get_value()但接收一个CSS选择器而不是一个值，用于从与此相关的选择器提取一个unicode字符串列表ItemLoader。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;参数：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;css（str） - 从中​​提取数据的CSS选择器&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;re（str 或compiled regex） - 用于从所选CSS区域提取数据的正则表达式
例子：&lt;/p&gt;

    &lt;p&gt;# HTML snippet: &amp;lt;p class=&quot;product-name&quot;&amp;gt;Color TV&amp;lt;/p&amp;gt;
  loader.get_css(‘p.product-name’)
  # HTML snippet: &amp;lt;p id=&quot;price&quot;&amp;gt;the price is $1200&amp;lt;/p&amp;gt;
  loader.get_css(‘p#price’, TakeFirst(), re=’the price is (.*)’)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;add_css（field_name，css，* processors，** kwargs ）&lt;/code&gt;
类似于ItemLoader.add_value()但接收一个CSS选择器而不是一个值，用于从与此相关的选择器提取一个unicode字符串列表ItemLoader。&lt;/p&gt;

&lt;p&gt;见get_css()的kwargs。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;参数：&lt;/strong&gt;
css（str） - 从中​​提取数据的CSS选择器
例子：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# HTML snippet: &amp;lt;p class=&quot;product-name&quot;&amp;gt;Color TV&amp;lt;/p&amp;gt;
loader.add_css('name', 'p.product-name')
# HTML snippet: &amp;lt;p id=&quot;price&quot;&amp;gt;the price is $1200&amp;lt;/p&amp;gt;
loader.add_css('price', 'p#price', re='the price is (.*)')
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;replace_css（field_name，css，* processors，** kwargs ）&lt;/code&gt;
类似于add_css()但替换收集的数据，而不是添加它。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;load_item（）&lt;/code&gt;
使用目前收集的数据填充项目，并返回。收集的数据首先通过输出处理器，以获得要分配给每个项目字段的最终值。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;nested_xpath（xpath ）&lt;/code&gt;
使用xpath选择器创建嵌套加载器。所提供的选择器应用于与此相关的选择器ItemLoader。嵌套装载机股份Item 与母公司ItemLoader这么调用add_xpath()， add_value()，replace_value()等会像预期的那样。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;nested_css（css ）&lt;/code&gt;
使用css选择器创建嵌套加载器。所提供的选择器应用于与此相关的选择器ItemLoader。嵌套装载机股份Item 与母公司ItemLoader这么调用add_xpath()， add_value()，replace_value()等会像预期的那样。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;get_collected_values（field_name ）&lt;/code&gt;
返回给定字段的收集值。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;get_output_value（field_name ）&lt;/code&gt;
返回给定字段使用输出处理器解析的收集值。此方法根本不填充或修改项目。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;get_input_processor（field_name ）&lt;/code&gt;
返回给定字段的输入处理器。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;get_output_processor（field_name ）&lt;/code&gt;
返回给定字段的输出处理器。&lt;/p&gt;

&lt;h3 id=&quot;itemloader--1&quot;&gt;ItemLoader 实例具有以下属性：&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;item&lt;/code&gt;
Item此项目加载器解析的对象。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;context&lt;/code&gt;
此项目Loader 的当前活动上下文。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;default_item_class&lt;/code&gt;
Item类（或工厂），用于在构造函数中未给出时实例化项。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;default_input_processor&lt;/code&gt;
用于不指定一个字段的字段的默认输入处理器。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;default_output_processor&lt;/code&gt;
用于不指定一个字段的字段的默认输出处理器。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;default_selector_class&lt;/code&gt;
所使用的类构造selector的此 ItemLoader，如果只响应在构造函数给出。如果在构造函数中给出了选择器，则忽略此属性。此属性有时在子类中被覆盖。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;selector&lt;/code&gt;
Selector从中提取数据的对象。它是在构造函数中给出的选择器，或者是从构造函数中使用的给定的响应创建的 default_selector_class。此属性意味着是只读的。&lt;/p&gt;

&lt;h2 id=&quot;section-6&quot;&gt;嵌套装载器&lt;/h2&gt;

&lt;p&gt;当解析来自文档的子部分的相关值时，创建嵌套加载器可能是有用的。假设您从页面的页脚中提取细节，看起来像：&lt;/p&gt;

&lt;p&gt;例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;footer&amp;gt;
    &amp;lt;a class=&quot;social&quot; href=&quot;http://facebook.com/whatever&quot;&amp;gt;Like Us&amp;lt;/a&amp;gt;
    &amp;lt;a class=&quot;social&quot; href=&quot;http://twitter.com/whatever&quot;&amp;gt;Follow Us&amp;lt;/a&amp;gt;
    &amp;lt;a class=&quot;email&quot; href=&quot;mailto:whatever@example.com&quot;&amp;gt;Email Us&amp;lt;/a&amp;gt;
&amp;lt;/footer&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;如果没有嵌套加载器，则需要为要提取的每个值指定完整的xpath（或css）。&lt;/p&gt;

&lt;p&gt;例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;loader = ItemLoader(item=Item())
# load stuff not in the footer
loader.add_xpath('social', '//footer/a[@class = &quot;social&quot;]/@href')
loader.add_xpath('email', '//footer/a[@class = &quot;email&quot;]/@href')
loader.load_item()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;相反，您可以使用页脚选择器创建嵌套加载器，并相对于页脚添加值。功能是相同的，但您避免重复页脚选择器。&lt;/p&gt;

&lt;p&gt;例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;loader = ItemLoader(item=Item())
# load stuff not in the footer
footer_loader = loader.nested_xpath('//footer')
footer_loader.add_xpath('social', 'a[@class = &quot;social&quot;]/@href')
footer_loader.add_xpath('email', 'a[@class = &quot;email&quot;]/@href')
# no need to call footer_loader.load_item()
loader.load_item()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;您可以任意嵌套加载器，并且可以使用xpath或css选择器。作为一般的指导原则，当他们使你的代码更简单，但不要超越嵌套或使用解析器可能变得难以阅读使用嵌套加载程序。&lt;/p&gt;

&lt;h2 id=&quot;section-7&quot;&gt;重用和扩展项目加载器&lt;/h2&gt;

&lt;p&gt;随着你的项目越来越大，越来越多的爬虫，维护成为一个根本的问题，特别是当你必须处理每个爬虫的许多不同的解析规则，有很多异常，但也想重用公共处理器。&lt;/p&gt;

&lt;p&gt;项目加载器旨在减轻解析规则的维护负担，同时不会失去灵活性，同时提供了扩展和覆盖它们的方便的机制。因此，项目加载器支持传统的Python类继承，以处理特定爬虫（或爬虫组）的差异。&lt;/p&gt;

&lt;p&gt;例如，假设某个特定站点以三个短划线（例如）包含其产品名称，并且您不希望最终在最终产品名称中删除那些破折号。—Plasma TV—&lt;/p&gt;

&lt;p&gt;以下是如何通过重用和扩展默认产品项目Loader（ProductLoader）来删除这些破折号：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from scrapy.loader.processors import MapCompose
from myproject.ItemLoaders import ProductLoader

def strip_dashes(x):
    return x.strip('-')

class SiteSpecificLoader(ProductLoader):
    name_in = MapCompose(strip_dashes, ProductLoader.name_in)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;另一种扩展项目加载器可能非常有用的情况是，当您有多种源格式，例如XML和HTML。在XML版本中，您可能想要删除CDATA事件。下面是一个如何做的例子：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from scrapy.loader.processors import MapCompose
from myproject.ItemLoaders import ProductLoader
from myproject.utils.xml import remove_cdata

class XmlProductLoader(ProductLoader):
    name_in = MapCompose(remove_cdata, ProductLoader.name_in)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;这就是你通常扩展输入处理器的方式。&lt;/p&gt;

&lt;p&gt;对于输出处理器，更常见的是在字段元数据中声明它们，因为它们通常仅依赖于字段而不是每个特定站点解析规则（如输入处理器）。另请参见： 声明输入和输出处理器。&lt;/p&gt;

&lt;p&gt;还有许多其他可能的方法来扩展，继承和覆盖您的项目加载器，不同的项目加载器层次结构可能更适合不同的项目。Scrapy只提供了机制; 它不强加任何特定的组织你的Loader集合 - 这取决于你和你的项目的需要。&lt;/p&gt;

&lt;h3 id=&quot;section-8&quot;&gt;可用内置处理器&lt;/h3&gt;

&lt;p&gt;即使您可以使用任何可调用函数作为输入和输出处理器，Scrapy也提供了一些常用的处理器，如下所述。其中一些，像MapCompose（通常用作输入处理器）组成按顺序执行的几个函数的输出，以产生最终的解析值。&lt;/p&gt;

&lt;p&gt;下面是所有内置处理器的列表：&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;class scrapy.loader.processors.Identity&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;最简单的处理器，什么都不做。它返回原始值不变。它不接收任何构造函数参数，也不接受Loader上下文。&lt;/p&gt;

&lt;p&gt;例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from scrapy.loader.processors import Identity
&amp;gt;&amp;gt;&amp;gt; proc = Identity()
&amp;gt;&amp;gt;&amp;gt; proc(['one', 'two', 'three'])
['one', 'two', 'three']
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;class scrapy.loader.processors.TakeFirst&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;从接收到的值中返回第一个非空值/非空值，因此它通常用作单值字段的输出处理器。它不接收任何构造函数参数，也不接受Loader上下文。&lt;/p&gt;

&lt;p&gt;例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from scrapy.loader.processors import TakeFirst
&amp;gt;&amp;gt;&amp;gt; proc = TakeFirst()
&amp;gt;&amp;gt;&amp;gt; proc(['', 'one', 'two', 'three'])
'one'
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;class scrapy.loader.processors.Join(separator=u' ')&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;返回与构造函数中给定的分隔符联接的值，默认为。它不接受加载器上下文。u’ ‘&lt;/p&gt;

&lt;p&gt;当使用默认分隔符时，此处理器相当于以下功能： u’ ‘.join&lt;/p&gt;

&lt;p&gt;例子：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from scrapy.loader.processors import Join
&amp;gt;&amp;gt;&amp;gt; proc = Join()
&amp;gt;&amp;gt;&amp;gt; proc(['one', 'two', 'three'])
u'one two three'
&amp;gt;&amp;gt;&amp;gt; proc = Join('&amp;lt;br&amp;gt;')
&amp;gt;&amp;gt;&amp;gt; proc(['one', 'two', 'three'])
u'one&amp;lt;br&amp;gt;two&amp;lt;br&amp;gt;three'
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;class scrapy.loader.processors.Compose(*functions, **default_loader_context)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;由给定函数的组合构成的处理器。这意味着该处理器的每个输入值都被传递给第一个函数，并且该函数的结果被传递给第二个函数，依此类推，直到最后一个函数返回该处理器的输出值。&lt;/p&gt;

&lt;p&gt;默认情况下，停止进程None值。可以通过传递关键字参数来更改此行为stop_on_none=False。&lt;/p&gt;

&lt;p&gt;例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from scrapy.loader.processors import Compose
&amp;gt;&amp;gt;&amp;gt; proc = Compose(lambda v: v[0], str.upper)
&amp;gt;&amp;gt;&amp;gt; proc(['hello', 'world'])
'HELLO'
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;每个功能可以可选地接收loader_context参数。对于那些处理器，这个处理器将通过该参数传递当前活动的Loader上下文。&lt;/p&gt;

&lt;p&gt;在构造函数中传递的关键字参数用作传递给每个函数调用的默认Loader上下文值。但是，传递给函数的最后一个Loader上下文值将被当前可用该属性访问的当前活动Loader上下文ItemLoader.context() 覆盖。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;class scrapy.loader.processors.MapCompose(*functions, **default_loader_context)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;与处理器类似，由给定功能的组成构成的Compose处理器。与此处理器的区别在于内部结果在函数之间传递的方式，如下所示：&lt;/p&gt;

&lt;p&gt;该处理器的输入值被迭代，并且第一函数被应用于每个元素。这些函数调用的结果（每个元素一个）被连接以构造新的迭代，然后用于应用​​第二个函数，等等，直到最后一个函数被应用于收集的值列表的每个值远。最后一个函数的输出值被连接在一起以产生该处理器的输出。&lt;/p&gt;

&lt;p&gt;每个特定函数可以返回值或值列表，这些值通过应用于其他输入值的相同函数返回的值列表展平。函数也可以返回None，在这种情况下，该函数的输出将被忽略，以便在链上进行进一步处理。&lt;/p&gt;

&lt;p&gt;此处理器提供了一种方便的方法来组合只使用单个值（而不是iterables）的函数。由于这个原因， MapCompose处理器通常用作输入处理器，因为数据通常使用选择器的 extract()方法提取，选择器返回unicode字符串的列表。&lt;/p&gt;

&lt;p&gt;下面的例子应该说明它是如何工作的：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; def filter_world(x):
...     return None if x == 'world' else x
...
&amp;gt;&amp;gt;&amp;gt; from scrapy.loader.processors import MapCompose
&amp;gt;&amp;gt;&amp;gt; proc = MapCompose(filter_world, unicode.upper)
&amp;gt;&amp;gt;&amp;gt; proc([u'hello', u'world', u'this', u'is', u'scrapy'])
[u'HELLO, u'THIS', u'IS', u'SCRAPY']
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;与Compose处理器一样，函数可以接收Loader上下文，并且构造函数关键字参数用作默认上下文值。有关Compose更多信息，请参阅 处理器。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;class scrapy.loader.processors.SelectJmes(json_path)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;使用提供给构造函数的json路径查询值，并返回输出。需要运行jmespath（&lt;a href=&quot;https://github.com/jmespath/jmespath.py&quot;&gt;https://github.com/jmespath/jmespath.py&lt;/a&gt;）。该处理器一次只需要一个输入。&lt;/p&gt;

&lt;p&gt;例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from scrapy.loader.processors import SelectJmes, Compose, MapCompose
&amp;gt;&amp;gt;&amp;gt; proc = SelectJmes(&quot;foo&quot;) #for direct use on lists and dictionaries
&amp;gt;&amp;gt;&amp;gt; proc({'foo': 'bar'})
'bar'
&amp;gt;&amp;gt;&amp;gt; proc({'foo': {'bar': 'baz'}})
{'bar': 'baz'}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;使用Json：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import json
&amp;gt;&amp;gt;&amp;gt; proc_single_json_str = Compose(json.loads, SelectJmes(&quot;foo&quot;))
&amp;gt;&amp;gt;&amp;gt; proc_single_json_str('{&quot;foo&quot;: &quot;bar&quot;}')
u'bar'
&amp;gt;&amp;gt;&amp;gt; proc_json_list = Compose(json.loads, MapCompose(SelectJmes('foo')))
&amp;gt;&amp;gt;&amp;gt; proc_json_list('[{&quot;foo&quot;:&quot;bar&quot;}, {&quot;baz&quot;:&quot;tar&quot;}]')
[u'bar']
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</content>
<category term="Scrapy" />
<summary>Scrapy爬虫入门教程七 Item Loaders（项目加载器）</summary>
</entry>
<entry>
<title>Scrapy爬虫入门教程六 Items（项目）</title>
<link href="http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E5%85%AD-items-%E9%A1%B9%E7%9B%AE/" rel="alternate" type="text/html" title="Scrapy爬虫入门教程六 Items（项目）" />
<published>2017-04-09T14:30:00+08:00</published>
<updated>2017-04-09T14:30:00+08:00</updated>
<id>http://localhost:4000/scrapy爬虫入门教程六-items（项目）</id>
<content type="html" xml:base="http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E5%85%AD-items-%E9%A1%B9%E7%9B%AE/">&lt;h1 id=&quot;scrapy-items&quot;&gt;Scrapy爬虫入门教程六 Items（项目）&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;开发环境：&lt;/strong&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;Python 3.6.0 版本&lt;/code&gt; （当前最新）
&lt;code class=&quot;highlighter-rouge&quot;&gt;Scrapy 1.3.2 版本&lt;/code&gt; （当前最新）&lt;/p&gt;

&lt;h1 id=&quot;items&quot;&gt;Items&lt;/h1&gt;

&lt;p&gt;主要目标是从非结构化来源（通常是网页）提取结构化数据。Scrapy爬虫可以将提取的数据作为Python语句返回。虽然方便和熟悉，Python dicts缺乏结构：很容易在字段名称中输入错误或返回不一致的数据，特别是在与许多爬虫的大项目。&lt;/p&gt;

&lt;p&gt;要定义公共输出数据格式，Scrapy提供Item类。 Item对象是用于收集所抓取的数据的简单容器。它们提供了一个&lt;a href=&quot;https://docs.python.org/2/library/stdtypes.html#dict&quot;&gt;类似字典&lt;/a&gt;的 API，具有用于声明其可用字段的方便的语法。&lt;/p&gt;

&lt;p&gt;各种Scrapy组件使用项目提供的额外信息：导出器查看声明的字段以计算要导出的列，序列化可以使用项字段元数据trackref 定制，跟踪项实例以帮助查找内存泄漏（请参阅使用&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/leaks.html#topics-leaks-trackrefs&quot;&gt;trackref&lt;/a&gt;调试内存泄漏）等。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;声明项目&lt;/h2&gt;

&lt;p&gt;使用简单的类定义语法和Field 对象来声明项目。这里是一个例子：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import scrapy

class Product(scrapy.Item):
    name = scrapy.Field()
    price = scrapy.Field()
    stock = scrapy.Field()
    last_updated = scrapy.Field(serializer=str)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;熟悉Django的人会注意到Scrapy Items被声明为类似于Django Models，只是Scrapy Items比较简单，因为没有不同字段类型的概念。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;section-1&quot;&gt;项目字段&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Field&lt;/code&gt;对象用于为每个字段指定元数据。例如，&lt;code class=&quot;highlighter-rouge&quot;&gt;last_updated&lt;/code&gt;上面示例中所示的字段的序列化函数。&lt;/p&gt;

&lt;p&gt;您可以为每个字段指定任何种类的元数据。对Field对象接受的值没有限制。出于同样的原因，没有所有可用元数据键的参考列表。Field对象中定义的每个键可以由不同的组件使用，并且只有那些组件知道它。您也可以定义和使用Field项目中的任何其他 键，为您自己的需要。Field对象的主要目标 是提供一种在一个地方定义所有字段元数据的方法。通常，那些行为取决于每个字段的组件使用某些字段键来配置该行为。您必须参考他们的文档，以查看每个组件使用哪些元数据键。&lt;/p&gt;

&lt;p&gt;重要的是要注意，Field用于声明项目的对象不会被分配为类属性。相反，可以通过&lt;code class=&quot;highlighter-rouge&quot;&gt;Item.fields&lt;/code&gt;属性访问它们。&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;使用项目&lt;/h3&gt;

&lt;p&gt;下面是使用上面声明的Product项目对项目执行的常见任务的一些示例 。你会注意到API非常类似于dict API。&lt;/p&gt;

&lt;h4 id=&quot;section-3&quot;&gt;创建项目&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; product = Product(name='Desktop PC', price=1000)
&amp;gt;&amp;gt;&amp;gt; print product
Product(name='Desktop PC', price=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;section-4&quot;&gt;获取字段值&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; product['name']
Desktop PC
&amp;gt;&amp;gt;&amp;gt; product.get('name')
Desktop PC

&amp;gt;&amp;gt;&amp;gt; product['price']
1000

&amp;gt;&amp;gt;&amp;gt; product['last_updated']
Traceback (most recent call last):
    ...
KeyError: 'last_updated'

&amp;gt;&amp;gt;&amp;gt; product.get('last_updated', 'not set')
not set

&amp;gt;&amp;gt;&amp;gt; product['lala'] # getting unknown field
Traceback (most recent call last):
    ...
KeyError: 'lala'

&amp;gt;&amp;gt;&amp;gt; product.get('lala', 'unknown field')
'unknown field'

&amp;gt;&amp;gt;&amp;gt; 'name' in product  # is name field populated?
True

&amp;gt;&amp;gt;&amp;gt; 'last_updated' in product  # is last_updated populated?
False

&amp;gt;&amp;gt;&amp;gt; 'last_updated' in product.fields  # is last_updated a declared field?
True

&amp;gt;&amp;gt;&amp;gt; 'lala' in product.fields  # is lala a declared field?
False
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;section-5&quot;&gt;设置字段值&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; product [ 'last_updated' ]  =  'today'
&amp;gt;&amp;gt;&amp;gt; product [ 'last_updated' ]
today

&amp;gt;&amp;gt;&amp;gt; product [ 'lala' ]  =  'test'  ＃设置未知字段
Traceback（最近调用最后一次）：
    ...
KeyError：'产品不支持字段：lala'
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;section-6&quot;&gt;访问所有填充值&lt;/h4&gt;

&lt;p&gt;要访问所有填充值，只需使用典型的dict API：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; product.keys()
['price', 'name']

&amp;gt;&amp;gt;&amp;gt; product.items()
[('price', 1000), ('name', 'Desktop PC')]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;section-7&quot;&gt;其他常见任务&lt;/h4&gt;

&lt;p&gt;复制项目：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; product2 = Product(product)
&amp;gt;&amp;gt;&amp;gt; print product2
Product(name='Desktop PC', price=1000)

&amp;gt;&amp;gt;&amp;gt; product3 = product2.copy()
&amp;gt;&amp;gt;&amp;gt; print product3
Product(name='Desktop PC', price=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;section-8&quot;&gt;从项目创建词典：&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; dict(product) # create a dict from all populated values
{'price': 1000, 'name': 'Desktop PC'}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;section-9&quot;&gt;从短片创建项目：&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; Product({'name': 'Laptop PC', 'price': 1500})
Product(price=1500, name='Laptop PC')

&amp;gt;&amp;gt;&amp;gt; Product({'name': 'Laptop PC', 'lala': 1500}) # warning: unknown field in dict
Traceback (most recent call last):
    ...
KeyError: 'Product does not support field: lala'
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;section-10&quot;&gt;扩展项目&lt;/h3&gt;

&lt;p&gt;您可以通过声明原始项的子类来扩展项（以添加更多字段或更改某些字段的某些元数据）。&lt;/p&gt;

&lt;p&gt;例如：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class DiscountedProduct(Product):
    discount_percent = scrapy.Field(serializer=str)
    discount_expiration_date = scrapy.Field()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;您还可以通过使用先前的字段元数据并附加更多值或更改现有值来扩展字段元数据，如下所示：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class SpecificProduct(Product):
    name = scrapy.Field(Product.fields['name'], serializer=my_serializer)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;添加（或替换）字段的serializer元数据键name，保留所有以前存在的元数据值。&lt;/p&gt;

&lt;h4 id=&quot;section-11&quot;&gt;项目对象&lt;/h4&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;class scrapy.item.Item([arg])&lt;/code&gt;
返回一个可以从给定参数初始化的新项目。&lt;/p&gt;

&lt;p&gt;项目复制标准&lt;a href=&quot;https://docs.python.org/2/library/stdtypes.html#dict&quot;&gt;dict API&lt;/a&gt;，包括其构造函数。Items提供的唯一附加属性是：&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;fields&lt;/code&gt;
包含字典中的所有声明的字段为这个项目，不仅是那些填充。键是字段名称，值是&lt;code class=&quot;highlighter-rouge&quot;&gt;Field&lt;/code&gt;在&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/items.html#topics-items-declaring&quot;&gt;项目声明中&lt;/a&gt;使用的 对象。&lt;/p&gt;

&lt;h4 id=&quot;section-12&quot;&gt;字段对象&lt;/h4&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;class scrapy.item.Field([arg])&lt;/code&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;Field&lt;/code&gt;类只是一个别名内置字典类和不提供任何额外的功能或属性。换句话说， &lt;code class=&quot;highlighter-rouge&quot;&gt;Field&lt;/code&gt;对象是普通的&lt;code class=&quot;highlighter-rouge&quot;&gt;Python&lt;/code&gt;代码。单独的类用于支持 基于类属性的项声明语法。&lt;/p&gt;
</content>
<category term="Scrapy" />
<summary>Scrapy爬虫入门教程六 Items（项目）</summary>
</entry>
<entry>
<title>Scrapy爬虫入门教程五 Selectors（选择器）</title>
<link href="http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E4%BA%94-selectors-%E9%80%89%E6%8B%A9%E5%99%A8/" rel="alternate" type="text/html" title="Scrapy爬虫入门教程五 Selectors（选择器）" />
<published>2017-04-09T14:30:00+08:00</published>
<updated>2017-04-09T14:30:00+08:00</updated>
<id>http://localhost:4000/scrapy爬虫入门教程五-selectors（选择器）</id>
<content type="html" xml:base="http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E4%BA%94-selectors-%E9%80%89%E6%8B%A9%E5%99%A8/">&lt;h1 id=&quot;scrapy-selectors&quot;&gt;Scrapy爬虫入门教程五 Selectors（选择器）&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;开发环境：&lt;/strong&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;Python 3.6.0 版本&lt;/code&gt; （当前最新）
&lt;code class=&quot;highlighter-rouge&quot;&gt;Scrapy 1.3.2 版本&lt;/code&gt; （当前最新）&lt;/p&gt;

&lt;h1 id=&quot;selectors&quot;&gt;Selectors（选择器）&lt;/h1&gt;

&lt;p&gt;当您抓取网页时，您需要执行的最常见任务是从HTML源中提取数据。有几个库可以实现这一点：&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.crummy.com/software/BeautifulSoup/&quot;&gt;BeautifulSoup&lt;/a&gt;是Python程序员中非常流行的网络抓取库，它基于HTML代码的结构构建一个Python对象，并且处理相当糟糕的标记，但它有一个缺点：它很慢。
&lt;a href=&quot;http://lxml.de/&quot;&gt;lxml&lt;/a&gt;是一个XML解析库（它还解析HTML）与基于&lt;a href=&quot;https://docs.python.org/2/library/xml.etree.elementtree.html&quot;&gt;ElementTree&lt;/a&gt;的pythonic API 。（lxml不是Python标准库的一部分。）
Scrapy自带了提取数据的机制。它们称为选择器，因为它们“选择”由&lt;a href=&quot;https://www.w3.org/TR/xpath&quot;&gt;XPath&lt;/a&gt;或&lt;a href=&quot;https://www.w3.org/TR/selectors&quot;&gt;CSS&lt;/a&gt;表达式指定的HTML文档的某些部分。&lt;/p&gt;

&lt;p&gt;XPath是用于选择XML文档中的节点的语言，其也可以与HTML一起使用。CSS是一种用于将样式应用于HTML文档的语言。它定义了选择器以将这些样式与特定的HTML元素相关联。&lt;/p&gt;

&lt;p&gt;Scrapy选择器构建在lxml库之上，这意味着它们的速度和解析精度非常相似。&lt;/p&gt;

&lt;p&gt;这个页面解释了选择器是如何工作的，并描述了他们的API是非常小和简单，不像lxml API是更大，因为 lxml库可以用于许多其他任务，除了选择标记文档。&lt;/p&gt;

&lt;p&gt;有关&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/selectors.html#topics-selectors-ref&quot;&gt;选择器 API&lt;/a&gt;的完整参考，请参阅 &lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/selectors.html#topics-selectors-ref&quot;&gt;选择器引用&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;使用选择器&lt;/h2&gt;

&lt;h3 id=&quot;section-1&quot;&gt;构造选择器&lt;/h3&gt;

&lt;p&gt;Scrapy选择器是Selector通过传递文本或TextResponse 对象构造的类的实例。它根据输入类型自动选择最佳的解析规则（XML与HTML）：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from scrapy.selector import Selector
&amp;gt;&amp;gt;&amp;gt; from scrapy.http import HtmlResponse
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;从文本构造：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; body = '&lt;span class=&quot;nt&quot;&gt;&amp;lt;html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;span&amp;gt;&lt;/span&gt;good&lt;span class=&quot;nt&quot;&gt;&amp;lt;/span&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;&lt;/span&gt;'
&amp;gt;&amp;gt;&amp;gt; Selector(text=body).xpath('//span/text()').extract()
[u'good']
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;构建响应：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; response = HtmlResponse(url='http://example.com', body=body)
&amp;gt;&amp;gt;&amp;gt; Selector(response=response).xpath('//span/text()').extract()
[u'good']
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;为了方便起见，响应对象在.selector属性上显示一个选择器，在可能的情况下使用此快捷键是完全正确的：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; response.selector.xpath('//span/text()').extract()
[u'good']
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;section-2&quot;&gt;使用选择器&lt;/h3&gt;

&lt;p&gt;为了解释如何使用选择器，我们将使用Scrapy shell（提供交互式测试）和位于Scrapy文档服务器中的示例页面：&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://doc.scrapy.org/en/latest/_static/selectors-sample1.html&quot;&gt;http://doc.scrapy.org/en/latest/_static/selectors-sample1.html&lt;/a&gt;
这里是它的HTML代码：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;html&amp;gt;&lt;/span&gt;
 &lt;span class=&quot;nt&quot;&gt;&amp;lt;head&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;base&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'http://example.com/'&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;title&amp;gt;&lt;/span&gt;Example website&lt;span class=&quot;nt&quot;&gt;&amp;lt;/title&amp;gt;&lt;/span&gt;
 &lt;span class=&quot;nt&quot;&gt;&amp;lt;/head&amp;gt;&lt;/span&gt;
 &lt;span class=&quot;nt&quot;&gt;&amp;lt;body&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;id=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'images'&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
   &lt;span class=&quot;nt&quot;&gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'image1.html'&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;Name: My image 1 &lt;span class=&quot;nt&quot;&gt;&amp;lt;br&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;![](image1_thumb.jpg)&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;
   &lt;span class=&quot;nt&quot;&gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'image2.html'&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;Name: My image 2 &lt;span class=&quot;nt&quot;&gt;&amp;lt;br&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;![](image2_thumb.jpg)&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;
   &lt;span class=&quot;nt&quot;&gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'image3.html'&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;Name: My image 3 &lt;span class=&quot;nt&quot;&gt;&amp;lt;br&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;![](image3_thumb.jpg)&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;
   &lt;span class=&quot;nt&quot;&gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'image4.html'&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;Name: My image 4 &lt;span class=&quot;nt&quot;&gt;&amp;lt;br&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;![](image4_thumb.jpg)&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;
   &lt;span class=&quot;nt&quot;&gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'image5.html'&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;Name: My image 5 &lt;span class=&quot;nt&quot;&gt;&amp;lt;br&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;![](image5_thumb.jpg)&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
 &lt;span class=&quot;nt&quot;&gt;&amp;lt;/body&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/html&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;首先，让我们打开shell：
&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy shell http://doc.scrapy.org/en/latest/_static/selectors-sample1.html&lt;/code&gt;
然后，在加载shell之后，您将有可用的响应作为response shell变量，以及其附加的选择器response.selector属性。&lt;/p&gt;

&lt;p&gt;由于我们处理HTML，选择器将自动使用HTML解析器。&lt;/p&gt;

&lt;p&gt;因此，通过查看该页面的&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/selectors.html#topics-selectors-htmlcode&quot;&gt;HTML代码&lt;/a&gt;，让我们构造一个XPath来选择标题标签中的文本：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; response.selector.xpath('//title/text()')
[&amp;lt;Selector (text) xpath=//title/text()&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;使用XPath和CSS查询响应非常普遍，响应包括两个方便的快捷键：response.xpath()和response.css()：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; response.xpath('//title/text()')
[&amp;lt;Selector (text) xpath=//title/text()&amp;gt;]
&amp;gt;&amp;gt;&amp;gt; response.css('title::text')
[&amp;lt;Selector (text) xpath=//title/text()&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;正如你所看到的，.xpath()而.css()方法返回一个 SelectorList实例，它是新的选择列表。此API可用于快速选择嵌套数据：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; response.css('img').xpath('@src').extract()
[u'image1_thumb.jpg',
 u'image2_thumb.jpg',
 u'image3_thumb.jpg',
 u'image4_thumb.jpg',
 u'image5_thumb.jpg']
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;要实际提取文本数据，必须调用选择器.extract() 方法，如下所示：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; response.xpath('//title/text()').extract()
[u'Example website']
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;如果只想提取第一个匹配的元素，可以调用选择器 .extract_first()&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; response.xpath('//div[@id=&quot;images&quot;]/a/text()').extract_first()
u'Name: My image 1 '
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;None如果没有找到元素则返回：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; response.xpath('//div[@id=&quot;not-exists&quot;]/text()').extract_first() is None
True
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;可以提供默认返回值作为参数，而不是使用None：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; response.xpath('//div[@id=&quot;not-exists&quot;]/text()').extract_first(default='not-found')
'not-found'
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;请注意，CSS选择器可以使用CSS3伪元素选择文本或属性节点：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; response.css('title::text').extract()
[u'Example website']
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;现在我们要获取基本URL和一些图像链接：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; response.xpath('//base/@href').extract()
[u'http://example.com/']

&amp;gt;&amp;gt;&amp;gt; response.css('base::attr(href)').extract()
[u'http://example.com/']

&amp;gt;&amp;gt;&amp;gt; response.xpath('//a[contains(@href, &quot;image&quot;)]/@href').extract()
[u'image1.html',
 u'image2.html',
 u'image3.html',
 u'image4.html',
 u'image5.html']

&amp;gt;&amp;gt;&amp;gt; response.css('a[href*=image]::attr(href)').extract()
[u'image1.html',
 u'image2.html',
 u'image3.html',
 u'image4.html',
 u'image5.html']

&amp;gt;&amp;gt;&amp;gt; response.xpath('//a[contains(@href, &quot;image&quot;)]/img/@src').extract()
[u'image1_thumb.jpg',
 u'image2_thumb.jpg',
 u'image3_thumb.jpg',
 u'image4_thumb.jpg',
 u'image5_thumb.jpg']

&amp;gt;&amp;gt;&amp;gt; response.css('a[href*=image] img::attr(src)').extract()
[u'image1_thumb.jpg',
 u'image2_thumb.jpg',
 u'image3_thumb.jpg',
 u'image4_thumb.jpg',
 u'image5_thumb.jpg']
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;section-3&quot;&gt;嵌套选择器&lt;/h3&gt;

&lt;p&gt;选择方法（.xpath()或.css()）返回相同类型的选择器的列表，因此您也可以调用这些选择器的选择方法。这里有一个例子：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; links = response.xpath('//a[contains(@href, &quot;image&quot;)]')
&amp;gt;&amp;gt;&amp;gt; links.extract()
[u'&amp;lt;a href=&quot;image1.html&quot;&amp;gt;Name: My image 1 &amp;lt;br&amp;gt;![](image1_thumb.jpg)&amp;lt;/a&amp;gt;',
 u'&amp;lt;a href=&quot;image2.html&quot;&amp;gt;Name: My image 2 &amp;lt;br&amp;gt;![](image2_thumb.jpg)&amp;lt;/a&amp;gt;',
 u'&amp;lt;a href=&quot;image3.html&quot;&amp;gt;Name: My image 3 &amp;lt;br&amp;gt;![](image3_thumb.jpg)&amp;lt;/a&amp;gt;',
 u'&amp;lt;a href=&quot;image4.html&quot;&amp;gt;Name: My image 4 &amp;lt;br&amp;gt;![](image4_thumb.jpg)&amp;lt;/a&amp;gt;',
 u'&amp;lt;a href=&quot;image5.html&quot;&amp;gt;Name: My image 5 &amp;lt;br&amp;gt;![](image5_thumb.jpg)&amp;lt;/a&amp;gt;']

&amp;gt;&amp;gt;&amp;gt; for index, link in enumerate(links):
...     args = (index, link.xpath('@href').extract(), link.xpath('img/@src').extract())
...     print 'Link number %d points to url %s and image %s' % args

Link number 0 points to url [u'image1.html'] and image [u'image1_thumb.jpg']
Link number 1 points to url [u'image2.html'] and image [u'image2_thumb.jpg']
Link number 2 points to url [u'image3.html'] and image [u'image3_thumb.jpg']
Link number 3 points to url [u'image4.html'] and image [u'image4_thumb.jpg']
Link number 4 points to url [u'image5.html'] and image [u'image5_thumb.jpg']
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;section-4&quot;&gt;使用带有正则表达式的选择器&lt;/h4&gt;

&lt;p&gt;Selector也有一种.re()使用正则表达式提取数据的方法。但是，不同于使用 .xpath()或 .css()methods，.re()返回一个unicode字符串列表。所以你不能构造嵌套.re()调用。&lt;/p&gt;

&lt;p&gt;以下是用于从上面的HTML代码中提取图片名称的示例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; response.xpath('//a[contains(@href, &quot;image&quot;)]/text()').re(r'Name:\s*(.*)')
[u'My image 1',
 u'My image 2',
 u'My image 3',
 u'My image 4',
 u'My image 5']
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;这里有一个额外的辅助往复.extract_first()进行.re()，命名.re_first()。使用它只提取第一个匹配的字符串：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; response.xpath('//a[contains(@href, &quot;image&quot;)]/text()').re_first(r'Name:\s*(.*)')
u'My image 1'
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;xpath&quot;&gt;使用相对XPath&lt;/h4&gt;

&lt;p&gt;请记住，如果您嵌套选择器并使用以XPath开头的XPath /，该XPath将是绝对的文档，而不是相对于 Selector您调用它。&lt;/p&gt;

&lt;p&gt;例如，假设要提取&lt;/p&gt;

&lt;p&gt;元素中的所有&amp;lt;div&amp;gt; 元素。首先，你会得到所有的&amp;lt;div&amp;gt;元素：
&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;gt;&amp;gt;&amp;gt; divs = response.xpath('//div')&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;首先，你可能会使用下面的方法，这是错误的，因为它实际上&lt;/p&gt;

&lt;p&gt;从文档中提取所有元素，而不仅仅是那些内部&amp;lt;div&amp;gt;元素：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; for p in divs.xpath('//p'):  # this is wrong - gets all &amp;lt;p&amp;gt; from the whole document
...     print p.extract()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;这是正确的方式（注意点前面的.//pXPath 的点）：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; for p in divs.xpath('.//p'):  # extracts all &amp;lt;p&amp;gt; inside
...     print p.extract()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;另一个常见的情况是提取所有直接的\&lt;/p&gt;

&lt;p&gt;孩子：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; for p in divs.xpath('p'):
...     print p.extract()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;有关相对XPath的更多详细信息，请参阅XPath规范中的&lt;a href=&quot;https://www.w3.org/TR/xpath#location-paths&quot;&gt;位置路径部分&lt;/a&gt;。&lt;/p&gt;

&lt;h4 id=&quot;xpath-1&quot;&gt;XPath表达式中的变量&lt;/h4&gt;

&lt;p&gt;XPath允许您使用&lt;code class=&quot;highlighter-rouge&quot;&gt;$somevariable&lt;/code&gt;语法来引用XPath表达式中的变量。这在某种程度上类似于SQL世界中的参数化查询或预准备语句，您在查询中使用占位符替换一些参数，&lt;code class=&quot;highlighter-rouge&quot;&gt;?&lt;/code&gt;然后用查询传递的值替换。&lt;/p&gt;

&lt;p&gt;这里有一个例子来匹配元素基于其“id”属性值，没有硬编码它（如前所示）：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; # `$val` used in the expression, a `val` argument needs to be passed
&amp;gt;&amp;gt;&amp;gt; response.xpath('//div[@id=$val]/a/text()', val='images').extract_first()
u'Name: My image 1 '
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;这里是另一个例子，找到一个&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;div&amp;gt;&lt;/code&gt;标签的“id” 属性包含五个&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;a&amp;gt;&lt;/code&gt;孩子（这里我们传递的值&lt;code class=&quot;highlighter-rouge&quot;&gt;5&lt;/code&gt;作为一个整数）：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; response.xpath('//div[count(a)=$cnt]/@id', cnt=5).extract_first()
u'images'
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;所有变量引用在调用时必须有一个绑定值&lt;code class=&quot;highlighter-rouge&quot;&gt;.xpath()&lt;/code&gt;（否则你会得到一个异常）。这是通过传递必要的命名参数。&lt;code class=&quot;highlighter-rouge&quot;&gt;ValueError: XPath error:&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://parsel.readthedocs.io/&quot;&gt;parsel&lt;/a&gt;是为Scrapy选择器提供动力的库，有关于&lt;a href=&quot;https://parsel.readthedocs.io/en/latest/usage.html#variables-in-xpath-expressions&quot;&gt;XPath变量&lt;/a&gt;的更多详细信息和示例。&lt;/p&gt;

&lt;h4 id=&quot;exslt&quot;&gt;使用EXSLT扩展&lt;/h4&gt;

&lt;p&gt;在构建在&lt;a href=&quot;http://lxml.de/&quot;&gt;lxml&lt;/a&gt;之上时，Scrapy选择器还支持一些&lt;a href=&quot;http://exslt.org/&quot;&gt;EXSLT&lt;/a&gt;扩展，并带有这些预先注册的命名空间以在XPath表达式中使用：
字首命名空间&lt;span class=&quot;Apple-tab-span&quot; style=&quot;white-space:pre&quot;&gt;&lt;/span&gt;用法回覆&lt;a href=&quot;http://exslt.org/regular-expressions&quot;&gt;http://exslt.org/regular-expressions&lt;/a&gt;&lt;a href=&quot;http://exslt.org/regexp/index.html&quot;&gt;正则表达式&lt;/a&gt;组&lt;a href=&quot;http://exslt.org/sets&quot;&gt;http://exslt.org/sets&lt;/a&gt;&lt;a href=&quot;http://exslt.org/set/index.html&quot;&gt;设置操作&lt;/a&gt;
##### 正则表达式&lt;/p&gt;

&lt;p&gt;test()例如，当XPath starts-with()或者contains()不足时，该函数可以证明是非常有用的 。&lt;/p&gt;

&lt;p&gt;示例选择列表项中的链接，其中“类”属性以数字结尾：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from scrapy import Selector
&amp;gt;&amp;gt;&amp;gt; doc = &quot;&quot;&quot;
... &amp;lt;div&amp;gt;
...     &amp;lt;ul&amp;gt;
...         &amp;lt;li class=&quot;item-0&quot;&amp;gt;&amp;lt;a href=&quot;link1.html&quot;&amp;gt;first item&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;
...         &amp;lt;li class=&quot;item-1&quot;&amp;gt;&amp;lt;a href=&quot;link2.html&quot;&amp;gt;second item&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;
...         &amp;lt;li class=&quot;item-inactive&quot;&amp;gt;&amp;lt;a href=&quot;link3.html&quot;&amp;gt;third item&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;
...         &amp;lt;li class=&quot;item-1&quot;&amp;gt;&amp;lt;a href=&quot;link4.html&quot;&amp;gt;fourth item&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;
...         &amp;lt;li class=&quot;item-0&quot;&amp;gt;&amp;lt;a href=&quot;link5.html&quot;&amp;gt;fifth item&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;
...     &amp;lt;/ul&amp;gt;
... &amp;lt;/div&amp;gt;
... &quot;&quot;&quot;
&amp;gt;&amp;gt;&amp;gt; sel = Selector(text=doc, type=&quot;html&quot;)
&amp;gt;&amp;gt;&amp;gt; sel.xpath('//li//@href').extract()
[u'link1.html', u'link2.html', u'link3.html', u'link4.html', u'link5.html']
&amp;gt;&amp;gt;&amp;gt; sel.xpath('//li[re:test(@class, &quot;item-\d$&quot;)]//@href').extract()
[u'link1.html', u'link2.html', u'link4.html', u'link5.html']
&amp;gt;&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;警告&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;C库libxslt本身不支持EXSLT正则表达式，所以lxml的实现使用钩子到Python的re模块。因此，在XPath表达式中使用regexp函数可能会增加小的性能损失。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h5 id=&quot;section-5&quot;&gt;设置操作&lt;/h5&gt;

&lt;p&gt;这些可以方便地在提取文本元素之前排除文档树的部分。&lt;/p&gt;

&lt;p&gt;使用项目范围组和相应的itemprops组提取微数据（从&lt;a href=&quot;http://schema.org/Product&quot;&gt;http://schema.org/Product&lt;/a&gt;中提取的示例内容）示例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; doc = &quot;&quot;&quot;
... &amp;lt;div itemscope itemtype=&quot;http://schema.org/Product&quot;&amp;gt;
...   &amp;lt;span itemprop=&quot;name&quot;&amp;gt;Kenmore White 17&quot; Microwave&amp;lt;/span&amp;gt;
...   ![](kenmore-microwave-17in.jpg)
...   &amp;lt;div itemprop=&quot;aggregateRating&quot;
...     itemscope itemtype=&quot;http://schema.org/AggregateRating&quot;&amp;gt;
...    Rated &amp;lt;span itemprop=&quot;ratingValue&quot;&amp;gt;3.5&amp;lt;/span&amp;gt;/5
...    based on &amp;lt;span itemprop=&quot;reviewCount&quot;&amp;gt;11&amp;lt;/span&amp;gt; customer reviews
...   &amp;lt;/div&amp;gt;
...
...   &amp;lt;div itemprop=&quot;offers&quot; itemscope itemtype=&quot;http://schema.org/Offer&quot;&amp;gt;
...     &amp;lt;span itemprop=&quot;price&quot;&amp;gt;$55.00&amp;lt;/span&amp;gt;
...     &amp;lt;link itemprop=&quot;availability&quot; href=&quot;http://schema.org/InStock&quot; /&amp;gt;In stock
...   &amp;lt;/div&amp;gt;
...
...   Product description:
...   &amp;lt;span itemprop=&quot;description&quot;&amp;gt;0.7 cubic feet countertop microwave.
...   Has six preset cooking categories and convenience features like
...   Add-A-Minute and Child Lock.&amp;lt;/span&amp;gt;
...
...   Customer reviews:
...
...   &amp;lt;div itemprop=&quot;review&quot; itemscope itemtype=&quot;http://schema.org/Review&quot;&amp;gt;
...     &amp;lt;span itemprop=&quot;name&quot;&amp;gt;Not a happy camper&amp;lt;/span&amp;gt; -
...     by &amp;lt;span itemprop=&quot;author&quot;&amp;gt;Ellie&amp;lt;/span&amp;gt;,
...     &amp;lt;meta itemprop=&quot;datePublished&quot; content=&quot;2011-04-01&quot;&amp;gt;April 1, 2011
...     &amp;lt;div itemprop=&quot;reviewRating&quot; itemscope itemtype=&quot;http://schema.org/Rating&quot;&amp;gt;
...       &amp;lt;meta itemprop=&quot;worstRating&quot; content = &quot;1&quot;&amp;gt;
...       &amp;lt;span itemprop=&quot;ratingValue&quot;&amp;gt;1&amp;lt;/span&amp;gt;/
...       &amp;lt;span itemprop=&quot;bestRating&quot;&amp;gt;5&amp;lt;/span&amp;gt;stars
...     &amp;lt;/div&amp;gt;
...     &amp;lt;span itemprop=&quot;description&quot;&amp;gt;The lamp burned out and now I have to replace
...     it. &amp;lt;/span&amp;gt;
...   &amp;lt;/div&amp;gt;
...
...   &amp;lt;div itemprop=&quot;review&quot; itemscope itemtype=&quot;http://schema.org/Review&quot;&amp;gt;
...     &amp;lt;span itemprop=&quot;name&quot;&amp;gt;Value purchase&amp;lt;/span&amp;gt; -
...     by &amp;lt;span itemprop=&quot;author&quot;&amp;gt;Lucas&amp;lt;/span&amp;gt;,
...     &amp;lt;meta itemprop=&quot;datePublished&quot; content=&quot;2011-03-25&quot;&amp;gt;March 25, 2011
...     &amp;lt;div itemprop=&quot;reviewRating&quot; itemscope itemtype=&quot;http://schema.org/Rating&quot;&amp;gt;
...       &amp;lt;meta itemprop=&quot;worstRating&quot; content = &quot;1&quot;/&amp;gt;
...       &amp;lt;span itemprop=&quot;ratingValue&quot;&amp;gt;4&amp;lt;/span&amp;gt;/
...       &amp;lt;span itemprop=&quot;bestRating&quot;&amp;gt;5&amp;lt;/span&amp;gt;stars
...     &amp;lt;/div&amp;gt;
...     &amp;lt;span itemprop=&quot;description&quot;&amp;gt;Great microwave for the price. It is small and
...     fits in my apartment.&amp;lt;/span&amp;gt;
...   &amp;lt;/div&amp;gt;
...   ...
... &amp;lt;/div&amp;gt;
... &quot;&quot;&quot;
&amp;gt;&amp;gt;&amp;gt; sel = Selector(text=doc, type=&quot;html&quot;)
&amp;gt;&amp;gt;&amp;gt; for scope in sel.xpath('//div[@itemscope]'):
...     print &quot;current scope:&quot;, scope.xpath('@itemtype').extract()
...     props = scope.xpath('''
...                 set:difference(./descendant::*/@itemprop,
...                                .//*[@itemscope]/*/@itemprop)''')
...     print &quot;    properties:&quot;, props.extract()
...     print

current scope: [u'http://schema.org/Product']
    properties: [u'name', u'aggregateRating', u'offers', u'description', u'review', u'review']

current scope: [u'http://schema.org/AggregateRating']
    properties: [u'ratingValue', u'reviewCount']

current scope: [u'http://schema.org/Offer']
    properties: [u'price', u'availability']

current scope: [u'http://schema.org/Review']
    properties: [u'name', u'author', u'datePublished', u'reviewRating', u'description']

current scope: [u'http://schema.org/Rating']
    properties: [u'worstRating', u'ratingValue', u'bestRating']

current scope: [u'http://schema.org/Review']
    properties: [u'name', u'author', u'datePublished', u'reviewRating', u'description']

current scope: [u'http://schema.org/Rating']
    properties: [u'worstRating', u'ratingValue', u'bestRating']

&amp;gt;&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;这里我们先迭代&lt;code class=&quot;highlighter-rouge&quot;&gt;itemscope&lt;/code&gt;元素，对于每一个元素，我们寻找所有&lt;code class=&quot;highlighter-rouge&quot;&gt;itemprops&lt;/code&gt;元素，并排除那些在另一个元素内部的元素&lt;code class=&quot;highlighter-rouge&quot;&gt;itemscope&lt;/code&gt;。&lt;/p&gt;

&lt;h4 id=&quot;xpath-2&quot;&gt;一些XPath提示&lt;/h4&gt;

&lt;p&gt;这里有一些提示，你可能会发现有用的使用XPath与Scrapy选择器，基于这个帖子从ScrapingHub的博客。如果你不太熟悉XPath，你可能想先看看这个XPath教程。&lt;/p&gt;

&lt;h5 id=&quot;section-6&quot;&gt;在条件中使用文本节点&lt;/h5&gt;

&lt;p&gt;当您需要使用文本内容作为&lt;a href=&quot;https://www.w3.org/TR/xpath/#section-String-Functions&quot;&gt;XPath字符串函数的&lt;/a&gt;参数时，请避免使用&lt;code class=&quot;highlighter-rouge&quot;&gt;.//text()&lt;/code&gt;和使用&lt;code class=&quot;highlighter-rouge&quot;&gt;.&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;这是因为表达式&lt;code class=&quot;highlighter-rouge&quot;&gt;.//text()&lt;/code&gt;产生一组文本元素 - 一个节点集。当一个节点集被转换为一个字符串，当它作为参数传递给一个字符串函数，如&lt;code class=&quot;highlighter-rouge&quot;&gt;contains()&lt;/code&gt;or &lt;code class=&quot;highlighter-rouge&quot;&gt;starts-with()&lt;/code&gt;时，会导致第一个元素的文本。&lt;/p&gt;

&lt;p&gt;例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from scrapy import Selector
&amp;gt;&amp;gt;&amp;gt; sel = Selector(text='&amp;lt;a href=&quot;#&quot;&amp;gt;Click here to go to the &amp;lt;strong&amp;gt;Next Page&amp;lt;/strong&amp;gt;&amp;lt;/a&amp;gt;')
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;将节点集转换为字符串：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; sel.xpath('//a//text()').extract() # take a peek at the node-set
[u'Click here to go to the ', u'Next Page']
&amp;gt;&amp;gt;&amp;gt; sel.xpath(&quot;string(//a[1]//text())&quot;).extract() # convert it to string
[u'Click here to go to the ']
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;一个节点转换为字符串，但是，拼文本本身及其所有的后代：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; sel.xpath(&quot;//a[1]&quot;).extract() # select the first node
[u'&amp;lt;a href=&quot;#&quot;&amp;gt;Click here to go to the &amp;lt;strong&amp;gt;Next Page&amp;lt;/strong&amp;gt;&amp;lt;/a&amp;gt;']
&amp;gt;&amp;gt;&amp;gt; sel.xpath(&quot;string(//a[1])&quot;).extract() # convert it to string
[u'Click here to go to the Next Page']
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;所以，&lt;code class=&quot;highlighter-rouge&quot;&gt;.//text()&lt;/code&gt;在这种情况下使用节点集不会选择任何东西：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; sel.xpath(&quot;//a[contains(.//text(), 'Next Page')]&quot;).extract()
[]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;但是使用的&lt;code class=&quot;highlighter-rouge&quot;&gt;.&lt;/code&gt;意思是节点，工作原理：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; sel.xpath(&quot;//a[contains(., 'Next Page')]&quot;).extract()
[u'&amp;lt;a href=&quot;#&quot;&amp;gt;Click here to go to the &amp;lt;strong&amp;gt;Next Page&amp;lt;/strong&amp;gt;&amp;lt;/a&amp;gt;']
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;注意// node [1]和（// node）之间的区别[1]&lt;/strong&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;//node[1]&lt;/code&gt;选择在它们各自的父亲下首先出现的所有节点。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;(//node)[1]&lt;/code&gt; 选择文档中的所有节点，然后仅获取其中的第一个。&lt;/p&gt;

&lt;p&gt;例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from scrapy import Selector
&amp;gt;&amp;gt;&amp;gt; sel = Selector(text=&quot;&quot;&quot;
....:     &amp;lt;ul class=&quot;list&quot;&amp;gt;
....:         &amp;lt;li&amp;gt;1&amp;lt;/li&amp;gt;
....:         &amp;lt;li&amp;gt;2&amp;lt;/li&amp;gt;
....:         &amp;lt;li&amp;gt;3&amp;lt;/li&amp;gt;
....:     &amp;lt;/ul&amp;gt;
....:     &amp;lt;ul class=&quot;list&quot;&amp;gt;
....:         &amp;lt;li&amp;gt;4&amp;lt;/li&amp;gt;
....:         &amp;lt;li&amp;gt;5&amp;lt;/li&amp;gt;
....:         &amp;lt;li&amp;gt;6&amp;lt;/li&amp;gt;
....:     &amp;lt;/ul&amp;gt;&quot;&quot;&quot;)
&amp;gt;&amp;gt;&amp;gt; xp = lambda x: sel.xpath(x).extract()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;这将获得所有第一个&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;li&amp;gt;&lt;/code&gt; 元素，无论它是它的父：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; xp(&quot;//li[1]&quot;)
[u'&amp;lt;li&amp;gt;1&amp;lt;/li&amp;gt;', u'&amp;lt;li&amp;gt;4&amp;lt;/li&amp;gt;']
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;这&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;li&amp;gt;&lt;/code&gt; 是整个文档的第一个元素：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; xp(&quot;(//li)[1]&quot;)
[u'&amp;lt;li&amp;gt;1&amp;lt;/li&amp;gt;']
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;这将获得 父&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;li&amp;gt;&lt;/code&gt; 下的所有第一个元素&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;ul&amp;gt;&lt;/code&gt;：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; xp(&quot;//ul/li[1]&quot;)
[u'&amp;lt;li&amp;gt;1&amp;lt;/li&amp;gt;', u'&amp;lt;li&amp;gt;4&amp;lt;/li&amp;gt;']
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;这将获得 整个文档中父级&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;li&amp;gt;&lt;/code&gt; 下的第一个元素&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;ul&amp;gt;&lt;/code&gt;：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; xp(&quot;(//ul/li)[1]&quot;)
[u'&amp;lt;li&amp;gt;1&amp;lt;/li&amp;gt;']
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;css&quot;&gt;当按类查询时，请考虑使用CSS&lt;/h5&gt;

&lt;p&gt;因为一个元素可以包含多个CSS类，所以XPath选择元素的方法是相当冗长：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;*[contains(concat(' ', normalize-space(@class), ' '), ' someclass ')]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;如果你使用&lt;code class=&quot;highlighter-rouge&quot;&gt;@class='someclass'&lt;/code&gt;你可能最终缺少有其他类的元素，如果你只是使用补偿，你可能会得到更多的你想要的元素，如果他们有一个不同的类名共享字符串。&lt;code class=&quot;highlighter-rouge&quot;&gt;contains(@class, 'someclass')someclass&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;事实证明，Scrapy选择器允许你链接选择器，所以大多数时候你可以使用CSS选择类，然后在需要时切换到XPath：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from scrapy import Selector
&amp;gt;&amp;gt;&amp;gt; sel = Selector(text='&amp;lt;div class=&quot;hero shout&quot;&amp;gt;&amp;lt;time datetime=&quot;2014-07-23 19:00&quot;&amp;gt;Special date&amp;lt;/time&amp;gt;&amp;lt;/div&amp;gt;')
&amp;gt;&amp;gt;&amp;gt; sel.css('.shout').xpath('./time/@datetime').extract()
[u'2014-07-23 19:00']
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;这比使用上面显示的详细XPath技巧更清晰。只要记住.在后面的XPath表达式中使用。&lt;/p&gt;

&lt;h2 id=&quot;section-7&quot;&gt;内置选择器参考&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class scrapy.selector.Selector(response=None, text=None, type=None)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;一个实例&lt;code class=&quot;highlighter-rouge&quot;&gt;Selector&lt;/code&gt;是一个包装器响应来选择其内容的某些部分。&lt;/p&gt;

&lt;p&gt;response是一个&lt;code class=&quot;highlighter-rouge&quot;&gt;HtmlResponse&lt;/code&gt;或一个&lt;code class=&quot;highlighter-rouge&quot;&gt;XmlResponse&lt;/code&gt;将被用于选择和提取的数据对象。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;text&lt;/code&gt;是一个&lt;code class=&quot;highlighter-rouge&quot;&gt;unicode&lt;/code&gt;字符串或&lt;code class=&quot;highlighter-rouge&quot;&gt;utf-8&lt;/code&gt;编码的文本，当一个 &lt;code class=&quot;highlighter-rouge&quot;&gt;response&lt;/code&gt;不可用时。使用&lt;code class=&quot;highlighter-rouge&quot;&gt;text&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;response&lt;/code&gt;一起是未定义的行为。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;type&lt;/code&gt;定义选择器类型，它可以是&lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;html&quot;&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;xml&quot;&lt;/code&gt;或&lt;code class=&quot;highlighter-rouge&quot;&gt;None（默认）&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;如果&lt;code class=&quot;highlighter-rouge&quot;&gt;type&lt;/code&gt;是&lt;code class=&quot;highlighter-rouge&quot;&gt;None&lt;/code&gt;，选择器将根据&lt;code class=&quot;highlighter-rouge&quot;&gt;response&lt;/code&gt;类型（见下文）自动选择最佳类型，或者默认&lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;html&quot;&lt;/code&gt;情况下与选项一起使用&lt;code class=&quot;highlighter-rouge&quot;&gt;text&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;如果&lt;code class=&quot;highlighter-rouge&quot;&gt;type&lt;/code&gt;是&lt;code class=&quot;highlighter-rouge&quot;&gt;None&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;response&lt;/code&gt;传递，选择器类型从响应类型推断如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;html&quot;&lt;/code&gt;对于HtmlResponse类型&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;xml&quot;&lt;/code&gt;对于XmlResponse类型&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;html&quot;&lt;/code&gt;为任何其他&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;否则，如果&lt;code class=&quot;highlighter-rouge&quot;&gt;type&lt;/code&gt;设置，选择器类型将被强制，并且不会发生检测。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;xpath（查询）&lt;/strong&gt;
查找与&lt;code class=&quot;highlighter-rouge&quot;&gt;xpath&lt;/code&gt;匹配的节点&lt;code class=&quot;highlighter-rouge&quot;&gt;query&lt;/code&gt;，并将结果作为 &lt;code class=&quot;highlighter-rouge&quot;&gt;SelectorList&lt;/code&gt;实例将所有元素展平。列表元素也实现&lt;code class=&quot;highlighter-rouge&quot;&gt;Selector&lt;/code&gt;接口。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;query&lt;/code&gt; 是一个包含要应用的XPATH查询的字符串。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;为了方便起见，这种方法可以称为 response.xpath()&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;css（查询）&lt;/strong&gt;
应用给定的CSS选择器并返回一个SelectorList实例。&lt;/p&gt;

&lt;p&gt;query 是一个包含要应用的CSS选择器的字符串。&lt;/p&gt;

&lt;p&gt;在后台，CSS查询使用cssselect库和run .xpath()方法转换为XPath查询 。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;为了方便起见，该方法可以称为 response.css()&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;extract（）&lt;/strong&gt;
序列化并返回匹配的节点作为unicode字符串列表。编码内容的百分比未引用。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;re（regex）&lt;/strong&gt;
应用给定的正则表达式并返回一个包含匹配项的unicode字符串的列表。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;regex&lt;/code&gt;可以是编译的正则表达式或将被编译为正则表达式的字符串 &lt;code class=&quot;highlighter-rouge&quot;&gt;re.compile(regex)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;注意，re()和re_first()解码HTML实体（除&amp;lt;和\&amp;amp;）。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;register_namespace（prefix，uri）&lt;/strong&gt;
注册在此使用的给定命名空间Selector。如果不注册命名空间，则无法从非标准命名空间中选择或提取数据。参见下面的例子。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;remove_namespaces（）&lt;/strong&gt;
删除所有命名空间，允许使用无命名空间的xpaths遍历文档。参见下面的例子。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;**nonzero&lt;/strong&gt;（）**
返回True如果有选择或任何实际的内容False 除外。换句话说，a的布尔值Selector由它选择的内容给出。&lt;/p&gt;

&lt;h3 id=&quot;selectorlist&quot;&gt;SelectorList对象&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;class scrapy.selector.SelectorList&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;本SelectorList类是内置的一个子list 类，它提供了几个方法。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;xpath（查询）&lt;/strong&gt;
调用.xpath()此列表中每个元素的方法，并将其结果作为另一个返回SelectorList。&lt;/p&gt;

&lt;p&gt;query 是同一个参数 Selector.xpath()&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;css（查询）&lt;/strong&gt;
调用.css()此列表中每个元素的方法，并将其结果作为另一个返回SelectorList。&lt;/p&gt;

&lt;p&gt;query 是同一个参数 Selector.css()&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;extract（）&lt;/strong&gt;
调用.extract()此列表中每个元素的方法，并将其结果作为unicode字符串列表返回展平。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;re（）&lt;/strong&gt;
调用.re()此列表中每个元素的方法，并将其结果作为unicode字符串列表返回展平。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;**nonzero&lt;/strong&gt;（）**
如果列表不为空，则返回True，否则返回False。&lt;/p&gt;

&lt;h4 id=&quot;html&quot;&gt;HTML响应的选择器示例&lt;/h4&gt;

&lt;p&gt;这里有几个Selector例子来说明几个概念。在所有情况下，我们假设已经Selector实例化了一个HtmlResponse对象，如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sel = Selector(html_response)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;h1&amp;gt;&lt;/code&gt;从HTML响应主体中选择所有元素，返回Selector对象列表 （即SelectorList对象）：&lt;/p&gt;

    &lt;p&gt;sel.xpath(“//h1”)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;h1&amp;gt;&lt;/code&gt;从HTML响应正文中提取所有元素的文本，返回unicode字符串&lt;/p&gt;

    &lt;p&gt;sel.xpath(“//h1”).extract()         # this includes the h1 tag
 sel.xpath(“//h1/text()”).extract()  # this excludes the h1 tag&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;迭代所有&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;p&amp;gt;&lt;/code&gt;标签并打印其类属性：&lt;/p&gt;

    &lt;p&gt;for node in sel.xpath(“//p”):
     print node.xpath(“@class”).extract()&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;xml&quot;&gt;XML响应的选择器示例&lt;/h4&gt;

&lt;p&gt;这里有几个例子来说明几个概念。在这两种情况下，我们假设已经Selector实例化了一个 XmlResponse对象，像这样：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sel = Selector(xml_response)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;product&gt;从XML响应主体中选择所有元素，返回Selector对象列表（即SelectorList对象）：

 sel.xpath(&quot;//product&quot;)

&lt;/product&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;从需要注册命名空间的&lt;a href=&quot;https://support.google.com/merchants/answer/160589?hl=en&amp;amp;ref_topic=2473799&quot;&gt;Google Base XML Feed&lt;/a&gt;中提取所有价格：&lt;/p&gt;

    &lt;p&gt;sel.register_namespace(“g”, “http://base.google.com/ns/1.0”)
 sel.xpath(“//g:price”).extract()&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;section-8&quot;&gt;删除名称空间&lt;/h4&gt;

&lt;p&gt;当处理抓取项目时，通常很方便地完全删除命名空间，只需处理元素名称，编写更简单/方便的XPath。你可以使用的 Selector.remove_namespaces()方法。&lt;/p&gt;

&lt;p&gt;让我们展示一个例子，用GitHub博客atom feed来说明这一点。&lt;/p&gt;

&lt;p&gt;首先，我们打开shell和我们想要抓取的url：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ scrapy shell https://github.com/blog.atom
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;一旦在shell中，我们可以尝试选择所有&lt;link /&gt;对象，并看到它不工作（因为Atom XML命名空间模糊了这些节点）：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; response.xpath(&quot;//link&quot;)
[]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;但是一旦我们调用该Selector.remove_namespaces()方法，所有节点都可以直接通过他们的名字访问：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; response.selector.remove_namespaces()
&amp;gt;&amp;gt;&amp;gt; response.xpath(&quot;//link&quot;)
[&amp;lt;Selector xpath='//link' data=u'&amp;lt;link xmlns=&quot;http://www.w3.org/2005/Atom'&amp;gt;,
 &amp;lt;Selector xpath='//link' data=u'&amp;lt;link xmlns=&quot;http://www.w3.org/2005/Atom'&amp;gt;,
 ...
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;如果你想知道为什么默认情况下不调用命名空间删除过程，而不是手动调用它，这是因为两个原因，按照相关性的顺序：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;删除命名空间需要迭代和修改文档中的所有节点，这对于Scrapy爬取的所有文档来说是一个相当昂贵的操作&lt;/li&gt;
  &lt;li&gt;可能有一些情况下，实际上需要使用命名空间，以防某些元素名称在命名空间之间冲突。这些情况非常罕见。&lt;/li&gt;
&lt;/ol&gt;
</content>
<category term="Scrapy" />
<summary>Scrapy爬虫入门教程五 Selectors（选择器）</summary>
</entry>
<entry>
<title>Scrapy爬虫入门教程四 Spider（爬虫）</title>
<link href="http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E5%9B%9B-spider-%E7%88%AC%E8%99%AB/" rel="alternate" type="text/html" title="Scrapy爬虫入门教程四 Spider（爬虫）" />
<published>2017-04-09T14:29:00+08:00</published>
<updated>2017-04-09T14:29:00+08:00</updated>
<id>http://localhost:4000/scrapy爬虫入门教程四-spider（爬虫）</id>
<content type="html" xml:base="http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E5%9B%9B-spider-%E7%88%AC%E8%99%AB/">&lt;h1 id=&quot;scrapy-spider&quot;&gt;Scrapy爬虫入门教程四 Spider（爬虫）&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;开发环境：&lt;/strong&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;Python 3.6.0 版本&lt;/code&gt; （当前最新）
&lt;code class=&quot;highlighter-rouge&quot;&gt;Scrapy 1.3.2 版本&lt;/code&gt; （当前最新）&lt;/p&gt;

&lt;h1 id=&quot;spider&quot;&gt;Spider&lt;/h1&gt;

&lt;p&gt;爬虫是定义如何抓取某个网站（或一组网站）的类，包括如何执行抓取（即关注链接）以及如何从其网页中提取结构化数据（即抓取项目）。换句话说，Spider是您定义用于为特定网站（或在某些情况下，一组网站）抓取和解析网页的自定义行为的位置。&lt;/p&gt;

&lt;p&gt;对于爬虫，循环经历这样的事情：&lt;/p&gt;

&lt;p&gt;1.
您首先生成用于抓取第一个URL的初始请求，然后指定要使用从这些请求下载的响应调用的回调函数。&lt;/p&gt;

&lt;p&gt;第一个执行的请求通过调用 start_requests()（默认情况下）Request为在start_urls和中指定的URL生成的parse方法获取， 并且该方法作为请求的回调函数。&lt;/p&gt;

&lt;p&gt;2.
在回调函数中，您将解析响应（网页），并返回带有提取的数据，Item对象， Request对象或这些对象的可迭代的对象。这些请求还将包含回调（可能是相同的），然后由Scrapy下载，然后由指定的回调处理它们的响应。&lt;/p&gt;

&lt;p&gt;3.
在回调函数中，您通常使用选择器来解析页面内容 （但您也可以使用BeautifulSoup，lxml或您喜欢的任何机制），并使用解析的数据生成项目。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;最后，从爬虫返回的项目通常将持久存储到数据库（在某些项目管道中）或使用Feed导出写入文件。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;即使这个循环（或多或少）适用于任何种类的爬虫，有不同种类的默认爬虫捆绑到Scrapy中用于不同的目的。我们将在这里谈论这些类型。&lt;/p&gt;

&lt;h2 id=&quot;class-scrapyspidersspider&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;class scrapy.spiders.Spider&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;这是最简单的爬虫，每个其他爬虫必须继承的爬虫（包括与Scrapy捆绑在一起的爬虫，以及你自己写的爬虫）。它不提供任何特殊功能。它只是提供了一个默认&lt;code class=&quot;highlighter-rouge&quot;&gt;start_requests()&lt;/code&gt;实现，它从&lt;code class=&quot;highlighter-rouge&quot;&gt;start_urlsspider&lt;/code&gt;属性发送请求，并&lt;code class=&quot;highlighter-rouge&quot;&gt;parse&lt;/code&gt; 为每个结果响应调用&lt;code class=&quot;highlighter-rouge&quot;&gt;spider&lt;/code&gt;的方法。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;name&lt;/code&gt;
定义此爬虫名称的字符串。爬虫名称是爬虫如何由Scrapy定位（和实例化），因此它&lt;strong&gt;必须是唯一的&lt;/strong&gt;。但是，没有什么能阻止你实例化同一个爬虫的多个实例。这是最重要的爬虫属性，它是必需的。&lt;/p&gt;

&lt;p&gt;如果爬虫抓取单个域名，通常的做法是在域后面命名爬虫。因此，例如，抓取的爬虫mywebsite.com通常会被调用 mywebsite。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;
在Python 2中，这必须是ASCII。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;allowed_domains&lt;/code&gt;
允许此爬虫抓取的域的字符串的可选列表，指定一个列表可以抓取，其它就不会抓取了。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;start_urls&lt;/code&gt;
当没有指定特定网址时，爬虫将开始抓取的网址列表。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;custom_settings&lt;/code&gt;
运行此爬虫时将从项目宽配置覆盖的设置字典。它必须定义为类属性，因为设置在实例化之前更新。&lt;/p&gt;

&lt;p&gt;有关可用内置设置的列表，请参阅： &lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/settings.html#topics-settings-ref&quot;&gt;内置设置参考&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;crawler&lt;/code&gt;
此属性from_crawler()在初始化类后由类方法设置，并链接Crawler到此爬虫实例绑定到的对象。&lt;/p&gt;

&lt;p&gt;Crawlers在项目中封装了很多组件，用于单个条目访问（例如扩展，中间件，信号管理器等）。有关详情，&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/api.html#topics-api-crawler&quot;&gt;请参阅抓取工具API&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;settings&lt;/code&gt;
运行此爬虫的配置。这是一个 Settings实例，有关此主题的详细介绍，&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/settings.html#topics-settings&quot;&gt;请参阅设置主题&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;logger&lt;/code&gt;
用Spider创建的Python记录器&lt;code class=&quot;highlighter-rouge&quot;&gt;name&lt;/code&gt;。您可以使用它通过它发送日志消息，如&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/logging.html#topics-logging-from-spiders&quot;&gt;记录爬虫程序中所述&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;from_crawler&lt;/code&gt;（crawler，* args，** kwargs ）
是Scrapy用来创建爬虫的类方法。&lt;/p&gt;

&lt;p&gt;您可能不需要直接覆盖这一点，因为默认实现充当方法的代理，&lt;code class=&quot;highlighter-rouge&quot;&gt;__init__()&lt;/code&gt;使用给定的参数args和命名参数kwargs调用它。&lt;/p&gt;

&lt;p&gt;尽管如此，此方法 在新实例中设置crawler和settings属性，以便以后可以在爬虫程序中访问它们。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;参数：&lt;/li&gt;
  &lt;li&gt;crawler（Crawlerinstance） - 爬虫将绑定到的爬虫&lt;/li&gt;
  &lt;li&gt;args（list） - 传递给&lt;strong&gt;init&lt;/strong&gt;()方法的参数&lt;/li&gt;
  &lt;li&gt;kwargs（dict） - 传递给&lt;strong&gt;init&lt;/strong&gt;()方法的关键字参数&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;start_requests（）&lt;/code&gt;
此方法必须返回一个可迭代的第一个请求来抓取这个爬虫。&lt;/p&gt;

&lt;p&gt;有了start_requests()，就不写了start_urls，写了也没有用。&lt;/p&gt;

&lt;p&gt;默认实现是：start_urls，但是可以复写的方法start_requests。
例如，如果您需要通过使用POST请求登录来启动，您可以：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class MySpider(scrapy.Spider):
    name = 'myspider'

    def start_requests(self):
        return [scrapy.FormRequest(&quot;http://www.example.com/login&quot;,
                                   formdata={'user': 'john', 'pass': 'secret'},
                                   callback=self.logged_in)]

    def logged_in(self, response):
        # here you would extract links to follow and return Requests for
        # each of them, with another callback
        pass
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;make_requests_from_url(url)&lt;/code&gt;
一种接收URL并返回Request 对象（或Request对象列表）进行抓取的方法。此方法用于在方法中构造初始请求 start_requests()，并且通常用于将URL转换为请求。&lt;/p&gt;

&lt;p&gt;除非重写，此方法返回具有方法的Requests parse() 作为它们的回调函数，并启用dont_filter参数（Request有关更多信息，请参阅类）。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;parse(response)&lt;/code&gt;
这是Scrapy用于处理下载的响应的默认回调，当它们的请求没有指定回调时。&lt;/p&gt;

&lt;p&gt;该parse方法负责处理响应并返回所抓取的数据或更多的URL。其他请求回调具有与Spider类相同的要求。&lt;/p&gt;

&lt;p&gt;此方法以及任何其他请求回调必须返回一个可迭代的Request和dicts或Item对象。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;参数：&lt;/li&gt;
  &lt;li&gt;response（Response） - 解析的响应&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;log(message[, level, component])&lt;/code&gt;
包装器通过爬虫发送日志消息logger，保持向后兼容性。有关详细信息，请参阅 从Spider记录。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;closed(reason)&lt;/code&gt;
当爬虫关闭时召唤。此方法为spider_closed信号的signals.connect()提供了一个快捷方式。&lt;/p&gt;

&lt;p&gt;让我们看一个例子：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import scrapy


class MySpider(scrapy.Spider):
    name = 'example.com'
    allowed_domains = ['example.com']
    start_urls = [
        'http://www.example.com/1.html',
        'http://www.example.com/2.html',
        'http://www.example.com/3.html',
    ]

    def parse(self, response):
        self.logger.info('A response from %s just arrived!', response.url)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;从单个回调中返回多个请求和项：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import scrapy

class MySpider(scrapy.Spider):
    name = 'example.com'
    allowed_domains = ['example.com']
    start_urls = [
        'http://www.example.com/1.html',
        'http://www.example.com/2.html',
        'http://www.example.com/3.html',
    ]

    def parse(self, response):
        for h3 in response.xpath('//h3').extract():
            yield {&quot;title&quot;: h3}

        for url in response.xpath('//a/@href').extract():
            yield scrapy.Request(url, callback=self.parse)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;你可以直接使用start_requests()，而不是start_urls; 项目可以更加方便获取数据：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import scrapy
from myproject.items import MyItem

class MySpider(scrapy.Spider):
    name = 'example.com'
    allowed_domains = ['example.com']

    def start_requests(self):
        yield scrapy.Request('http://www.example.com/1.html', self.parse)
        yield scrapy.Request('http://www.example.com/2.html', self.parse)
        yield scrapy.Request('http://www.example.com/3.html', self.parse)

    def parse(self, response):
        for h3 in response.xpath('//h3').extract():
            yield MyItem(title=h3)

        for url in response.xpath('//a/@href').extract():
            yield scrapy.Request(url, callback=self.parse)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;spider-arguments&quot;&gt;Spider arguments&lt;/h3&gt;

&lt;p&gt;爬虫可以接收修改其行为的参数。爬虫参数的一些常见用法是定义起始URL或将爬网限制到网站的某些部分，但它们可用于配置爬虫的任何功能。&lt;/p&gt;

&lt;p&gt;Spider crawl参数使用该-a选项通过命令 传递。例如：&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy crawl myspider -a category=electronics&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;爬虫可以在他们的&lt;strong&gt;init&lt;/strong&gt;方法中访问参数：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import scrapy

class MySpider(scrapy.Spider):
    name = 'myspider'

    def __init__(self, category=None, *args, **kwargs):
        super(MySpider, self).__init__(*args, **kwargs)
        self.start_urls = ['http://www.example.com/categories/%s' % category]
        # ...
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;默认的&lt;strong&gt;init&lt;/strong&gt;方法将获取任何爬虫参数，并将它们作为属性复制到爬虫。上面的例子也可以写成如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import scrapy

class MySpider(scrapy.Spider):
    name = 'myspider'

    def start_requests(self):
        yield scrapy.Request('http://www.example.com/categories/%s' % self.category)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;请记住，spider参数只是字符串。爬虫不会自己做任何解析。如果要从命令行设置start_urls属性，则必须将它自己解析为列表，使用像 ast.literal_eval 或json.loads之类的属性 ，然后将其设置为属性。否则，你会导致迭代一个start_urls字符串（一个非常常见的python陷阱），导致每个字符被看作一个单独的url。&lt;/p&gt;

&lt;p&gt;有效的用例是设置使用的http验证凭据HttpAuthMiddleware 或用户代理使用的用户代理UserAgentMiddleware：
&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy crawl myspider -a http_user=myuser -a http_pass=mypassword -a user_agent=mybot&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Spider参数也可以通过Scrapyd schedule.jsonAPI 传递。请参阅&lt;a href=&quot;http://scrapyd.readthedocs.org/en/latest/&quot;&gt;Scrapyd文档&lt;/a&gt;。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;section&quot;&gt;通用爬虫&lt;/h3&gt;

&lt;p&gt;Scrapy附带一些有用的通用爬虫，你可以使用它来子类化你的爬虫。他们的目的是为一些常见的抓取案例提供方便的功能，例如根据某些规则查看网站上的所有链接，从站点&lt;a href=&quot;http://www.sitemaps.org/&quot;&gt;地图抓取&lt;/a&gt;或解析XML / CSV Feed。&lt;/p&gt;

&lt;p&gt;对于在以下爬虫中使用的示例，我们假设您有一个&lt;code class=&quot;highlighter-rouge&quot;&gt;TestItem&lt;/code&gt;在&lt;code class=&quot;highlighter-rouge&quot;&gt;myproject.items&lt;/code&gt;模块中声明的项目：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import scrapy

class TestItem(scrapy.Item):
    id = scrapy.Field()
    name = scrapy.Field()
    description = scrapy.Field()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;section-1&quot;&gt;抓取爬虫&lt;/h4&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;类 scrapy.spiders.CrawlSpider&lt;/code&gt;
这是最常用的爬行常规网站的爬虫，因为它通过定义一组规则为下列链接提供了一种方便的机制。它可能不是最适合您的特定网站或项目，但它是足够通用的几种情况，所以你可以从它开始，根据需要覆盖更多的自定义功能，或只是实现自己的爬虫。&lt;/p&gt;

&lt;p&gt;除了从Spider继承的属性（你必须指定），这个类支持一个新的属性：&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;rules&lt;/code&gt;
它是一个（或多个）&lt;code class=&quot;highlighter-rouge&quot;&gt;Rule&lt;/code&gt;对象的列表。每个都&lt;code class=&quot;highlighter-rouge&quot;&gt;Rule&lt;/code&gt;定义了抓取网站的某种行为。规则对象如下所述。如果多个规则匹配相同的链接，则将根据它们在此属性中定义的顺序使用第一个。&lt;/p&gt;

&lt;p&gt;这个爬虫还暴露了可覆盖的方法：&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;parse_start_url(response)&lt;/code&gt;
对于start_urls响应调用此方法。它允许解析初始响应，并且必须返回&lt;code class=&quot;highlighter-rouge&quot;&gt;Item&lt;/code&gt;对象，&lt;code class=&quot;highlighter-rouge&quot;&gt;Request&lt;/code&gt;对象或包含任何对象的迭代器。&lt;/p&gt;

&lt;h4 id=&quot;section-2&quot;&gt;抓取规则&lt;/h4&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;class scrapy.spiders.Rule（link_extractor，callback = None，cb_kwargs = None，follow = None，process_links = None，process_request = None ）&lt;/code&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;link_extractor&lt;/code&gt;是一个链接提取程序对象，它定义如何从每个爬网页面提取链接。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;callback&lt;/code&gt;是一个可调用的或字符串（在这种情况下，将使用具有该名称的爬虫对象的方法），以便为使用指定的link_extractor提取的每个链接调用。这个回调接收一个响应作为其第一个参数，并且必须返回一个包含Item和 Request对象（或它们的任何子类）的列表。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;警告&lt;/strong&gt;
当编写爬网爬虫规则时，避免使用&lt;code class=&quot;highlighter-rouge&quot;&gt;parse&lt;/code&gt;作为回调，因为&lt;code class=&quot;highlighter-rouge&quot;&gt;CrawlSpider&lt;/code&gt;使用&lt;code class=&quot;highlighter-rouge&quot;&gt;parse&lt;/code&gt;方法本身来实现其逻辑。所以如果你重写的&lt;code class=&quot;highlighter-rouge&quot;&gt;parse&lt;/code&gt;方法，爬行爬虫将不再工作。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cb_kwargs&lt;/code&gt; 是包含要传递给回调函数的关键字参数的dict。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;follow&lt;/code&gt;是一个布尔值，它指定是否应该从使用此规则提取的每个响应中跟踪链接。如果&lt;code class=&quot;highlighter-rouge&quot;&gt;callback&lt;/code&gt;是&lt;code class=&quot;highlighter-rouge&quot;&gt;None follow&lt;/code&gt;默认为&lt;code class=&quot;highlighter-rouge&quot;&gt;True&lt;/code&gt;，否则默认为&lt;code class=&quot;highlighter-rouge&quot;&gt;False&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;process_links&lt;/code&gt;是一个可调用的或一个字符串（在这种情况下，将使用具有该名称的爬虫对象的方法），将使用指定从每个响应提取的每个链接列表调用该方法&lt;code class=&quot;highlighter-rouge&quot;&gt;link_extractor&lt;/code&gt;。这主要用于过滤目的。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;process_request&lt;/code&gt; 是一个可调用的或一个字符串（在这种情况下，将使用具有该名称的爬虫对象的方法），它将被此规则提取的每个请求调用，并且必须返回一个请求或无（过滤出请求） 。&lt;/p&gt;

&lt;h4 id=&quot;section-3&quot;&gt;抓取爬虫示例&lt;/h4&gt;

&lt;p&gt;现在让我们来看一个CrawlSpider的例子：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import scrapy
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor

class MySpider(CrawlSpider):
    name = 'example.com'
    allowed_domains = ['example.com']
    start_urls = ['http://www.example.com']

    rules = (
        # Extract links matching 'category.php' (but not matching 'subsection.php')
        # and follow links from them (since no callback means follow=True by default).
        Rule(LinkExtractor(allow=('category\.php', ), deny=('subsection\.php', ))),

        # Extract links matching 'item.php' and parse them with the spider's method parse_item
        Rule(LinkExtractor(allow=('item\.php', )), callback='parse_item'),
    )

    def parse_item(self, response):
        self.logger.info('Hi, this is an item page! %s', response.url)
        item = scrapy.Item()
        item['id'] = response.xpath('//td[@id=&quot;item_id&quot;]/text()').re(r'ID: (\d+)')
        item['name'] = response.xpath('//td[@id=&quot;item_name&quot;]/text()').extract()
        item['description'] = response.xpath('//td[@id=&quot;item_description&quot;]/text()').extract()
        return item
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;这个爬虫会开始抓取example.com的主页，收集类别链接和项链接，用parse_item方法解析后者。对于每个项目响应，将使用XPath从HTML中提取一些数据，并将Item使用它填充。&lt;/p&gt;

&lt;h4 id=&quot;xmlfeedspider&quot;&gt;XMLFeedSpider&lt;/h4&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;class scrapy.spiders.XMLFeedSpider&lt;/code&gt;
XMLFeedSpider设计用于通过以特定节点名称迭代XML订阅源来解析XML订阅源。迭代器可以选自：iternodes，xml和html。iternodes为了性能原因，建议使用迭代器，因为xml和迭代器html一次生成整个DOM为了解析它。但是，html当使用坏标记解析XML时，使用作为迭代器可能很有用。&lt;/p&gt;

&lt;p&gt;要设置迭代器和标记名称，必须定义以下类属性：&lt;/p&gt;

&lt;p&gt;-
&lt;code class=&quot;highlighter-rouge&quot;&gt;iterator&lt;/code&gt;
定义要使用的迭代器的字符串。它可以是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;'iternodes'&lt;/code&gt; - 基于正则表达式的快速迭代器&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;'html'&lt;/code&gt;- 使用的迭代器Selector。请记住，这使用DOM解析，并且必须加载所有DOM在内存中，这可能是一个大饲料的问题&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;'xml'&lt;/code&gt;- 使用的迭代器Selector。请记住，这使用DOM解析，并且必须加载所有DOM在内存中，这可能是一个大饲料的问题
它默认为：&lt;code class=&quot;highlighter-rouge&quot;&gt;'iternodes'&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;itertag&lt;/code&gt;
一个具有要迭代的节点（或元素）的名称的字符串。示​​例：
&lt;code class=&quot;highlighter-rouge&quot;&gt;itertag = 'product'&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;namespaces&lt;/code&gt;
定义该文档中将使用此爬虫处理的命名空间的元组列表。在 与将用于自动注册使用的命名空间 的方法。(prefix, uri)prefixuriregister_namespace()&lt;/p&gt;

&lt;p&gt;然后，您可以在属性中指定具有命名空间的itertag 节点。&lt;/p&gt;

&lt;p&gt;例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class YourSpider(XMLFeedSpider):

    namespaces = [('n', 'http://www.sitemaps.org/schemas/sitemap/0.9')]
    itertag = 'n:url'
    # ...
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;除了这些新的属性，这个爬虫也有以下可重写的方法：&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;adapt_response(response)&lt;/code&gt;
一种在爬虫开始解析响应之前，在响应从爬虫中间件到达时立即接收的方法。它可以用于在解析之前修改响应主体。此方法接收响应并返回响应（它可以是相同的或另一个）。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;parse_node(response, selector)&lt;/code&gt;
对于与提供的标记名称（itertag）匹配的节点，将调用此方法。接收Selector每个节点的响应和 。覆盖此方法是必需的。否则，你的爬虫将不工作。此方法必须返回一个Item对象，一个 Request对象或包含任何对象的迭代器。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;process_results(response, results)&lt;/code&gt;
对于由爬虫返回的每个结果（Items or Requests），将调用此方法，并且它将在将结果返回到框架核心之前执行所需的任何最后处理，例如设置项目ID。它接收结果列表和产生那些结果的响应。它必须返回结果列表（Items or Requests）。&lt;/p&gt;

&lt;h4 id=&quot;xmlfeedspider-1&quot;&gt;XMLFeedSpider示例&lt;/h4&gt;

&lt;p&gt;这些爬虫很容易使用，让我们看一个例子：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from scrapy.spiders import XMLFeedSpider
from myproject.items import TestItem

class MySpider(XMLFeedSpider):
    name = 'example.com'
    allowed_domains = ['example.com']
    start_urls = ['http://www.example.com/feed.xml']
    iterator = 'iternodes'  # This is actually unnecessary, since it's the default value
    itertag = 'item'

    def parse_node(self, response, node):
        self.logger.info('Hi, this is a &amp;lt;%s&amp;gt; node!: %s', self.itertag, ''.join(node.extract()))

        item = TestItem()
        item['id'] = node.xpath('@id').extract()
        item['name'] = node.xpath('name').extract()
        item['description'] = node.xpath('description').extract()
        return item
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;基本上我们做的是创建一个爬虫，从给定的下载一个start_urls，然后遍历每个item标签，打印出来，并存储一些随机数据Item。&lt;/p&gt;

&lt;h3 id=&quot;csvfeedspider&quot;&gt;CSVFeedSpider&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;class scrapy.spiders.CSVF&lt;/code&gt;
这个爬虫非常类似于XMLFeedSpider，除了它迭代行，而不是节点。在每次迭代中调用的方法是parse_row()。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;delimiter&lt;/code&gt;
CSV文件中每个字段的带分隔符的字符串默认为’,’（逗号）。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;quotechar&lt;/code&gt;
CSV文件中每个字段的包含字符的字符串默认为’”‘（引号）。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;headers&lt;/code&gt;
文件CSV Feed中包含的行的列表，用于从中提取字段。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;parse_row(response, row)&lt;/code&gt;
使用CSV文件的每个提供（或检测到）标头的键接收响应和dict（表示每行）。这个爬虫还给予机会重写adapt_response和process_results方法的前和后处理的目的。&lt;/p&gt;

&lt;h4 id=&quot;csvfeedspider-1&quot;&gt;CSVFeedSpider示例&lt;/h4&gt;

&lt;p&gt;让我们看一个类似于前一个例子，但使用 CSVFeedSpider：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from scrapy.spiders import CSVFeedSpider
from myproject.items import TestItem

class MySpider(CSVFeedSpider):
    name = 'example.com'
    allowed_domains = ['example.com']
    start_urls = ['http://www.example.com/feed.csv']
    delimiter = ';'
    quotechar = &quot;'&quot;
    headers = ['id', 'name', 'description']

    def parse_row(self, response, row):
        self.logger.info('Hi, this is a row!: %r', row)

        item = TestItem()
        item['id'] = row['id']
        item['name'] = row['name']
        item['description'] = row['description']
        return item
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;sitemapspider&quot;&gt;SitemapSpider&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;class scrapy.spiders.SitemapSpider&lt;/code&gt;
SitemapSpider允许您通过使用&lt;a href=&quot;http://www.sitemaps.org/&quot;&gt;Sitemaps&lt;/a&gt;发现网址来抓取&lt;a href=&quot;http://www.sitemaps.org/&quot;&gt;网站&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;它支持嵌套Sitemap和从&lt;a href=&quot;http://www.robotstxt.org/&quot;&gt;robots.txt&lt;/a&gt;发现Sitemap网址 。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sitemap_urls&lt;/code&gt;
指向您要抓取的网址的网站的网址列表。&lt;/p&gt;

&lt;p&gt;您还可以指向robots.txt，它会解析为从中提取Sitemap网址。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sitemap_rules&lt;/code&gt;
元组列表其中：&lt;code class=&quot;highlighter-rouge&quot;&gt;(regex, callback)&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;regex是与从Sitemap中提取的网址相匹配的正则表达式。 regex可以是一个str或一个编译的正则表达式对象。&lt;/li&gt;
  &lt;li&gt;callback是用于处理与正则表达式匹配的url的回调。callback可以是字符串（指示蜘蛛方法的名称）或可调用的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;例如：
&lt;code class=&quot;highlighter-rouge&quot;&gt;sitemap_rules = [('/product/', 'parse_product')]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;规则按顺序应用，只有匹配的第一个将被使用。
如果省略此属性，则会在parse回调中处理在站点地图中找到的所有网址。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sitemap_follow&lt;/code&gt;
应遵循的网站地图的正则表达式列表。这只适用于使用指向其他Sitemap文件的&lt;a href=&quot;http://www.sitemaps.org/protocol.html#index&quot;&gt;Sitemap索引文件&lt;/a&gt;的网站。&lt;/p&gt;

&lt;p&gt;默认情况下，将跟踪所有网站地图。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sitemap_alternate_links&lt;/code&gt;
指定是否url应遵循一个备用链接。这些是在同一个url块中传递的另一种语言的同一网站的链接。&lt;/p&gt;

&lt;p&gt;例如：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;url&amp;gt;
    &amp;lt;loc&amp;gt;http://example.com/&amp;lt;/loc&amp;gt;
    &amp;lt;xhtml:link rel=&quot;alternate&quot; hreflang=&quot;de&quot; href=&quot;http://example.com/de&quot;/&amp;gt;
&amp;lt;/url&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;使用&lt;code class=&quot;highlighter-rouge&quot;&gt;sitemap_alternate_linksset&lt;/code&gt;，这将检索两个URL。随着 &lt;code class=&quot;highlighter-rouge&quot;&gt;sitemap_alternate_links&lt;/code&gt;禁用，只有&lt;a href=&quot;http://example.com/%E5%B0%86%E8%BF%9B%E8%A1%8C%E6%A3%80%E7%B4%A2%E3%80%82&quot;&gt;http://example.com/将进行检索。&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;默认为&lt;code class=&quot;highlighter-rouge&quot;&gt;sitemap_alternate_links&lt;/code&gt;禁用。&lt;/p&gt;

&lt;h4 id=&quot;sitemapspider-1&quot;&gt;SitemapSpider示例&lt;/h4&gt;

&lt;p&gt;最简单的示例：使用parse回调处理通过站点地图发现的所有网址 ：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from scrapy.spiders import SitemapSpider

class MySpider(SitemapSpider):
    sitemap_urls = ['http://www.example.com/sitemap.xml']

    def parse(self, response):
        pass # ... scrape item here ...
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;使用某个回调处理一些网址，并使用不同的回调处理其他网址：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from scrapy.spiders import SitemapSpider

class MySpider(SitemapSpider):
    sitemap_urls = ['http://www.example.com/sitemap.xml']
    sitemap_rules = [
        ('/product/', 'parse_product'),
        ('/category/', 'parse_category'),
    ]

    def parse_product(self, response):
        pass # ... scrape product ...

    def parse_category(self, response):
        pass # ... scrape category ...
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;关注&lt;a href=&quot;http://www.robotstxt.org/&quot;&gt;robots.txt&lt;/a&gt;文件中定义的sitemaps，并且只跟踪其网址包含&lt;code class=&quot;highlighter-rouge&quot;&gt;/sitemap_shop&lt;/code&gt;以下内容的Sitemap ：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from scrapy.spiders import SitemapSpider

class MySpider(SitemapSpider):
    sitemap_urls = ['http://www.example.com/robots.txt']
    sitemap_rules = [
        ('/shop/', 'parse_shop'),
    ]
    sitemap_follow = ['/sitemap_shops']

    def parse_shop(self, response):
        pass # ... scrape shop here ...
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;将SitemapSpider与其他来源网址结合使用：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from scrapy.spiders import SitemapSpider

class MySpider(SitemapSpider):
    sitemap_urls = ['http://www.example.com/robots.txt']
    sitemap_rules = [
        ('/shop/', 'parse_shop'),
    ]

    other_urls = ['http://www.example.com/about']

    def start_requests(self):
        requests = list(super(MySpider, self).start_requests())
        requests += [scrapy.Request(x, self.parse_other) for x in self.other_urls]
        return requests

    def parse_shop(self, response):
        pass # ... scrape shop here ...

    def parse_other(self, response):
        pass # ... scrape other here ...
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</content>
<category term="Scrapy" />
<summary>Scrapy爬虫入门教程四 Spider（爬虫）</summary>
</entry>
<entry>
<title>Scrapy爬虫入门教程三 命令行工具介绍和示例</title>
<link href="http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E4%B8%89-%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7%E4%BB%8B%E7%BB%8D%E5%92%8C%E7%A4%BA%E4%BE%8B/" rel="alternate" type="text/html" title="Scrapy爬虫入门教程三 命令行工具介绍和示例" />
<published>2017-04-09T11:55:00+08:00</published>
<updated>2017-04-09T11:55:00+08:00</updated>
<id>http://localhost:4000/scrapy爬虫入门教程三-命令行工具介绍和示例</id>
<content type="html" xml:base="http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E4%B8%89-%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7%E4%BB%8B%E7%BB%8D%E5%92%8C%E7%A4%BA%E4%BE%8B/">&lt;h1 id=&quot;scrapy-&quot;&gt;Scrapy爬虫入门教程三 命令行工具介绍和示例&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;开发环境：&lt;/strong&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;Python 3.6.0 版本&lt;/code&gt; （当前最新）
&lt;code class=&quot;highlighter-rouge&quot;&gt;Scrapy 1.3.2 版本&lt;/code&gt; （当前最新）&lt;/p&gt;

&lt;p&gt;[toc]&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;命令行工具&lt;/h2&gt;

&lt;p&gt;Scrapy是通过scrapy命令行工具来控制的，当前最新版本0.10&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;配置设置&lt;/h3&gt;

&lt;p&gt;Scrapy将scrapy.cfg在标准位置的ini样式文件中查找配置参数：&lt;/p&gt;

&lt;p&gt;1.
系统默认配置：&lt;code class=&quot;highlighter-rouge&quot;&gt;/etc/scrapy.cfg&lt;/code&gt;或&lt;code class=&quot;highlighter-rouge&quot;&gt;c:\scrapy\scrapy.cfg&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;2.
全局配置文件：&lt;code class=&quot;highlighter-rouge&quot;&gt;~/.config/scrapy.cfg&lt;/code&gt;（&lt;code class=&quot;highlighter-rouge&quot;&gt;$XDG_CONFIG_HOME&lt;/code&gt;）和&lt;code class=&quot;highlighter-rouge&quot;&gt;~/.scrapy.cfg&lt;/code&gt;（&lt;code class=&quot;highlighter-rouge&quot;&gt;$HOME&lt;/code&gt;）用于&lt;/p&gt;

&lt;p&gt;3.
项目配置文件：&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy.cfg&lt;/code&gt; 在scrapy项目的根目录中。&lt;/p&gt;

&lt;p&gt;来自这些文件的设置将按照所列的优先顺序进行合并：用户定义的值比系统级默认值具有更高的优先级，&lt;strong&gt;项目范围的设置将在定义时覆盖所有其他设置&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;Scrapy也理解，并且可以通过配置一些环境变量。目前这些是：&lt;/p&gt;

&lt;p&gt;SCRAPY_SETTINGS_MODULE（&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/settings.html#topics-settings-module-envvar&quot;&gt;请参阅指定设置&lt;/a&gt;）
SCRAPY_PROJECT
SCRAPY_PYTHON_SHELL（&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/shell.html#topics-shell&quot;&gt;见Scrapy shell&lt;/a&gt;）&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;scrapy&quot;&gt;Scrapy项目的默认结构&lt;/h2&gt;

&lt;p&gt;在深入了解命令行工具及其子命令之前，让我们先了解Scrapy项目的目录结构。&lt;/p&gt;

&lt;p&gt;虽然可以修改，但所有Scrapy项目默认情况下具有相同的文件结构，类似于：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scrapy.cfg
myproject/
    __init__.py
    items.py
    pipelines.py
    settings.py
    spiders/
        __init__.py
        spider1.py
        spider2.py
        ...
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;其中，目录scrapy.cfg文件位于项目的根目录。该文件包含定义项目设置的python模块的名称。这里是一个例子：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[settings]
default = myproject.settings
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;scrapy-1&quot;&gt;使用scrapy工具&lt;/h2&gt;

&lt;p&gt;您可以从运行没有参数的Scrapy工具开始，它将打印一些使用帮助和可用的命令：
直接在项目根目录的命令行输入： scrapy&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;会得到如下提示：
Scrapy 1.3.2 - no active project

Usage:
  scrapy &amp;lt;command&amp;gt; [options] [args]

Available commands:
  bench         Run quick benchmark test
  commands
  fetch         Fetch a URL using the Scrapy downloader
  genspider     Generate new spider using pre-defined templates
  runspider     Run a self-contained spider (without creating a project)
  settings      Get settings values
  shell         Interactive scraping console
  startproject  Create new project
  version       Print Scrapy version
  view          Open URL in browser, as seen by Scrapy

  [ more ]      More commands available when run from project directory

Use &quot;scrapy &amp;lt;command&amp;gt; -h&quot; to see more info about a command
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;section-2&quot;&gt;创建项目&lt;/h3&gt;

&lt;p&gt;你通常用这个scrapy工具做的第一件事是创建你的Scrapy项目：
&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy  startproject  myproject  [ project_dir ]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;这将在该project_dir目录下创建一个Scrapy项目。如果project_dir没有指定，project_dir将会和myproject名称一样。&lt;/p&gt;

&lt;p&gt;接下来，进入新的项目目录：
&lt;code class=&quot;highlighter-rouge&quot;&gt;cd  project_dir&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;您可以使用scrapy命令从那里管理和控制您的项目。&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;section-3&quot;&gt;控制项目&lt;/h3&gt;

&lt;p&gt;您可以使用scrapy项目内部的工具来控制和管理它们。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;大家不要着急一下子把所以东西都介绍到，具体细节后面都会写到。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;例如，要创建一个新的爬虫：
&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy genspider mydomain mydomain.com&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;通过上述命令创建了一个spider name为mydomain的爬虫，start_urls为&lt;a href=&quot;http://www.cnblogs.com/%E7%9A%84%E7%88%AC%E8%99%AB%E3%80%82&quot;&gt;http://www.cnblogs.com/的爬虫。&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;一些Scrapy命令（如crawl）必须从Scrapy项目内部运行。请参阅命令参考下文中的哪些命令必须从内部项目运行的&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/commands.html#topics-commands-ref&quot;&gt;详细信息&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;还要记住，一些命令在从项目中运行时可能有稍微不同的行为。例如，user_agent如果正在获取的URL与某个特定的爬虫相关联，fetch命令将使用爬虫覆盖的行为（例如属性覆盖用户代理）。这是有意的，因为该fetch命令用于检查爬虫程序如何下载页面。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;section-4&quot;&gt;常用的工具命令&lt;/h2&gt;

&lt;p&gt;此部分包含可用内置命令的列表，其中包含描述和一些用法示例。记住，您可以随时通过运行以下命令获得有关每个命令的更多信息：&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy &amp;lt;command&amp;gt; -h&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;你可以看到所有可用的命令：
&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy  -h&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;上面两种命令，它们只能在Scrapy项目内部工作，也可以全局命令的情况下工作（但它们可能会被项目内的配置覆盖）。&lt;/p&gt;

&lt;p&gt;-
全局命令：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;[startproject]&lt;/li&gt;
  &lt;li&gt;[genspider]&lt;/li&gt;
  &lt;li&gt;[settings]&lt;/li&gt;
  &lt;li&gt;[runspider]&lt;/li&gt;
  &lt;li&gt;[shell]&lt;/li&gt;
  &lt;li&gt;[fetch]&lt;/li&gt;
  &lt;li&gt;[view]&lt;/li&gt;
  &lt;li&gt;[version]&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;-
仅项目命令：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;[crawl]&lt;/li&gt;
  &lt;li&gt;[check]&lt;/li&gt;
  &lt;li&gt;[list]&lt;/li&gt;
  &lt;li&gt;[edit]&lt;/li&gt;
  &lt;li&gt;[parse]&lt;/li&gt;
  &lt;li&gt;[bench]&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;startproject&quot;&gt;startproject&lt;/h3&gt;

&lt;p&gt;语法：
&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy startproject &amp;lt;project_name&amp;gt; [project_dir]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;在目录project_name下创建一个名为的Scrapy项目project_dir 。如果project_dir没有指定，project_dir将会和myproject名称一样。&lt;/p&gt;

&lt;p&gt;用法示例：
&lt;code class=&quot;highlighter-rouge&quot;&gt;$ scrapy startproject myproject&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;genspider&quot;&gt;genspider&lt;/h3&gt;

&lt;p&gt;语法：
&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy genspider [-t template] &amp;lt;name&amp;gt; &amp;lt;domain&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;在当前文件夹或当前项目的spiders文件夹中创建一个新的爬虫，如果从项目中调用。该&lt;name&gt;参数设置为爬虫的name，而&lt;domain&gt;用于生成allowed_domains和start_urls爬虫的属性。&lt;/domain&gt;&lt;/name&gt;&lt;/p&gt;

&lt;p&gt;用法示例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ scrapy genspider -l
Available templates:
  basic
  crawl
  csvfeed
  xmlfeed

$ scrapy genspider example example.com
Created spider 'example' using template 'basic'

$ scrapy genspider -t crawl scrapyorg scrapy.org
Created spider 'scrapyorg' using template 'crawl'
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;这只是一个方便的快捷命令，用于创建基于预定义模板的爬虫，但当然不是唯一的方式来创建爬虫。您可以自己创建爬虫源代码文件，而不是使用此命令。&lt;/p&gt;

&lt;h3 id=&quot;crawl&quot;&gt;crawl&lt;/h3&gt;

&lt;p&gt;语法：
&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy crawl &amp;lt;spider&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;使用爬虫开始爬行。&lt;/p&gt;

&lt;p&gt;用法示例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ scrapy crawl myspider
[ ... myspider starts crawling ... ]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;check&quot;&gt;check&lt;/h3&gt;

&lt;p&gt;语法：
&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy check [-l] &amp;lt;spider&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;用法示例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ scrapy check -l
first_spider
  * parse
  * parse_item
second_spider
  * parse
  * parse_item

$ scrapy check
[FAILED] first_spider:parse_item
&amp;gt;&amp;gt;&amp;gt; 'RetailPricex' field is missing

[FAILED] first_spider:parse
&amp;gt;&amp;gt;&amp;gt; Returned 92 requests, expected 0..4
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;list&quot;&gt;list&lt;/h3&gt;

&lt;p&gt;语法：
&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy list&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;列出当前项目中的所有可用爬虫。每行输出一个爬虫。&lt;/p&gt;

&lt;p&gt;用法示例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ scrapy列表
spider1
spider2
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;edit&quot;&gt;edit&lt;/h3&gt;

&lt;p&gt;语法：
&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy edit &amp;lt;spider&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;此命令仅作为最常见情况的方便快捷方式提供，开发人员当然可以选择任何工具或IDE来编写和调试他的爬虫。&lt;/p&gt;

&lt;p&gt;用法示例：&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$ scrapy edit spider1&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;fetch&quot;&gt;fetch&lt;/h3&gt;

&lt;p&gt;语法：
&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy fetch &amp;lt;url&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;使用Scrapy下载器下载给定的URL，并将内容写入标准输出。&lt;/p&gt;

&lt;p&gt;这个命令的有趣的事情是它获取爬虫下载它的页面。例如，如果爬虫有一个USER_AGENT 属性覆盖用户代理，它将使用那个。&lt;/p&gt;

&lt;p&gt;所以这个命令可以用来“看”你的爬虫如何获取一个页面。&lt;/p&gt;

&lt;p&gt;如果在项目外部使用，将不应用特定的每个爬虫行为，它将只使用默认的Scrapy下载器设置。&lt;/p&gt;

&lt;p&gt;支持的选项：&lt;/p&gt;

&lt;p&gt;–spider=SPIDER：绕过爬虫自动检测和强制使用特定的爬虫
–headers：打印响应的HTTP头，而不是响应的正文
–no-redirect：不遵循HTTP 3xx重定向（默认是遵循它们）&lt;/p&gt;

&lt;p&gt;用法示例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ scrapy fetch --nolog http://www.example.com/some/page.html
[ ... html content here ... ]

$ scrapy fetch --nolog --headers http://www.example.com/
{'Accept-Ranges': ['bytes'],
 'Age': ['1263   '],
 'Connection': ['close     '],
 'Content-Length': ['596'],
 'Content-Type': ['text/html; charset=UTF-8'],
 'Date': ['Wed, 18 Aug 2010 23:59:46 GMT'],
 'Etag': ['&quot;573c1-254-48c9c87349680&quot;'],
 'Last-Modified': ['Fri, 30 Jul 2010 15:30:18 GMT'],
 'Server': ['Apache/2.2.3 (CentOS)']}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;view&quot;&gt;view&lt;/h3&gt;

&lt;p&gt;语法:
&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy view &amp;lt;url&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;在浏览器中打开给定的URL，因为您的Scrapy爬虫会“看到”它。有时，爬虫会看到与普通用户不同的网页，因此可以用来检查爬虫“看到了什么”并确认它是您期望的。&lt;/p&gt;

&lt;p&gt;支持的选项：&lt;/p&gt;

&lt;p&gt;–spider=SPIDER：绕过爬虫自动检测和强制使用特定的爬虫
–no-redirect：不遵循HTTP 3xx重定向（默认是遵循它们）
用法示例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ scrapy view http://www.example.com/some/page.html
[ ... browser starts ... ]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;shell&quot;&gt;shell&lt;/h3&gt;

&lt;p&gt;语法:
&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy shell [url]&lt;/code&gt;
启动给定URL（如果给定）的Scrapy shell，如果没有给出URL，则为空。还支持UNIX样式的本地文件路径，相对于 ./或../前缀或绝对文件路径。有关详细信息，&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/shell.html#topics-shell&quot;&gt;请参阅Scrapy shell。&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;支持的选项：&lt;/p&gt;

&lt;p&gt;–spider=SPIDER：绕过爬虫自动检测和强制使用特定的爬虫
-c code：评估shell中的代码，打印结果并退出
–no-redirect：不遵循HTTP 3xx重定向（默认是遵循它们）; 这只影响你可以在命令行上作为参数传递的URL; 一旦你在shell中，fetch(url)默认情况下仍然会遵循HTTP重定向。
用法示例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ scrapy shell http://www.example.com/some/page.html
[ ... scrapy shell starts ... ]

$ scrapy shell --nolog http://www.example.com/ -c '(response.status, response.url)'
(200, 'http://www.example.com/')

# shell follows HTTP redirects by default
$ scrapy shell --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c '(response.status, response.url)'
(200, 'http://example.com/')

# you can disable this with --no-redirect
# (only for the URL passed as command line argument)
$ scrapy shell --no-redirect --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c '(response.status, response.url)'
(302, 'http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F')
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;parse&quot;&gt;parse&lt;/h3&gt;

&lt;p&gt;语法：
&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy parse &amp;lt;url&amp;gt; [options]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;获取给定的URL并使用处理它的爬虫解析它，使用通过–callback选项传递的方法，或者parse如果没有给出。&lt;/p&gt;

&lt;p&gt;支持的选项：&lt;/p&gt;

&lt;p&gt;–spider=SPIDER：绕过爬虫自动检测和强制使用特定的爬虫
–a NAME=VALUE：set spider argument（可以重复）
–callback或者-c：spider方法用作回调来解析响应
–pipelines：通过管道处理项目
–rules或者-r：使用CrawlSpider 规则来发现用于解析响应的回调（即，spider方法）
–noitems：不显示已抓取的项目
–nolinks：不显示提取的链接
–nocolour：避免使用pygments来着色输出
–depth或-d：请求应递归跟踪的深度级别（默认值：1）
–verbose或-v：显示每个深度级别的信息
用法示例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ scrapy parse http://www.example.com/ -c parse_item
[ ... scrapy log lines crawling example.com spider ... ]

&amp;gt;&amp;gt;&amp;gt; STATUS DEPTH LEVEL 1 &amp;lt;&amp;lt;&amp;lt;
# Scraped Items  ------------------------------------------------------------
[{'name': u'Example item',
 'category': u'Furniture',
 'length': u'12 cm'}]

# Requests  -----------------------------------------------------------------
[]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;settings&quot;&gt;settings&lt;/h3&gt;

&lt;p&gt;语法：
&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy settings [options]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;获取Scrapy设置的值。&lt;/p&gt;

&lt;p&gt;如果在项目中使用，它将显示项目设置值，否则将显示该设置的默认Scrapy值。&lt;/p&gt;

&lt;p&gt;用法示例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ scrapy settings --get BOT_NAME
scrapybot
$ scrapy settings --get DOWNLOAD_DELAY
0
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;runspider&quot;&gt;runspider&lt;/h3&gt;

&lt;p&gt;语法：
&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy runspider &amp;lt;spider_file.py&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;运行一个自包含在Python文件中的爬虫，而不必创建一个项目。&lt;/p&gt;

&lt;p&gt;用法示例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ scrapy runspider myspider.py
[...爬虫开始爬行...]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;version&quot;&gt;version&lt;/h3&gt;

&lt;p&gt;语法：
&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy version [-v]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;打印Scrapy版本。如果使用-v它也打印Python，Twisted和平台信息，这是有用的错误报告。&lt;/p&gt;

&lt;h3 id=&quot;bench&quot;&gt;bench&lt;/h3&gt;

&lt;p&gt;新版本0.17。
语法：
&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy bench&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;运行快速基准测试。&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/benchmarking.html#benchmarking&quot;&gt;基准&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-5&quot;&gt;自定义项目命令&lt;/h3&gt;

&lt;p&gt;您还可以使用COMMANDS_MODULE设置添加自定义项目命令 。有关如何实现命令的示例，请参阅&lt;a href=&quot;https://github.com/scrapy/scrapy/tree/master/scrapy/commands&quot;&gt;scrapy/commands&lt;/a&gt;中的Scrapy命令。&lt;/p&gt;
</content>
<category term="Scrapy" />
<summary>Scrapy爬虫入门教程三 命令行工具介绍和示例</summary>
</entry>
</feed>
