<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.5.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2017-07-23T12:30:13+08:00</updated><id>http://localhost:4000/</id><title type="html">朱智博在Github上的Blog</title><subtitle>朱智博，朱智博的博客，zhuio,zhuio.github.io,</subtitle><entry><title type="html">在安卓手机上安装linux</title><link href="http://localhost:4000/%E5%9C%A8%E5%AE%89%E5%8D%93%E6%89%8B%E6%9C%BA%E4%B8%8A%E5%AE%89%E8%A3%85linux/" rel="alternate" type="text/html" title="在安卓手机上安装linux" /><published>2017-07-23T12:28:00+08:00</published><updated>2017-07-23T12:28:00+08:00</updated><id>http://localhost:4000/%E5%9C%A8%E5%AE%89%E5%8D%93%E6%89%8B%E6%9C%BA%E4%B8%8A%E5%AE%89%E8%A3%85linux</id><content type="html" xml:base="http://localhost:4000/%E5%9C%A8%E5%AE%89%E5%8D%93%E6%89%8B%E6%9C%BA%E4%B8%8A%E5%AE%89%E8%A3%85linux/">&lt;h1 id=&quot;是时候给你的android来一发linux了&quot;&gt;是时候给你的android来一发Linux了&lt;/h1&gt;

&lt;p&gt;﻿先来一张配置好的图片，我主要是用来下pt所以只装了个transmission.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/1498027-4b0d076b2e66f880.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1440/q/50&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;首先准备3个app&lt;/p&gt;

&lt;p&gt;Linux deploy&lt;/p&gt;

&lt;p&gt;VNC viewer&lt;/p&gt;

&lt;p&gt;juiceSSH&lt;/p&gt;

&lt;p&gt;然后打开Linux deploy&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/1498027-f8c1a8335a0eaadc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1440/q/50&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;点击右下角的小箭头&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/1498027-b02c34b376fc4979.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1440/q/50&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上面的设置基本不用动，当然也可以根据自己的需要配置。&lt;/p&gt;

&lt;p&gt;如果要图形界面，把这3项勾选。&lt;/p&gt;

&lt;p&gt;挂载点/sdcard，这样就可以访问sdcard了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/1498027-13e85abf4abbf5ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1440/q/50&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图片发自简书App&lt;/p&gt;

&lt;p&gt;然后返回这里&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/1498027-bfd432b7b519d0d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1440/q/50&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图片发自简书App&lt;/p&gt;

&lt;p&gt;点击右上角3个点&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/1498027-274ac75505e103ed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1440/q/50&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图片发自简书App&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/1498027-bcf3c1b979015878.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1440/q/50&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图片发自简书App&lt;/p&gt;

&lt;p&gt;选择安装就可以了，当出现«&amp;lt;deploy时说明安装成功了，整个过程还是比较简单的。&lt;/p&gt;

&lt;p&gt;要登录图形界面的话需要配置一下VNC&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/1498027-26778b0656614d1c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1440/q/50&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图片发自简书App&lt;/p&gt;

&lt;p&gt;下一步，会要求你填密码，在刚才的配置界面里面。&lt;/p&gt;

&lt;p&gt;然后不出意外你就连接成功了。SSH配置大同小异，不再赘述。&lt;/p&gt;

&lt;p&gt;进去之后会看到桌面，跟电脑一样。&lt;/p&gt;

&lt;p&gt;不过我遇到几个问题，一是浏览器不能用，不过也没啥意义，二是中文乱码。下面讲一下怎么解决。&lt;/p&gt;

&lt;p&gt;1.浏览器打不开我重新装了一个iceweasel,然后默认浏览器也能打开了，不知道为啥，反正是能打开了。&lt;/p&gt;

&lt;p&gt;sudo apt install iceweasel&lt;/p&gt;

&lt;p&gt;2.解决中文字体乱码&lt;/p&gt;

&lt;p&gt;打开终端&lt;/p&gt;

&lt;p&gt;sudo dpkg-reconfigure locales&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/1498027-4bd98c85dbbdc9de.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1440/q/50&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图片发自简书App&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/1498027-6dfd85e212c17c1a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1440/q/50&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图片发自简书App&lt;/p&gt;

&lt;p&gt;按照上面的选，然后下一步。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/1498027-9a50c09a37e2e577.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1440/q/50&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图片发自简书App&lt;/p&gt;

&lt;p&gt;确定。&lt;/p&gt;

&lt;p&gt;然后安装中文字体&lt;/p&gt;

&lt;p&gt;﻿sudo apt-get install ttf-wqy-zenhei&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/1498027-19556a9334ceac8c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1440/q/50&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图片发自简书App&lt;/p&gt;

&lt;p&gt;感谢小溪帮忙，我第一个装不上，不过不影响。&lt;/p&gt;

&lt;p&gt;装完之后重启一下,大功告成。&lt;/p&gt;</content><author><name></name></author><category term="linux deploy" /><category term="android" /><summary type="html">是时候给你的android来一发Linux了</summary></entry><entry><title type="html">简单配置samba服务</title><link href="http://localhost:4000/%E7%AE%80%E5%8D%95%E9%85%8D%E7%BD%AEsamba%E6%9C%8D%E5%8A%A1/" rel="alternate" type="text/html" title="简单配置samba服务" /><published>2017-07-23T12:24:00+08:00</published><updated>2017-07-23T12:24:00+08:00</updated><id>http://localhost:4000/%E7%AE%80%E5%8D%95%E9%85%8D%E7%BD%AEsamba%E6%9C%8D%E5%8A%A1</id><content type="html" xml:base="http://localhost:4000/%E7%AE%80%E5%8D%95%E9%85%8D%E7%BD%AEsamba%E6%9C%8D%E5%8A%A1/">&lt;h1 id=&quot;简单的配置-samba-共享&quot;&gt;简单的配置 samba 共享&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;建立一个要共享的目录&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这里我直接建立了 &lt;code class=&quot;highlighter-rouge&quot;&gt;/home/share&lt;/code&gt; 我是准备把这整个目录作为共享&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo mkdir /home/share
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;然后修改这个目录的权限为 &lt;code class=&quot;highlighter-rouge&quot;&gt;777&lt;/code&gt;， 如果做只读共享可以不用这么大的权限&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo chmod 777 -R /home/share
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;修改 samba 的配置文件&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo vim /etc/samba/smb.conf +
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;vim 在文件参数后面接 &lt;code class=&quot;highlighter-rouge&quot;&gt;+&lt;/code&gt; 可以直接打开到文件末尾，相当于在命令行模式下按 &lt;code class=&quot;highlighter-rouge&quot;&gt;G&lt;/code&gt;
然后直接按 o 可以在下一行插入下面内容&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[share]
    path=/home/share
    available = yes
    browseable = yes
    public = yes
    writable = yes
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Notice: 如果你想让必须知道 samba 帐号密码的用户才可以打开共享目录，可以把 public 设置为 no&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;建立用来访问的  samba 用户&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;useradd share
smbpasswd -a share
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;启动或重启 samba 服务&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo /etc/init.d/samba start        //启动服务
sudo /etc/init.d/samba restart      //重启服务
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;</content><author><name></name></author><category term="samba" /><summary type="html">简单的配置 samba 共享</summary></entry><entry><title type="html">图片爬虫</title><link href="http://localhost:4000/%E5%9B%BE%E7%89%87%E7%88%AC%E8%99%AB/" rel="alternate" type="text/html" title="图片爬虫" /><published>2017-07-04T20:10:00+08:00</published><updated>2017-07-04T20:10:00+08:00</updated><id>http://localhost:4000/%E5%9B%BE%E7%89%87%E7%88%AC%E8%99%AB</id><content type="html" xml:base="http://localhost:4000/%E5%9B%BE%E7%89%87%E7%88%AC%E8%99%AB/">&lt;h2 id=&quot;两个图片爬虫&quot;&gt;两个图片爬虫&lt;/h2&gt;

&lt;h3 id=&quot;meizitucom爬虫&quot;&gt;meizitu.com爬虫&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/zhuio/meizitu&quot;&gt;meizitu.com爬虫地址&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;mmjpgcom爬虫&quot;&gt;mmjpg.com爬虫&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/zhuio/mmjpg-spider&quot;&gt;mmjpg.com爬虫地址&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="爬虫" /><category term="scrapy" /><summary type="html">两个图片爬虫</summary></entry><entry><title type="html">网易云音乐（高品质音乐）（MV）离线脚本</title><link href="http://localhost:4000/%E7%BD%91%E6%98%93%E4%BA%91%E9%9F%B3%E4%B9%90-%E9%AB%98%E5%93%81%E8%B4%A8%E9%9F%B3%E4%B9%90-mv-%E7%A6%BB%E7%BA%BF%E8%84%9A%E6%9C%AC/" rel="alternate" type="text/html" title="网易云音乐（高品质音乐）（MV）离线脚本" /><published>2017-07-04T20:01:00+08:00</published><updated>2017-07-04T20:01:00+08:00</updated><id>http://localhost:4000/%E7%BD%91%E6%98%93%E4%BA%91%E9%9F%B3%E4%B9%90(%E9%AB%98%E5%93%81%E8%B4%A8%E9%9F%B3%E4%B9%90)(mv)%E7%A6%BB%E7%BA%BF%E8%84%9A%E6%9C%AC</id><content type="html" xml:base="http://localhost:4000/%E7%BD%91%E6%98%93%E4%BA%91%E9%9F%B3%E4%B9%90-%E9%AB%98%E5%93%81%E8%B4%A8%E9%9F%B3%E4%B9%90-mv-%E7%A6%BB%E7%BA%BF%E8%84%9A%E6%9C%AC/">&lt;h2 id=&quot;net_music_dt320&quot;&gt;net_music_DT320&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zhuio/net_music_DT320&quot;&gt;github地址&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;第一步：获取列表ID&lt;/p&gt;

&lt;p&gt;到网易云音乐的网页客户端，点开列表网址，在网址栏中找到列表ID&lt;/p&gt;

&lt;p&gt;第二步：&lt;/p&gt;

&lt;p&gt;下载python3和requests库&lt;/p&gt;

&lt;p&gt;第三步：&lt;/p&gt;

&lt;p&gt;python3 net_music_320_Tool.py&lt;/p&gt;

&lt;h2 id=&quot;获取网易云音乐1080p-mv&quot;&gt;获取网易云音乐1080P MV&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zhuio/net_music_mv&quot;&gt;github地址&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;第一步：&lt;/p&gt;

&lt;p&gt;下载python 和requests&lt;/p&gt;

&lt;p&gt;第二步：&lt;/p&gt;

&lt;p&gt;python net_music_mv.py&lt;/p&gt;

&lt;p&gt;第三步：&lt;/p&gt;

&lt;p&gt;输入网易云音乐的歌单ID，如果有1080P MV的话，则下载&lt;/p&gt;</content><author><name></name></author><category term="netmusic" /><summary type="html">net_music_DT320</summary></entry><entry><title type="html">docker</title><link href="http://localhost:4000/docker/" rel="alternate" type="text/html" title="docker" /><published>2017-05-26T11:16:00+08:00</published><updated>2017-05-26T11:16:00+08:00</updated><id>http://localhost:4000/docker</id><content type="html" xml:base="http://localhost:4000/docker/">&lt;p&gt;###docker最基本的使用方法&lt;/p&gt;

&lt;p&gt;####docker国内加速
使用网易hub加速
地址&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  https://c.163.com/wiki/index.php?title=DockerHub%E9%95%9C%E5%83%8F%E5%8A%A0%E9%80%9F
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;ubuntu&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  $ sudo echo &quot;DOCKER_OPTS=\&quot;\$DOCKER_OPTS --registry-mirror=http://hub-mirror.c.163.com\&quot;&quot; &amp;gt;&amp;gt; /etc/default/docker
  $ service docker restart
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;#####建立Dockerfile&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  https://hub.docker.com/
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;####docker 常用命令&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  docker build //建立container
  docker run //运行docker
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;</content><author><name></name></author><category term="docker" /><summary type="html">###docker最基本的使用方法</summary></entry><entry><title type="html">anaconda简单使用</title><link href="http://localhost:4000/anaconda%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/" rel="alternate" type="text/html" title="anaconda简单使用" /><published>2017-05-23T01:19:00+08:00</published><updated>2017-05-23T01:19:00+08:00</updated><id>http://localhost:4000/anaconda%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8</id><content type="html" xml:base="http://localhost:4000/anaconda%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/">&lt;h3 id=&quot;anaconda基本命令&quot;&gt;anaconda基本命令&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  conda create -n pyenv python=3.5
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;anaconda加速&quot;&gt;anaconda加速&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  conda config --add channels 'https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/'
  conda config --set show_channel_urls yes
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;</content><author><name></name></author><category term="anaconda" /><summary type="html">anaconda基本命令</summary></entry><entry><title type="html">Sqlite3学习笔记</title><link href="http://localhost:4000/sqlite3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="alternate" type="text/html" title="Sqlite3学习笔记" /><published>2017-04-10T21:24:00+08:00</published><updated>2017-04-10T21:24:00+08:00</updated><id>http://localhost:4000/sqlite3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0</id><content type="html" xml:base="http://localhost:4000/sqlite3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">&lt;h2 id=&quot;sqlite3学习笔记&quot;&gt;Sqlite3学习笔记&lt;/h2&gt;

&lt;h2 id=&quot;开始&quot;&gt;开始&lt;/h2&gt;

&lt;p&gt;安装:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ubuntu: sudo apt-get install sqlite3&lt;/li&gt;
  &lt;li&gt;Mac: brew install sqlite3&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;sqlite3 dbname: 若库名存在则打开,不存在则创建,不指定则创建临时的库,退出时删除.&lt;/p&gt;

&lt;p&gt;可以直接在命令行执行命令: sqlite3 dbname .tables 或执行sql语句: sqlite3 dbname &quot;select * from tb&quot; (不需要’;’).&lt;/p&gt;

&lt;p&gt;sqlite_master是一个特殊表, 存储有数据库的元信息, 如表(table), 索引(index), 视图(view), 触发器(trigger), 可通过select查询相关信息.&lt;/p&gt;

&lt;p&gt;sql关键字以常用函数, 大小写不敏感, 反正我喜欢都小写, 因为caps lock要给esc.&lt;/p&gt;

&lt;h2 id=&quot;常用命令&quot;&gt;常用命令&lt;/h2&gt;

&lt;p&gt;sqlite3提供的特殊命令, 以.开头:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;.help: 帮助&lt;/li&gt;
  &lt;li&gt;.databases: 列出数据库&lt;/li&gt;
  &lt;li&gt;.tables: 列出表名&lt;/li&gt;
  &lt;li&gt;.open dbname: 打开数据库&lt;/li&gt;
  &lt;li&gt;.save dbname: 保存为数据库&lt;/li&gt;
  &lt;li&gt;.exit: 退出, 或Ctrl-D&lt;/li&gt;
  &lt;li&gt;.schema [tbname]: 列出表, 索引, 触发器的创建语句&lt;/li&gt;
  &lt;li&gt;.output fname.txt: 写结果到文件&lt;/li&gt;
  &lt;li&gt;.show, 显示各种设置的默认值&lt;/li&gt;
  &lt;li&gt;.indices tbname, 列出某表的索引&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;各种设置:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;.echo on&lt;/td&gt;
          &lt;td&gt;off, 开启或关闭命令回显&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;.explain on&lt;/td&gt;
          &lt;td&gt;off, 开启或关闭适合于EXPLAIN的输出模式, 更适合人阅读&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;.headers on&lt;/td&gt;
          &lt;td&gt;off, 是否显示字段信息头&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;.stats on&lt;/td&gt;
          &lt;td&gt;off, 开启或关闭统计信息&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;.timer on&lt;/td&gt;
          &lt;td&gt;off, 开启或关闭命令执行的时间测量&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;模式, 即数据的显示方式, 有很多种, 但常用的就两个, .mode line用于表达式运算, .mode column用于表查询的多字段显示. 第二种通常还会开启信息头, 即字段名显示, .header on. 如果你想知道sql语句的执行时间, 可以.timer on.&lt;/p&gt;

&lt;h2 id=&quot;常用操作&quot;&gt;常用操作&lt;/h2&gt;

&lt;p&gt;创建新表create:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;create table company (
  id int primary key not null autoincrement,
  name text not null,
  age int not null unique,
  address char(50),
  salary real default 50000.00 check(salary&amp;gt;0)
); 注:
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;int, text, real, char(5), 都是字段的类型&lt;/li&gt;
  &lt;li&gt;char(50), 表明此字段存储字符不超过50个&lt;/li&gt;
  &lt;li&gt;not null, 表明此字段不能为空&lt;/li&gt;
  &lt;li&gt;primary key, 表明此字段为基键, 不能重复&lt;/li&gt;
  &lt;li&gt;unique, 确保某列中没有重复值&lt;/li&gt;
  &lt;li&gt;default, 当列没有值时,提供默认值&lt;/li&gt;
  &lt;li&gt;check, 确保某列中的所有值满足一定条件&lt;/li&gt;
  &lt;li&gt;autoincrement, 确保列中值自动增加, 自然无需手动提供&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;修改表alter:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;alter table company add column sex char(1);
--为company表添加列sex, 类型为char(1)
alter table company rename to old_company;
--为表重全名 注:
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Sqlite3中, alter允许用户重命名表,或向现有表添加一个新列&lt;/li&gt;
  &lt;li&gt;但不能重命名列, 删除列, 从表中添加或删除约束&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;删除表drop:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;drop table company;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;创建记录insert:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;insert into company (id, name, age, address, salary) values (1, 'Paul', 32, 'Beijing', 20000.00);
insert into company values (1, 'Paul', 32, 'Beijing', 20000.00);
--插入所有字段时可省略列名
insert into company_bkp select * from company;
--将company表中的所有记录全部插入到company_bkp表中, 两表结构必须相似
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;更新记录update:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;update company set address='Texas' where id==6;
--将id为6的记录更新address字段为Texas
update company set address='Texas', salary=20000.00;
--将所有记录的address字段更新为Texas, salary字段为20000
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;检索记录select:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;select * from company;
select id, name from company;
--查询company表中id和name字段
select tbl_name from sqlite_master where type=='table';
--查询当前数据库存在的表
select current_timestamp;
--查询当前时间戳
select * from company limit 6;
--只显示查询结果的前6行
select * from company limit 3 offset 2;
--只显示从第3行起, 再多2行, 一共3行
select * from company order by salary asc;
--以salary字段升序显示记录, desc为降序
select * from company order by name, salary asc;
--将结果按name和salary字段升序显示, 即name相同的按salary排序
select name, sum(salary) from company group by name;
--将结果中相同name的salary相加, 再构成name, sum(salary)列表
select name, sum(salary) from company group by name order by name;
--同上, 将结果以name升序显示
select * from company group by name having count(name) &amp;lt; 2;
--以name分组, 相同name记录数小于2, having设置分组的过滤条件
select distinct name from company;
--去重, 相同name不显示
select * from company where salary&amp;gt;10000 group by name having count(name)&amp;gt;=2 order by name
--相同name的记录数大于或等于2, 且salary大于10000, 以name升序显示
select * from company cross join department;
select * from company, department;
--将company的每一行与第二个表的每一行进行匹配, 分别有x和y行, 则结果有x*y行, 分别有x和y列, 则结果有x+y列. 交叉连接可能产生非常大的表
select * from company [inner] join department on company.id==department.emp_id;
--选取company的id列与department的emp_id列相等的行进行连接, 内连接是默认连接, 可省略inner, 横向连接
select * from company join department using (id);
--使用两表共有的id列进行相同值连接
select * from company natural join department;
--自动测试存在两个表中的每一列的值之间相等值
select * from company left outer join department on company.id==department.emp_id;
--不同于内连接, 左外连接还会合并进第一个表的非匹配行, 这些行多余的列, 即对应第二个表的列为null. 之所以第一表显示, 因为是left嘛.
select col1, col2, ... from table1 where conditions
union [all]
select col1, col2, ... from table2 where conditions;
--不局限于上面的语句, 事实上union将两个select的结果纵向连接去重.因此这要求结果必须列相同, 列类型相同. join则是横向连接. union all不去重.
select c.id, c.name, c.age, d.dept from company as c, department as d where c.id==d.emp_id;
select c.id, c.name, c.age, d.dept from company as c join department as d on c.id==d.emp_id;
--通过as给表起别名
select * from company where id in (select id from company where salary &amp;gt; 45000);
select * from company where salary &amp;gt; 45000;
--子查询, `()`中的select先执行, 此处两个查询相同, 可与select, insert, update, delete混合使用
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;注:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;where在所选列上设置条件, having在由group by创建的分组上设置条件&lt;/li&gt;
  &lt;li&gt;where -&amp;gt; group by -&amp;gt; having -&amp;gt; order by&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;删除记录delete:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;delete from company where id==7
--删除id为7的记录
delete from company;
--删除所有记录
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;触发器trigger:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;create trigger audit_log after insert on company
begin
insert into audit (emp_id, entry_date) values (new.ID, datetime('now'));
end;
--创建触发器audit_log, 当向company表执行insert操作后, 会触发向audit表插入记录, 值为插入company表的id和执行时间戳.
select name from sqlite_master where type=='trigger' and tbl_name=='company';
--列出关联于company表的触发器
drop trigger audit_log;
--删除触发器 索引index:

create index salary_index on company (salary);
--对company表的salary列创建索引salary_index
select name from sqlite_master where type=='index' and tbl_name=='company';
--列出对应于company表的索引
select * from company indexed by salary_index where salary &amp;gt; 5000;
--使用索引从company表中选择数据
drop index salary_index
--删除索引 注:
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;索引可加快数据检索, 但不利于数据更新和插入&lt;/li&gt;
  &lt;li&gt;索引不影响数据&lt;/li&gt;
  &lt;li&gt;可对多列索引, (col1, col2)&lt;/li&gt;
  &lt;li&gt;创建表时会自动创建主键primary key的索引&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;视图view:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;create view company_view as
select id, name, age from company;
--为company表的id,name,age列创建视图
select * from company_view;
--列出视图的所有数据, 因为其只有真表的id,name,age三个列, 因此这里只列出三列
drop view company_view;
--删除视图 注:
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;可将视图认为是虚表, 它本身不真正存储数据, 它只是提供真正表的一个观察角度&lt;/li&gt;
  &lt;li&gt;因为视图不是真正的表, 因此并不能插入或更新数据, 但可能创建触发器, 当插入或更新数据时, 执行真正的操作.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;事务:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;begin;  --事务开始
delete from company where age==25;  --删除age等于25的所有记录
rollback;  --回滚, 即恢复数据
commit;  --提交更改 注:
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;事务具有原子性, 即事务要么成功要么失败, 而不会停留在中间状态&lt;/li&gt;
  &lt;li&gt;事务只与insert, update, delete一起使用&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;表达式&quot;&gt;表达式&lt;/h2&gt;

&lt;p&gt;算术运算符, + - * / %, 加减乘除余.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;select 10+20;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;逻辑运算符: ==, !=, &amp;gt;, &amp;lt;, &amp;gt;=, &amp;lt;=. and, or&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;where col1 &amp;gt;= 25 and col2 &amp;lt;= 90
where col is not null
where col like 'Ki%'  
--字段为Ki开头的字串, %:零或一或多个, _:一个
where col glob 'Ki*'  
--同上, 大小写敏感, *:零或一或多个, ?:一个
where col in (25, 27)  
--字段为25或者27
where col not in (25, 27)  
--字段不是25也不是27
where col between 25 and 27  
--字段在25到27之间
select age from company
where exists (select age from company where salary &amp;gt; 65000)
-- 子查询, 如果存在salary大于65000的age字段, 则列出所有age字段 位运算符: &amp;amp;amp; | ~ &amp;amp;lt;&amp;amp;lt; &amp;amp;gt;&amp;amp;gt;, 并或反左右移.

select 60 | 13
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;null值, 只能用where col is null/not null, 而不能跟别的值比较. null值与零值或包含空格的字段是不同的, null是没有值, 而非值为空.&lt;/p&gt;

&lt;h2 id=&quot;时间函数&quot;&gt;时间函数&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;date, 日期&lt;/li&gt;
  &lt;li&gt;time, 时间&lt;/li&gt;
  &lt;li&gt;datetime, 日期和时间&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;strtime, 格式化字串&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;select date('now');
select strtime('%s', 'now');
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;常用函数&quot;&gt;常用函数&lt;/h2&gt;

&lt;p&gt;sqlite提供了少量常用的函数:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;count, 谋算表的行数&lt;/li&gt;
  &lt;li&gt;max, min, 选择某列的最大值, 最小值&lt;/li&gt;
  &lt;li&gt;avg, 计算某列的平均值&lt;/li&gt;
  &lt;li&gt;sum, 计算某列的总和&lt;/li&gt;
  &lt;li&gt;random, 返回伪随机数&lt;/li&gt;
  &lt;li&gt;abs, 返回绝对值, 所有字串返回0.0&lt;/li&gt;
  &lt;li&gt;upper, 将字符串转换为大写字母&lt;/li&gt;
  &lt;li&gt;lower, 将字符串转换为小写字母&lt;/li&gt;
  &lt;li&gt;length, 返回字串的长度&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;sqlite_version, 返回sqlite的版本&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;select count(*) from company;
--company表的行数, 注意, 指定特定列时, 为null值的记录不计数
select max(salary) from company;
--选择company表的salary列的最大值
select avg(salary) from company;
select sum(salary) from company;
select random();
select abs(-5);
select upper(name) from company;
--列出company表的name列的大写
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="Sqlite3" /><summary type="html">Sqlite3学习笔记</summary></entry><entry><title type="html">Scrapy爬虫入门教程十二 Link Extractors（链接提取器）</title><link href="http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E5%8D%81%E4%BA%8C-link-extractors-%E9%93%BE%E6%8E%A5%E6%8F%90%E5%8F%96%E5%99%A8/" rel="alternate" type="text/html" title="Scrapy爬虫入门教程十二 Link Extractors（链接提取器）" /><published>2017-04-09T14:34:00+08:00</published><updated>2017-04-09T14:34:00+08:00</updated><id>http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E5%8D%81%E4%BA%8C-link-extractors(%E9%93%BE%E6%8E%A5%E6%8F%90%E5%8F%96%E5%99%A8)</id><content type="html" xml:base="http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E5%8D%81%E4%BA%8C-link-extractors-%E9%93%BE%E6%8E%A5%E6%8F%90%E5%8F%96%E5%99%A8/">&lt;h1 id=&quot;scrapy爬虫入门教程十二-link-extractors链接提取器&quot;&gt;Scrapy爬虫入门教程十二 Link Extractors（链接提取器）&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;开发环境：&lt;/strong&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;Python 3.6.0 版本&lt;/code&gt; （当前最新）
&lt;code class=&quot;highlighter-rouge&quot;&gt;Scrapy 1.3.2 版本&lt;/code&gt; （当前最新）&lt;/p&gt;

&lt;h1 id=&quot;链接提取器&quot;&gt;链接提取器&lt;/h1&gt;

&lt;p&gt;链接提取器是其唯一目的是从&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy.http.Response&lt;/code&gt;最终将跟随的网页（对象）提取链接的对象。&lt;/p&gt;

&lt;p&gt;有Scrapy，但你可以创建自己的自定义链接提取器，以满足您的需求通​​过实现一个简单的界面。&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy.linkextractors import LinkExtractor&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;每个链接提取器唯一的公共方法是&lt;code class=&quot;highlighter-rouge&quot;&gt;extract_links&lt;/code&gt;接收一个Response对象并返回一个&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy.link.Link&lt;/code&gt;对象列表。链接提取器意在被实例化一次，并且它们的&lt;code class=&quot;highlighter-rouge&quot;&gt;extract_links&lt;/code&gt;方法被调用几次，具有不同的响应以提取跟随的链接。&lt;/p&gt;

&lt;p&gt;链接提取程序&lt;code class=&quot;highlighter-rouge&quot;&gt;CrawlSpider&lt;/code&gt; 通过一组规则在类中使用（可以在Scrapy中使用），但是您也可以在爬虫中使用它，即使不从其中&lt;code class=&quot;highlighter-rouge&quot;&gt;CrawlSpider&lt;/code&gt;提取子类 ，因为其目的非常简单：提取链接。&lt;/p&gt;

&lt;h2 id=&quot;内置链接提取器参考&quot;&gt;内置链接提取器参考&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;scrapy.linkextractors&lt;/code&gt;模块中提供了与Scrapy捆绑在一起的链接提取器类 。&lt;/p&gt;

&lt;p&gt;默认的链接提取器是&lt;code class=&quot;highlighter-rouge&quot;&gt;LinkExtractor&lt;/code&gt;，它是相同的 &lt;code class=&quot;highlighter-rouge&quot;&gt;LxmlLinkExtractor&lt;/code&gt;：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from scrapy.linkextractors import LinkExtractor
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;以前的Scrapy版本中曾经有过其他链接提取器类，但现在已经过时了。&lt;/p&gt;

&lt;h3 id=&quot;lxmllinkextractor&quot;&gt;LxmlLinkExtractor&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor(allow=(), deny=(), allow_domains=(), deny_domains=(), deny_extensions=None, restrict_xpaths=(), restrict_css=(), tags=('a', 'area'), attrs=('href', ), canonicalize=True, unique=True, process_value=None)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;LxmlLinkExtractor是推荐的链接提取器与方便的过滤选项。它使用lxml的强大的HTMLParser实现。&lt;/p&gt;

&lt;p&gt;**参数：    **&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;allow（正则表达式（或的列表）） - 一个单一的正则表达式（或正则表达式列表），（绝对）urls必须匹配才能提取。如果没有给出（或为空），它将匹配所有链接。&lt;/li&gt;
  &lt;li&gt;deny（正则表达式或正则表达式列表） - 一个正则表达式（或正则表达式列表），（绝对）urls必须匹配才能排除（即不提取）。它优先于allow参数。如果没有给出（或为空），它不会排除任何链接。&lt;/li&gt;
  &lt;li&gt;allow_domains（str或list） - 单个值或包含将被考虑用于提取链接的域的字符串列表&lt;/li&gt;
  &lt;li&gt;deny_domains（str或list） - 单个值或包含不会被考虑用于提取链接的域的字符串列表&lt;/li&gt;
  &lt;li&gt;deny_extensions（list） - 包含在提取链接时应该忽略的扩展的单个值或字符串列表。如果没有给出，它将默认为IGNORED_EXTENSIONS在&lt;a href=&quot;https://github.com/scrapy/scrapy/blob/master/scrapy/linkextractors/__init__.py&quot;&gt;scrapy.linkextractors&lt;/a&gt;包中定义的 列表 。&lt;/li&gt;
  &lt;li&gt;restrict_xpaths（str或list） - 是一个XPath（或XPath的列表），它定义响应中应从中提取链接的区域。如果给出，只有那些XPath选择的文本将被扫描链接。参见下面的例子。&lt;/li&gt;
  &lt;li&gt;restrict_css（str或list） - 一个CSS选择器（或选择器列表），用于定义响应中应提取链接的区域。有相同的行为restrict_xpaths。
标签（str或list） - 标签或在提取链接时要考虑的标签列表。默认为。(‘a’, ‘area’)&lt;/li&gt;
  &lt;li&gt;attrs（list） - 在查找要提取的链接时应该考虑的属性或属性列表（仅适用于参数中指定的那些标签tags ）。默认为(‘href’,)&lt;/li&gt;
  &lt;li&gt;canonicalize（boolean） - 规范化每个提取的url（使用w3lib.url.canonicalize_url）。默认为True。&lt;/li&gt;
  &lt;li&gt;unique（boolean） - 是否应对提取的链接应用重复过滤。&lt;/li&gt;
  &lt;li&gt;process_value（callable） -
接收从标签提取的每个值和扫描的属性并且可以修改值并返回新值的函数，或者返回None以完全忽略链接。如果没有给出，process_value默认为。lambda x: x&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;例如，要从此代码中提取链接：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;a href=&quot;javascript:goToPage('../other/page.html'); return false&quot;&amp;gt;Link text&amp;lt;/a&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;您可以使用以下功能process_value：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def process_value(value):
    m = re.search(&quot;javascript:goToPage\('(.*?)'&quot;, value)
    if m:
        return m.group(1)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;</content><author><name></name></author><category term="Scrapy" /><summary type="html">Scrapy爬虫入门教程十二 Link Extractors（链接提取器）</summary></entry><entry><title type="html">Scrapy爬虫入门教程十一 Request和Response（请求和响应）</title><link href="http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E5%8D%81%E4%B8%80-request%E5%92%8Cresponse-%E8%AF%B7%E6%B1%82%E5%92%8C%E5%93%8D%E5%BA%94/" rel="alternate" type="text/html" title="Scrapy爬虫入门教程十一 Request和Response（请求和响应）" /><published>2017-04-09T14:33:00+08:00</published><updated>2017-04-09T14:33:00+08:00</updated><id>http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E5%8D%81%E4%B8%80-request%E5%92%8Cresponse(%E8%AF%B7%E6%B1%82%E5%92%8C%E5%93%8D%E5%BA%94)</id><content type="html" xml:base="http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E5%8D%81%E4%B8%80-request%E5%92%8Cresponse-%E8%AF%B7%E6%B1%82%E5%92%8C%E5%93%8D%E5%BA%94/">&lt;h1 id=&quot;scrapy爬虫入门教程十一-request和response请求和响应&quot;&gt;Scrapy爬虫入门教程十一 Request和Response（请求和响应）&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;开发环境：&lt;/strong&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;Python 3.6.0 版本&lt;/code&gt; （当前最新）
&lt;code class=&quot;highlighter-rouge&quot;&gt;Scrapy 1.3.2 版本&lt;/code&gt; （当前最新）&lt;/p&gt;

&lt;h1 id=&quot;请求和响应&quot;&gt;请求和响应&lt;/h1&gt;

&lt;p&gt;Scrapy的Request和Response对象用于爬网网站。&lt;/p&gt;

&lt;p&gt;通常，Request对象在爬虫程序中生成并传递到系统，直到它们到达下载程序，后者执行请求并返回一个Response对象，该对象返回到发出请求的爬虫程序。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;上面一段话比较拗口，有web经验的同学，应该都了解的，不明白看下面的图大概理解下。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;爬虫-&amp;gt;Request:创建
Request-&amp;gt;Response:获取下载数据
Response-&amp;gt;爬虫:数据
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/2880699-f6847c1d6ebf140a.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;两个类Request和Response类都有一些子类，它们添加基类中不需要的功能。这些在下面的请求子类和 响应子类中描述。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;request-objects&quot;&gt;Request objects&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class scrapy.http.Request(url[, callback, method='GET', headers, body, cookies, meta, encoding='utf-8', priority=0, dont_filter=False, errback])
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;一个Request对象表示一个HTTP请求，它通常是在爬虫生成，并由下载执行，从而生成Response。&lt;/p&gt;

&lt;p&gt;-
参数：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;url（string）&lt;/code&gt; - 此请求的网址&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;callback（callable）&lt;/code&gt; - 将使用此请求的响应（一旦下载）作为其第一个参数调用的函数。有关更多信息，请参阅下面的&lt;a href=&quot;https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-response-ref-request-callback-arguments&quot;&gt;将附加数据传递给回调函数&lt;/a&gt;。如果请求没有指定回调，parse()将使用spider的 方法。请注意，如果在处理期间引发异常，则会调用errback。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;method（string）&lt;/code&gt; - 此请求的HTTP方法。默认为’GET’。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;meta（dict）&lt;/code&gt; - 属性的初始值Request.meta。如果给定，在此参数中传递的dict将被浅复制。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;body（str或unicode）&lt;/code&gt; - 请求体。如果unicode传递了a，那么它被编码为 str使用传递的编码（默认为utf-8）。如果 body没有给出，则存储一个空字符串。不管这个参数的类型，存储的最终值将是一个str（不会是unicode或None）。&lt;/li&gt;
  &lt;li&gt;
    &lt;h2 id=&quot;headersdict---这个请求的头dict值可以是字符串对于单值标头或列表对于多值标头如果-none作为值传递则不会发送http头&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;headers（dict）&lt;/code&gt; - 这个请求的头。dict值可以是字符串（对于单值标头）或列表（对于多值标头）。如果 None作为值传递，则不会发送HTTP头。&lt;/h2&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cookie（dict或list）&lt;/code&gt; - 请求cookie。这些可以以两种形式发送。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;使用dict：&lt;/p&gt;

    &lt;p&gt;request_with_cookies = Request(url=”http://www.example.com”,
                                 cookies={‘currency’: ‘USD’, ‘country’: ‘UY’})&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  * 使用列表：

  ```
  request_with_cookies = Request(url=&quot;http://www.example.com&quot;,
                                 cookies=[{'name': 'currency',
                                          'value': 'USD',
                                          'domain': 'example.com',
                                          'path': '/currency'}])
  ```
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;后一种形式允许定制 cookie的属性domain和path属性。这只有在保存Cookie用于以后的请求时才有用。&lt;/p&gt;

&lt;p&gt;当某些网站返回Cookie（在响应中）时，这些Cookie会存储在该域的Cookie中，并在将来的请求中再次发送。这是任何常规网络浏览器的典型行为。但是，如果由于某种原因，您想要避免与现有Cookie合并，您可以通过将dont_merge_cookies关键字设置为True 来指示Scrapy如此操作 Request.meta。&lt;/p&gt;

&lt;p&gt;不合并Cookie的请求示例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;request_with_cookies = Request(url=&quot;http://www.example.com&quot;,
                               cookies={'currency': 'USD', 'country': 'UY'},
                               meta={'dont_merge_cookies': True})
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;有关详细信息，请参阅&lt;a href=&quot;https://doc.scrapy.org/en/1.3/topics/downloader-middleware.html#cookies-mw&quot;&gt;CookiesMiddleware&lt;/a&gt;。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;encoding（string）&lt;/code&gt; - 此请求的编码（默认为’utf-8’）。此编码将用于对URL进行百分比编码，并将正文转换为str（如果给定unicode）。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;priority（int）&lt;/code&gt; - 此请求的优先级（默认为0）。调度器使用优先级来定义用于处理请求的顺序。具有较高优先级值的请求将较早执行。允许负值以指示相对低优先级。&lt;/li&gt;
  &lt;li&gt;
    &lt;h2 id=&quot;dont_filterboolean---表示此请求不应由调度程序过滤当您想要多次执行相同的请求时忽略重复过滤器时使用小心使用它或者你会进入爬行循环默认为false&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;dont_filter（boolean）&lt;/code&gt; - 表示此请求不应由调度程序过滤。当您想要多次执行相同的请求时忽略重复过滤器时使用。小心使用它，或者你会进入爬行循环。默认为False。&lt;/h2&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;errback（callable）&lt;/code&gt; - 如果在处理请求时引发任何异常，将调用的函数。这包括失败的404 HTTP错误等页面。它接收一个&lt;a href=&quot;https://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html&quot;&gt;Twisted Failure&lt;/a&gt;实例作为第一个参数。有关更多信息，请参阅&lt;a href=&quot;https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-response-ref-errbacks&quot;&gt;使用errbacks在请求处理&lt;/a&gt;中&lt;a href=&quot;https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-response-ref-errbacks&quot;&gt;捕获异常&lt;/a&gt;。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;-
&lt;code class=&quot;highlighter-rouge&quot;&gt;url&lt;/code&gt;
包含此请求的网址的字符串。请记住，此属性包含转义的网址，因此它可能与构造函数中传递的网址不同。&lt;/p&gt;

&lt;p&gt;此属性为只读。更改请求使用的URL &lt;code class=&quot;highlighter-rouge&quot;&gt;replace()&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;-
&lt;code class=&quot;highlighter-rouge&quot;&gt;method&lt;/code&gt;
表示请求中的HTTP方法的字符串。这保证是大写的。例如：&lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;GET&quot;，&quot;POST&quot;，&quot;PUT&quot;&lt;/code&gt;等&lt;/p&gt;

&lt;p&gt;-
&lt;code class=&quot;highlighter-rouge&quot;&gt;headers&lt;/code&gt;
包含请求标头的类似字典的对象。&lt;/p&gt;

&lt;p&gt;-
&lt;code class=&quot;highlighter-rouge&quot;&gt;body&lt;/code&gt;
包含请求正文的str。&lt;/p&gt;

&lt;p&gt;此属性为只读。更改请求使用的正文 &lt;code class=&quot;highlighter-rouge&quot;&gt;replace()&lt;/code&gt;。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;meta&lt;/code&gt;
包含此请求的任意元数据的字典。此dict对于新请求为空，通常由不同的Scrapy组件（扩展程序，中间件等）填充。因此，此dict中包含的数据取决于您启用的扩展。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;有关&lt;a href=&quot;https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-meta&quot;&gt;Scrapy识别&lt;/a&gt;的特殊元键列表，请参阅&lt;a href=&quot;https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-meta&quot;&gt;Request.meta特殊键&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;当使用or 方法克隆请求时，此dict是&lt;a href=&quot;https://docs.python.org/2/library/copy.html&quot;&gt;浅复制&lt;/a&gt;的 ，并且也可以在您的爬虫中从属性访问。copy()replace()response.meta&lt;/p&gt;

&lt;p&gt;-
&lt;code class=&quot;highlighter-rouge&quot;&gt;copy（）&lt;/code&gt;
返回一个新的请求，它是这个请求的副本。另请参见： &lt;a href=&quot;https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-response-ref-request-callback-arguments&quot;&gt;将附加数据传递到回调函数&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;-
&lt;code class=&quot;highlighter-rouge&quot;&gt;replace([url, method, headers, body, cookies, meta, encoding, dont_filter, callback, errback])&lt;/code&gt;
返回具有相同成员的Request对象，但通过指定的任何关键字参数赋予新值的成员除外。该属性Request.meta是默认复制（除非新的值在给定的meta参数）。另请参见 &lt;a href=&quot;https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-response-ref-request-callback-arguments&quot;&gt;将附加数据传递给回调函数&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;将附加数据传递给回调函数&quot;&gt;将附加数据传递给回调函数&lt;/h3&gt;

&lt;p&gt;请求的回调是当下载该请求的响应时将被调用的函数。将使用下载的Response对象作为其第一个参数来调用回调函数。&lt;/p&gt;

&lt;p&gt;例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def parse_page1(self, response):
    return scrapy.Request(&quot;http://www.example.com/some_page.html&quot;,
                          callback=self.parse_page2)

def parse_page2(self, response):
    # this would log http://www.example.com/some_page.html
    self.logger.info(&quot;Visited %s&quot;, response.url)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;在某些情况下，您可能有兴趣向这些回调函数传递参数，以便稍后在第二个回调中接收参数。您可以使用该&lt;code class=&quot;highlighter-rouge&quot;&gt;Request.meta&lt;/code&gt;属性。&lt;/p&gt;

&lt;p&gt;以下是使用此机制传递项目以填充来自不同页面的不同字段的示例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def parse_page1(self, response):
    item = MyItem()
    item['main_url'] = response.url
    request = scrapy.Request(&quot;http://www.example.com/some_page.html&quot;,
                             callback=self.parse_page2)
    request.meta['item'] = item
    yield request

def parse_page2(self, response):
    item = response.meta['item']
    item['other_url'] = response.url
    yield item
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;使用errbacks在请求处理中捕获异常&quot;&gt;使用errbacks在请求处理中捕获异常&lt;/h3&gt;

&lt;p&gt;请求的errback是在处理异常时被调用的函数。&lt;/p&gt;

&lt;p&gt;它接收一个&lt;a href=&quot;https://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html&quot;&gt;Twisted Failure&lt;/a&gt;实例作为第一个参数，并可用于跟踪连接建立超时，DNS错误等。&lt;/p&gt;

&lt;p&gt;这里有一个示例爬虫记录所有错误，并捕获一些特定的错误，如果需要：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import scrapy

from scrapy.spidermiddlewares.httperror import HttpError
from twisted.internet.error import DNSLookupError
from twisted.internet.error import TimeoutError, TCPTimedOutError

class ErrbackSpider(scrapy.Spider):
    name = &quot;errback_example&quot;
    start_urls = [
        &quot;http://www.httpbin.org/&quot;,              # HTTP 200 expected
        &quot;http://www.httpbin.org/status/404&quot;,    # Not found error
        &quot;http://www.httpbin.org/status/500&quot;,    # server issue
        &quot;http://www.httpbin.org:12345/&quot;,        # non-responding host, timeout expected
        &quot;http://www.httphttpbinbin.org/&quot;,       # DNS error expected
    ]

    def start_requests(self):
        for u in self.start_urls:
            yield scrapy.Request(u, callback=self.parse_httpbin,
                                    errback=self.errback_httpbin,
                                    dont_filter=True)

    def parse_httpbin(self, response):
        self.logger.info('Got successful response from {}'.format(response.url))
        # do something useful here...

    def errback_httpbin(self, failure):
        # log all failures
        self.logger.error(repr(failure))

        # in case you want to do something special for some errors,
        # you may need the failure's type:

        if failure.check(HttpError):
            # these exceptions come from HttpError spider middleware
            # you can get the non-200 response
            response = failure.value.response
            self.logger.error('HttpError on %s', response.url)

        elif failure.check(DNSLookupError):
            # this is the original request
            request = failure.request
            self.logger.error('DNSLookupError on %s', request.url)

        elif failure.check(TimeoutError, TCPTimedOutError):
            request = failure.request
            self.logger.error('TimeoutError on %s', request.url)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;requestmeta特殊键&quot;&gt;Request.meta特殊键&lt;/h2&gt;

&lt;p&gt;该&lt;code class=&quot;highlighter-rouge&quot;&gt;Request.meta&lt;/code&gt;属性可以包含任何任意数据，但有一些特殊的键由Scrapy及其内置扩展识别。&lt;/p&gt;

&lt;p&gt;那些是：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dont_redirect
dont_retry
handle_httpstatus_list
handle_httpstatus_all
dont_merge_cookies（参见cookies构造函数的Request参数）
cookiejar
dont_cache
redirect_urls
bindaddress
dont_obey_robotstxt
download_timeout
download_maxsize
download_latency
proxy
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;bindaddress&quot;&gt;bindaddress&lt;/h4&gt;

&lt;p&gt;用于执行请求的出站IP地址的IP。&lt;/p&gt;

&lt;h4 id=&quot;download_timeout&quot;&gt;download_timeout&lt;/h4&gt;

&lt;p&gt;下载器在超时前等待的时间量（以秒为单位）。参见：&lt;code class=&quot;highlighter-rouge&quot;&gt;DOWNLOAD_TIMEOUT&lt;/code&gt;。&lt;/p&gt;

&lt;h4 id=&quot;download_latency&quot;&gt;download_latency&lt;/h4&gt;

&lt;p&gt;自请求已启动以来，用于获取响应的时间量，即通过网络发送的HTTP消息。此元键仅在响应已下载时可用。虽然大多数其他元键用于控制Scrapy行为，但这应该是只读的。&lt;/p&gt;

&lt;h2 id=&quot;请求子类&quot;&gt;请求子类&lt;/h2&gt;

&lt;p&gt;这里是内置子类的Request列表。您还可以将其子类化以实现您自己的自定义功能。&lt;/p&gt;

&lt;p&gt;FormRequest对象
FormRequest类扩展了Request具有处理HTML表单的功能的基础。它使用lxml.html表单 从Response对象的表单数据预填充表单字段。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;class scrapy.http.FormRequest(url[, formdata, ...])&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;本&lt;code class=&quot;highlighter-rouge&quot;&gt;FormRequest&lt;/code&gt;类增加了新的构造函数的参数。其余的参数与&lt;code class=&quot;highlighter-rouge&quot;&gt;Request&lt;/code&gt;类相同，这里没有记录。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;参数：&lt;strong&gt;formdata&lt;/strong&gt;（元组的dict或iterable） - 是一个包含HTML Form数据的字典（或（key，value）元组的迭代），它将被url编码并分配给请求的主体。
该&lt;code class=&quot;highlighter-rouge&quot;&gt;FormRequest&lt;/code&gt;对象支持除标准以下类方法Request的方法：&lt;/p&gt;

    &lt;p&gt;classmethod from_response(response[, formname=None, formid=None, formnumber=0, formdata=None, formxpath=None, formcss=None, clickdata=None, dont_click=False, …])&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;返回一个新&lt;code class=&quot;highlighter-rouge&quot;&gt;FormRequest&lt;/code&gt;对象，其中的表单字段值已预先&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;form&amp;gt;&lt;/code&gt;填充在给定响应中包含的HTML 元素中。有关示例，请参阅 &lt;a href=&quot;https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-response-ref-request-userlogin&quot;&gt;使用FormRequest.from_response（）来模拟用户登录&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;该策略是在任何可查看的表单控件上默认自动模拟点击，如a 。即使这是相当方便，并且经常想要的行为，有时它可能导致难以调试的问题。例如，当使用使用javascript填充和/或提交的表单时，默认行为可能不是最合适的。要禁用此行为，您可以将参数设置 为。此外，如果要更改单击的控件（而不是禁用它），您还可以使用 参数。&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;input type=&quot;submit&quot;&amp;gt; from_response() dont_click True clickdata&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;参数：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;response（Responseobject） - 包含将用于预填充表单字段的HTML表单的响应&lt;/li&gt;
  &lt;li&gt;formname（string） - 如果给定，将使用name属性设置为此值的形式。&lt;/li&gt;
  &lt;li&gt;formid（string） - 如果给定，将使用id属性设置为此值的形式。&lt;/li&gt;
  &lt;li&gt;formxpath（string） - 如果给定，将使用匹配xpath的第一个表单。&lt;/li&gt;
  &lt;li&gt;formcss（string） - 如果给定，将使用匹配css选择器的第一个形式。&lt;/li&gt;
  &lt;li&gt;formnumber（integer） - 当响应包含多个表单时要使用的表单的数量。第一个（也是默认）是0。&lt;/li&gt;
  &lt;li&gt;formdata（dict） - 要在表单数据中覆盖的字段。如果响应&amp;lt;form&amp;gt;元素中已存在字段，则其值将被在此参数中传递的值覆盖。&lt;/li&gt;
  &lt;li&gt;clickdata（dict） - 查找控件被点击的属性。如果没有提供，表单数据将被提交，模拟第一个可点击元素的点击。除了html属性，控件可以通过其相对于表单中其他提交表输入的基于零的索引，通过nr属性来标识。&lt;/li&gt;
  &lt;li&gt;dont_click（boolean） - 如果为True，表单数据将在不点击任何元素的情况下提交。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;这个类方法的其他参数直接传递给 FormRequest构造函数。
在新版本0.10.3：该formname参数。
在新版本0.17：该formxpath参数。
新的版本1.1.0：该formcss参数。
新的版本1.1.0：该formid参数。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;请求使用示例&quot;&gt;请求使用示例&lt;/h3&gt;

&lt;h4 id=&quot;使用formrequest通过http-post发送数据&quot;&gt;使用FormRequest通过HTTP POST发送数据&lt;/h4&gt;

&lt;p&gt;如果你想在你的爬虫中模拟HTML表单POST并发送几个键值字段，你可以返回一个FormRequest对象（从你的爬虫）像这样：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;return [FormRequest(url=&quot;http://www.example.com/post/action&quot;,
                    formdata={'name': 'John Doe', 'age': '27'},
                    callback=self.after_post)]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;使用formrequestfrom_response来模拟用户登录&quot;&gt;使用FormRequest.from_response（）来模拟用户登录&lt;/h4&gt;

&lt;p&gt;网站通常通过元素（例如会话相关数据或认证令牌（用于登录页面））提供预填充的表单字段。进行剪贴时，您需要自动预填充这些字段，并且只覆盖其中的一些，例如用户名和密码。您可以使用 此作业的方法。这里有一个使用它的爬虫示例：&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;input type=&quot;hidden&quot;&amp;gt; FormRequest.from_response()&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import scrapy

class LoginSpider(scrapy.Spider):
    name = 'example.com'
    start_urls = ['http://www.example.com/users/login.php']

    def parse(self, response):
        return scrapy.FormRequest.from_response(
            response,
            formdata={'username': 'john', 'password': 'secret'},
            callback=self.after_login
        )

    def after_login(self, response):
        # check login succeed before going on
        if &quot;authentication failed&quot; in response.body:
            self.logger.error(&quot;Login failed&quot;)
            return

        # continue scraping with authenticated session...
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h1 id=&quot;响应对象&quot;&gt;响应对象&lt;/h1&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;class scrapy.http.Response(url[, status=200, headers=None, body=b'', flags=None, request=None])&lt;/code&gt;
一个Response对象表示的HTTP响应，这通常是下载（由下载），并供给到爬虫进行处理。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;参数：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;url（string） - 此响应的URL&lt;/li&gt;
  &lt;li&gt;status（integer） - 响应的HTTP状态。默认为&lt;strong&gt;200&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;headers（dict） - 这个响应的头。dict值可以是字符串（对于单值标头）或列表（对于多值标头）。&lt;/li&gt;
  &lt;li&gt;body（str） - 响应体。它必须是str，而不是unicode，除非你使用一个编码感知&lt;a href=&quot;https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-response-ref-response-subclasses&quot;&gt;响应子类&lt;/a&gt;，如 &lt;code class=&quot;highlighter-rouge&quot;&gt;TextResponse&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;flags（&lt;a href=&quot;https://doc.scrapy.org/en/1.3/topics/api.html#scrapy.loader.SpiderLoader.list&quot;&gt;list&lt;/a&gt;） - 是一个包含属性初始值的 &lt;code class=&quot;highlighter-rouge&quot;&gt;Response.flags&lt;/code&gt;列表。如果给定，列表将被浅复制。&lt;/li&gt;
  &lt;li&gt;request（Requestobject） - 属性的初始值&lt;code class=&quot;highlighter-rouge&quot;&gt;Response.request&lt;/code&gt;。这代表Request生成此响应。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;url&lt;/strong&gt;
包含响应的URL的字符串。&lt;/p&gt;

&lt;p&gt;此属性为只读。更改响应使用的URL replace()。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;status&lt;/strong&gt;
表示响应的HTTP状态的整数。示例：200， 404。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;headers&lt;/strong&gt;
包含响应标题的类字典对象。可以使用get()返回具有指定名称的第一个标头值或getlist()返回具有指定名称的所有标头值来访问值。例如，此调用会为您提供标题中的所有Cookie：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;response.headers.getlist('Set-Cookie')
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;body&lt;/strong&gt;
本回复的正文。记住Response.body总是一个字节对象。如果你想unicode版本使用 TextResponse.text（只在TextResponse 和子类中可用）。&lt;/p&gt;

&lt;p&gt;此属性为只读。更改响应使用的主体 replace()。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;request&lt;/strong&gt;
Request生成此响应的对象。在响应和请求通过所有&lt;a href=&quot;https://doc.scrapy.org/en/1.3/topics/downloader-middleware.html#topics-downloader-middleware&quot;&gt;下载中间件&lt;/a&gt;后，此属性在Scrapy引擎中分配。特别地，这意味着：&lt;/p&gt;

&lt;p&gt;HTTP重定向将导致将原始请求（重定向之前的URL）分配给重定向响应（重定向后具有最终URL）。
Response.request.url并不总是等于Response.url
此属性仅在爬虫程序代码和 &lt;a href=&quot;https://doc.scrapy.org/en/1.3/topics/spider-middleware.html#topics-spider-middleware&quot;&gt;Spider Middleware&lt;/a&gt;中可用，但不能在Downloader Middleware中使用（尽管您有通过其他方式可用的请求）和处理程序response_downloaded。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;meta&lt;/strong&gt;
的快捷方式Request.meta的属性 Response.request对象（即self.request.meta）。&lt;/p&gt;

&lt;p&gt;与Response.request属性不同，Response.meta 属性沿重定向和重试传播，因此您将获得Request.meta从您的爬虫发送的原始属性。&lt;/p&gt;

&lt;p&gt;也可以看看&lt;/p&gt;

&lt;p&gt;Request.meta 属性&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;flags&lt;/strong&gt;
包含此响应的标志的列表。标志是用于标记响应的标签。例如：’cached’，’redirected ‘等等。它们显示在Response（** str** 方法）的字符串表示上，它被引擎用于日志记录。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;copy（）&lt;/strong&gt;
返回一个新的响应，它是此响应的副本。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;replace（[ url，status，headers，body，request，flags，cls ] ）&lt;/strong&gt;
返回具有相同成员的Response对象，但通过指定的任何关键字参数赋予新值的成员除外。该属性Response.meta是默认复制。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;urljoin（url ）&lt;/strong&gt;
通过将响应url与可能的相对URL 组合构造绝对url。&lt;/p&gt;

&lt;p&gt;这是一个包装在&lt;a href=&quot;https://docs.python.org/2/library/urlparse.html#urlparse.urljoin&quot;&gt;urlparse.urljoin&lt;/a&gt;，它只是一个别名，使这个调用：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;urlparse.urljoin(response.url, url)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;响应子类&quot;&gt;响应子类&lt;/h3&gt;

&lt;p&gt;这里是可用的内置Response子类的列表。您还可以将Response类子类化以实现您自己的功能。&lt;/p&gt;

&lt;h4 id=&quot;textresponse对象&quot;&gt;TextResponse对象&lt;/h4&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;class scrapy.http.TextResponse(url[, encoding[, ...]])&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;TextResponse对象向基Response类添加编码能力 ，这意味着仅用于二进制数据，例如图像，声音或任何媒体文件。&lt;/p&gt;

&lt;p&gt;TextResponse对象支持一个新的构造函数参数，除了基础Response对象。其余的功能与Response类相同，这里没有记录。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;参数：    encoding（string）&lt;/strong&gt; - 是一个字符串，包含用于此响应的编码。如果你创建一个TextResponse具有unicode主体的对象，它将使用这个编码进行编码（记住body属性总是一个字符串）。如果encoding是None（默认值），则将在响应标头和正文中查找编码。
TextResponse除了标准对象之外，对象还支持以下属性Response&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;text&lt;/strong&gt;
响应体，如unicode。&lt;/p&gt;

&lt;p&gt;同样&lt;code class=&quot;highlighter-rouge&quot;&gt;response.body.decode(response.encoding)&lt;/code&gt;，但结果是在第一次调用后缓存，因此您可以访问 &lt;code class=&quot;highlighter-rouge&quot;&gt;response.text&lt;/code&gt;多次，无需额外的开销。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;
unicode(response.body)不是一个正确的方法来将响应身体转换为unicode：您将使用系统默认编码（通常为ascii）而不是响应编码。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;encoding&lt;/strong&gt;
包含此响应的编码的字符串。编码通过尝试以下机制按顺序解决：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;在构造函数编码参数中传递的编码&lt;/li&gt;
  &lt;li&gt;在Content-Type HTTP头中声明的编码。如果此编码无效（即未知），则会被忽略，并尝试下一个解析机制。&lt;/li&gt;
  &lt;li&gt;在响应主体中声明的编码。TextResponse类不提供任何特殊功能。然而， HtmlResponse和XmlResponse类做。&lt;/li&gt;
  &lt;li&gt;通过查看响应体来推断的编码。这是更脆弱的方法，但也是最后一个尝试。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;selector&lt;/strong&gt;
一个Selector使用响应为目标实例。选择器在第一次访问时被延迟实例化。&lt;/p&gt;

&lt;p&gt;TextResponse对象除了标准对象外还支持以下方法Response：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;xpath（查询）&lt;/strong&gt;
快捷方式&lt;code class=&quot;highlighter-rouge&quot;&gt;TextResponse.selector.xpath(query)&lt;/code&gt;：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;response.xpath('//p')
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;css(query)&lt;/strong&gt;
快捷方式 &lt;code class=&quot;highlighter-rouge&quot;&gt;TextResponse.selector.css(query)&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;response.css('p')
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;body_as_unicode()&lt;/strong&gt;
同样text，但可用作方法。保留此方法以实现向后兼容; 请喜欢response.text。&lt;/p&gt;

&lt;h4 id=&quot;htmlresponse对象&quot;&gt;HtmlResponse对象&lt;/h4&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;class scrapy.http.HtmlResponse（url [，... ] ）&lt;/code&gt;
本HtmlResponse类的子类，TextResponse 这增加了通过查看HTML编码自动发现支持META HTTP-EQUIV属性。见TextResponse.encoding。&lt;/p&gt;

&lt;h4 id=&quot;xmlresponse对象&quot;&gt;XmlResponse对象&lt;/h4&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;class scrapy.http.XmlResponse（url [，... ] ）&lt;/code&gt;
本XmlResponse类的子类，TextResponse这增加了通过查看XML声明线路编码自动发现支持。见TextResponse.encoding。&lt;/p&gt;</content><author><name></name></author><category term="Scrapy" /><summary type="html">Scrapy爬虫入门教程十一 Request和Response（请求和响应）</summary></entry><entry><title type="html">Scrapy爬虫入门教程十 Feed exports（导出文件）</title><link href="http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E5%8D%81-feed-exports-%E5%AF%BC%E5%87%BA%E6%96%87%E4%BB%B6/" rel="alternate" type="text/html" title="Scrapy爬虫入门教程十 Feed exports（导出文件）" /><published>2017-04-09T14:33:00+08:00</published><updated>2017-04-09T14:33:00+08:00</updated><id>http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E5%8D%81-feed-exports(%E5%AF%BC%E5%87%BA%E6%96%87%E4%BB%B6)</id><content type="html" xml:base="http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E5%8D%81-feed-exports-%E5%AF%BC%E5%87%BA%E6%96%87%E4%BB%B6/">&lt;h1 id=&quot;scrapy爬虫入门教程十-feed-exports导出文件&quot;&gt;Scrapy爬虫入门教程十 Feed exports（导出文件）&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;开发环境：&lt;/strong&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;Python 3.6.0 版本&lt;/code&gt; （当前最新）
&lt;code class=&quot;highlighter-rouge&quot;&gt;Scrapy 1.3.2 版本&lt;/code&gt; （当前最新）&lt;/p&gt;

&lt;h1 id=&quot;导出文件&quot;&gt;导出文件&lt;/h1&gt;

&lt;p&gt;新版本0.10。&lt;/p&gt;

&lt;p&gt;实现爬虫时最常需要的特征之一是能够正确地存储所过滤的数据，并且经常意味着使用被过滤的数据（通常称为“export feed”）生成要由其他系统消耗的“导出文件” 。&lt;/p&gt;

&lt;p&gt;Scrapy使用Feed导出功能即时提供此功能，这允许您使用多个序列化格式和存储后端来生成包含已抓取项目的Feed。&lt;/p&gt;

&lt;h2 id=&quot;序列化格式&quot;&gt;序列化格式&lt;/h2&gt;

&lt;p&gt;为了序列化抓取的数据，Feed导出使用项导出器。这些格式是开箱即用的：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/feed-exports.html#topics-feed-format-json&quot;&gt;JSON&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/feed-exports.html#topics-feed-format-jsonlines&quot;&gt;JSON lines&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/feed-exports.html#topics-feed-format-csv&quot;&gt;CSV&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/feed-exports.html#topics-feed-format-xml&quot;&gt;XML&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;但您也可以通过FEED_EXPORTERS设置扩展支持的格式 。&lt;/p&gt;

&lt;h3 id=&quot;json&quot;&gt;JSON&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;FEED_FORMAT： json&lt;/li&gt;
  &lt;li&gt;使用出口： JsonItemExporter&lt;/li&gt;
  &lt;li&gt;如果您对大型Feed使用JSON，请参阅&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/exporters.html#json-with-large-data&quot;&gt;此警告&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;json-lines&quot;&gt;JSON lines&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;FEED_FORMAT： jsonlines&lt;/li&gt;
  &lt;li&gt;使用出口： JsonLinesItemExporter&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;csv&quot;&gt;CSV&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;FEED_FORMAT： csv&lt;/li&gt;
  &lt;li&gt;使用出口： CsvItemExporter&lt;/li&gt;
  &lt;li&gt;指定要导出的列及其顺序使用 FEED_EXPORT_FIELDS。其他Feed导出程序也可以使用此选项，但它对CSV很重要，因为与许多其他导出格式不同，CSV使用固定标头。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;xml&quot;&gt;XML&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;FEED_FORMAT： xml&lt;/li&gt;
  &lt;li&gt;使用出口： XmlItemExporter&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pickle&quot;&gt;Pickle&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;FEED_FORMAT： pickle&lt;/li&gt;
  &lt;li&gt;使用出口： PickleItemExporter&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;marshal&quot;&gt;Marshal&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;FEED_FORMAT： marshal&lt;/li&gt;
  &lt;li&gt;使用出口： MarshalItemExporter&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;存储&quot;&gt;存储&lt;/h2&gt;

&lt;p&gt;使用Feed导出时，您可以使用&lt;a href=&quot;https://en.wikipedia.org/wiki/Uniform_Resource_Identifier&quot;&gt;URI&lt;/a&gt;（通过FEED_URI设置）定义在哪里存储Feed 。Feed导出支持由URI方案定义的多个存储后端类型。&lt;/p&gt;

&lt;p&gt;支持开箱即用的存储后端包括：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/feed-exports.html#topics-feed-storage-fs&quot;&gt;本地文件系统&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/feed-exports.html#topics-feed-storage-ftp&quot;&gt;FTP&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/feed-exports.html#topics-feed-storage-s3&quot;&gt;S3&lt;/a&gt;（需要 &lt;a href=&quot;https://github.com/boto/botocore&quot;&gt;botocore&lt;/a&gt;或 &lt;a href=&quot;https://github.com/boto/boto&quot;&gt;boto&lt;/a&gt;）&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://scrapy.readthedocs.io/en/latest/topics/feed-exports.html#topics-feed-storage-stdout&quot;&gt;标准输出&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果所需的外部库不可用，则某些存储后端可能无法使用。例如，S3后端仅在安装了&lt;a href=&quot;https://github.com/boto/botocore&quot;&gt;botocore&lt;/a&gt; 或&lt;a href=&quot;https://github.com/boto/boto&quot;&gt;boto&lt;/a&gt;库时可用（Scrapy仅支持&lt;a href=&quot;https://github.com/boto/boto&quot;&gt;boto&lt;/a&gt;到Python 2）。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;存储uri参数&quot;&gt;存储URI参数&lt;/h2&gt;

&lt;p&gt;存储URI还可以包含在创建订阅源时被替换的参数。这些参数是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;%(time)s - 在创建订阅源时由时间戳替换&lt;/li&gt;
  &lt;li&gt;%(name)s - 被蜘蛛名替换&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;任何其他命名参数将替换为同名的spider属性。例如， 在创建订阅源的那一刻，&lt;code class=&quot;highlighter-rouge&quot;&gt;%(site_id)s&lt;/code&gt;将被&lt;code class=&quot;highlighter-rouge&quot;&gt;spider.site_id&lt;/code&gt;属性替换。&lt;/p&gt;

&lt;p&gt;这里有一些例子来说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;存储在FTP中使用每个蜘蛛一个目录：&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ftp://user:password@ftp.example.com/scraping/feeds/%(name)s/%(time)s.json&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;存储在S3使用每个蜘蛛一个目录：&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;s3://mybucket/scraping/feeds/%(name)s/%(time)s.json&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;存储后端&quot;&gt;存储后端&lt;/h2&gt;

&lt;h3 id=&quot;本地文件系统&quot;&gt;本地文件系统&lt;/h3&gt;

&lt;p&gt;订阅源存储在本地文件系统中。&lt;/p&gt;

&lt;p&gt;URI方案： &lt;code class=&quot;highlighter-rouge&quot;&gt;file&lt;/code&gt;
示例URI： &lt;code class=&quot;highlighter-rouge&quot;&gt;file:///tmp/export.csv&lt;/code&gt;
所需的外部库：none
&lt;strong&gt;请注意&lt;/strong&gt;，（仅）对于本地文件系统存储，如果指定绝对路径，则可以省略该方案&lt;code class=&quot;highlighter-rouge&quot;&gt;/tmp/export.csv&lt;/code&gt;。这只适用于Unix系统。&lt;/p&gt;

&lt;h3 id=&quot;ftp&quot;&gt;FTP&lt;/h3&gt;

&lt;p&gt;订阅源存储在FTP服务器中。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;URI方案： &lt;code class=&quot;highlighter-rouge&quot;&gt;ftp&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;示例URI： &lt;code class=&quot;highlighter-rouge&quot;&gt;ftp://user:pass@ftp.example.com/&lt;/code&gt;path/to/export.csv&lt;/li&gt;
  &lt;li&gt;所需的外部库：none&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;s3&quot;&gt;S3&lt;/h3&gt;

&lt;p&gt;订阅源存储在&lt;a href=&quot;https://aws.amazon.com/s3/&quot;&gt;Amazon S3&lt;/a&gt;上。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;URI方案： &lt;code class=&quot;highlighter-rouge&quot;&gt;s3&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;示例URI：&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;s3://mybucket/path/to/export.csv&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;s3://aws_key:aws_secret@mybucket/path/to/export.csv&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;所需的外部库：botocore或boto&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;AWS凭证可以作为URI中的用户/密码传递，也可以通过以下设置传递：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;AWS_ACCESS_KEY_ID&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;AWS_SECRET_ACCESS_KEY&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;标准输出&quot;&gt;标准输出&lt;/h3&gt;

&lt;p&gt;Feed被写入Scrapy进程的标准输出。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;URI方案： &lt;code class=&quot;highlighter-rouge&quot;&gt;stdout&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;示例URI： &lt;code class=&quot;highlighter-rouge&quot;&gt;stdout:&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;所需的外部库：none&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;设置&quot;&gt;设置&lt;/h2&gt;

&lt;p&gt;这些是用于配置Feed导出的设置：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;FEED_URI （强制性）&lt;/li&gt;
  &lt;li&gt;FEED_FORMAT&lt;/li&gt;
  &lt;li&gt;FEED_STORAGES&lt;/li&gt;
  &lt;li&gt;FEED_EXPORTERS&lt;/li&gt;
  &lt;li&gt;FEED_STORE_EMPTY&lt;/li&gt;
  &lt;li&gt;FEED_EXPORT_ENCODING&lt;/li&gt;
  &lt;li&gt;FEED_EXPORT_FIELDS&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;feed_uri&quot;&gt;FEED_URI&lt;/h3&gt;

&lt;p&gt;默认： None&lt;/p&gt;

&lt;p&gt;导出Feed的URI。请参阅支持的URI方案的存储后端。&lt;/p&gt;

&lt;p&gt;启用Feed导出时需要此设置。&lt;/p&gt;

&lt;h3 id=&quot;feed_format&quot;&gt;FEED_FORMAT&lt;/h3&gt;

&lt;p&gt;要用于Feed的序列化格式。有关可能的值，请参阅 序列化格式。&lt;/p&gt;

&lt;h3 id=&quot;feed_export_encoding&quot;&gt;FEED_EXPORT_ENCODING&lt;/h3&gt;

&lt;p&gt;默认： None&lt;/p&gt;

&lt;p&gt;要用于Feed的编码。&lt;/p&gt;

&lt;p&gt;如果取消设置或设置为None（默认），它使用UTF-8除了JSON输出，\uXXXX由于历史原因使用安全的数字编码（序列）。&lt;/p&gt;

&lt;p&gt;使用utf-8，如果你想UTF-8 JSON了。&lt;/p&gt;

&lt;h3 id=&quot;feed_export_fields&quot;&gt;FEED_EXPORT_FIELDS&lt;/h3&gt;

&lt;p&gt;默认： None&lt;/p&gt;

&lt;p&gt;要导出的字段的列表，可选。示例：。FEED_EXPORT_FIELDS = [“foo”, “bar”, “baz”]&lt;/p&gt;

&lt;p&gt;使用FEED_EXPORT_FIELDS选项定义要导出的字段及其顺序。&lt;/p&gt;

&lt;p&gt;当FEED_EXPORT_FIELDS为空或无（默认）时，Scrapy使用在Item蜘蛛正在产生的dicts 或子类中定义的字段。&lt;/p&gt;

&lt;p&gt;如果导出器需要一组固定的字段（CSV导出格式为这种情况 ），并且FEED_EXPORT_FIELDS为空或无，则Scrapy会尝试从导出的​​数据中推断字段名称 - 当前它使用第一个项目中的字段名称。&lt;/p&gt;

&lt;h3 id=&quot;feed_store_empty&quot;&gt;FEED_STORE_EMPTY&lt;/h3&gt;

&lt;p&gt;默认： False&lt;/p&gt;

&lt;p&gt;是否导出空Feed（即，没有项目的Feed）。&lt;/p&gt;

&lt;p&gt;FEED_STORAGES
默认： {}&lt;/p&gt;

&lt;p&gt;包含您的项目支持的其他Feed存储后端的字典。键是URI方案，值是存储类的路径。&lt;/p&gt;

&lt;h3 id=&quot;feed_storages_base&quot;&gt;FEED_STORAGES_BASE&lt;/h3&gt;

&lt;p&gt;默认：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'scrapy.extensions.feedexport.FileFeedStorage',&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'file':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'scrapy.extensions.feedexport.FileFeedStorage',&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'stdout':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'scrapy.extensions.feedexport.StdoutFeedStorage',&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'s3':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'scrapy.extensions.feedexport.S3FeedStorage',&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'ftp':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'scrapy.extensions.feedexport.FTPFeedStorage',&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;包含Scrapy支持的内置Feed存储后端的字典。您可以通过分配其中None的URI方案 来禁用这些后端FEED_STORAGES。例如，要禁用内置FTP存储后端（无替换），请将其放置在settings.py：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FEED_STORAGES = {
    'ftp': None,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;feed_exporters&quot;&gt;FEED_EXPORTERS&lt;/h3&gt;

&lt;p&gt;默认： {}&lt;/p&gt;

&lt;p&gt;包含您的项目支持的其他导出器的字典。键是序列化格式，值是Item exporter类的路径。&lt;/p&gt;

&lt;h3 id=&quot;feed_exporters_base&quot;&gt;FEED_EXPORTERS_BASE&lt;/h3&gt;

&lt;p&gt;默认：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'json':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'scrapy.exporters.JsonItemExporter',&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'jsonlines':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'scrapy.exporters.JsonLinesItemExporter',&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'jl':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'scrapy.exporters.JsonLinesItemExporter',&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'csv':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'scrapy.exporters.CsvItemExporter',&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'xml':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'scrapy.exporters.XmlItemExporter',&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'marshal':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'scrapy.exporters.MarshalItemExporter',&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'pickle':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'scrapy.exporters.PickleItemExporter',&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;一个包含Scrapy支持的内置feed导出器的dict。您可以通过分配其中None的序列化格式来禁用任何这些导出器FEED_EXPORTERS。例如，要禁用内置的CSV导出器（无替换），请将其放置在settings.py：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FEED_EXPORTERS = {
    'csv': None,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;</content><author><name></name></author><category term="Scrapy" /><summary type="html">Scrapy爬虫入门教程十 Feed exports（导出文件）</summary></entry></feed>