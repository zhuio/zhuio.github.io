<!DOCTYPE html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]--><!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8"><![endif]--><!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9"><![endif]--><!--[if gt IE 8]><!--><html class="no-js">
<!--<![endif]--> <head> <meta charset="UTF-8"> <meta content="text/html; charset=UTF-8" http-equiv="Content-Type"> <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"> <title>Scrapy爬虫入门教程十一 Request和Response（请求和响应） – 朱智博在Github上的Blog</title> <meta name="description" content="朱智博，朱智博的博客，zhuio,zhuio.github.io,"> <meta name="keywords" content="Scrapy"> <!-- Twitter Cards --> <meta name="twitter:card" content="summary"> <meta name="twitter:image" content="http://localhost:4000/assets/img/logo.png"> <meta name="twitter:title" content="Scrapy爬虫入门教程十一 Request和Response（请求和响应）"> <meta name="twitter:description" content="Scrapy爬虫入门教程十一 Request和Response（请求和响应）"> <!-- Open Graph --> <meta property="og:locale" content="zh_CN"> <meta property="og:type" content="article"> <meta property="og:title" content="Scrapy爬虫入门教程十一 Request和Response（请求和响应）"> <meta property="og:description" content="Scrapy爬虫入门教程十一 Request和Response（请求和响应）"> <meta property="og:url" content="http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E5%8D%81%E4%B8%80-request%E5%92%8Cresponse-%E8%AF%B7%E6%B1%82%E5%92%8C%E5%93%8D%E5%BA%94/"> <meta property="og:site_name" content="朱智博在Github上的Blog"> <meta property="og:image" content="http://localhost:4000/assets/img/logo.png"> <link rel="canonical" href="http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E5%8D%81%E4%B8%80-request%E5%92%8Cresponse-%E8%AF%B7%E6%B1%82%E5%92%8C%E5%93%8D%E5%BA%94/"> <link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="朱智博在Github上的Blog Feed"> <!-- Handheld --> <meta name="HandheldFriendly" content="True"> <meta name="MobileOptimized" content="320"> <meta name="viewport" content="width=device-width, initial-scale=1.0"> <!-- CSS --> <link rel="stylesheet" href="http://localhost:4000/assets/css/main.css"> <!-- JS --> <script src="http://localhost:4000/assets/js/modernizr-3.3.1.custom.min.js"></script> <!-- Favicons --> <link rel="apple-touch-icon" href="http://localhost:4000/assets/img/favicons/apple-icon-precomposed.png"> <link rel="apple-touch-icon" sizes="72x72" href="http://localhost:4000/assets/img/favicons/apple-icon-72x72.png"> <link rel="apple-touch-icon" sizes="114x114" href="http://localhost:4000/assets/img/favicons/apple-icon-114x114.png"> <link rel="apple-touch-icon" sizes="144x144" href="http://localhost:4000/assets/img/favicons/apple-icon-144x144.png"> <link rel="shortcut icon" type="image/png" href="http://localhost:4000/favicon.png"> <link rel="shortcut icon" href="http://localhost:4000/favicon.ico"> <!-- Background Image --> <style type="text/css">body {background-image:url(http://localhost:4000/assets/img/placeholder-big.jpg); background-repeat: no-repeat; background-size: cover; }</style> <!-- Post Feature Image --> </head> <body> <nav id="dl-menu" class="dl-menuwrapper" role="navigation"> <button class="dl-trigger">Open Menu</button> <ul class="dl-menu"> <li><a href="http://localhost:4000/">Home</a></li> <li> <a href="#">About</a> <ul class="dl-submenu"> <li> <img src="http://localhost:4000/assets/img/logo.png" alt="朱智博在Github上的Blog photo" class="author-photo"> <h4>朱智博在Github上的Blog</h4> <p>朱智博，朱智博的博客，zhuio,zhuio.github.io,</p> </li> <li><a href="http://localhost:4000/about/"><span class="btn btn-inverse">Learn More</span></a></li> <li> <a href="mailto:185560083@qq.com" target="_blank" rel="noopener noreferrer"><i class="fa fa-fw fa-envelope-square"></i> Email</a> </li> <li> <a href="http://github.com/zhuio" target="_blank" rel="noopener noreferrer"><i class="fa fa-fw fa-github"></i> Github</a> </li> <li> <a href="http://www.weibo.com/EDM_LOVER" target="_blank" rel="noopener noreferrer"><i class="fa fa-fw fa-weibo"></i> Weibo</a> </li> </ul>
<!-- /.dl-submenu --> </li> <li> <a href="#">Posts</a> <ul class="dl-submenu"> <li><a href="http://localhost:4000/posts/">All Posts</a></li> <li><a href="http://localhost:4000/tags/">All Tags</a></li> </ul> </li> <li><a href="http://localhost:4000/projects/">Projects</a></li> </ul>
<!-- /.dl-menu --> </nav><!-- /.dl-menuwrapper --> <!-- Header --> <header class="header" role="banner"> <div class="wrapper animated fadeIn"> <div class="content"> <div class="post-title "> <h1>Scrapy爬虫入门教程十一 Request和Response（请求和响应）</h1> <h4>09 Apr 2017</h4> <p class="reading-time"> <i class="fa fa-clock-o"></i> Reading time ~3 minutes </p>
<!-- /.entry-reading-time --> <a class="btn zoombtn" href="http://localhost:4000/posts/"> <i class="fa fa-chevron-left"></i> </a> </div> <h1 id="scrapy爬虫入门教程十一-request和response请求和响应">Scrapy爬虫入门教程十一 Request和Response（请求和响应）</h1> <p><strong>开发环境：</strong> <code class="highlighter-rouge">Python 3.6.0 版本</code> （当前最新） <code class="highlighter-rouge">Scrapy 1.3.2 版本</code> （当前最新）</p> <h1 id="请求和响应">请求和响应</h1> <p>Scrapy的Request和Response对象用于爬网网站。</p> <p>通常，Request对象在爬虫程序中生成并传递到系统，直到它们到达下载程序，后者执行请求并返回一个Response对象，该对象返回到发出请求的爬虫程序。</p> <blockquote> <p>上面一段话比较拗口，有web经验的同学，应该都了解的，不明白看下面的图大概理解下。</p> </blockquote> <div class="highlighter-rouge"><pre class="highlight"><code>爬虫-&gt;Request:创建
Request-&gt;Response:获取下载数据
Response-&gt;爬虫:数据
</code></pre></div> <p><img src="http://upload-images.jianshu.io/upload_images/2880699-f6847c1d6ebf140a.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p> <p>两个类Request和Response类都有一些子类，它们添加基类中不需要的功能。这些在下面的请求子类和 响应子类中描述。</p> <hr> <h2 id="request-objects">Request objects</h2> <div class="highlighter-rouge"><pre class="highlight"><code>class scrapy.http.Request(url[, callback, method='GET', headers, body, cookies, meta, encoding='utf-8', priority=0, dont_filter=False, errback])
</code></pre></div> <p>一个Request对象表示一个HTTP请求，它通常是在爬虫生成，并由下载执行，从而生成Response。</p> <p>- 参数：</p> <ul> <li>
<code class="highlighter-rouge">url（string）</code> - 此请求的网址</li> <li>
<code class="highlighter-rouge">callback（callable）</code> - 将使用此请求的响应（一旦下载）作为其第一个参数调用的函数。有关更多信息，请参阅下面的<a href="https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-response-ref-request-callback-arguments">将附加数据传递给回调函数</a>。如果请求没有指定回调，parse()将使用spider的 方法。请注意，如果在处理期间引发异常，则会调用errback。</li> <li>
<code class="highlighter-rouge">method（string）</code> - 此请求的HTTP方法。默认为’GET’。</li> <li>
<code class="highlighter-rouge">meta（dict）</code> - 属性的初始值Request.meta。如果给定，在此参数中传递的dict将被浅复制。</li> <li>
<code class="highlighter-rouge">body（str或unicode）</code> - 请求体。如果unicode传递了a，那么它被编码为 str使用传递的编码（默认为utf-8）。如果 body没有给出，则存储一个空字符串。不管这个参数的类型，存储的最终值将是一个str（不会是unicode或None）。</li> <li> <h2 id="headersdict---这个请求的头dict值可以是字符串对于单值标头或列表对于多值标头如果-none作为值传递则不会发送http头">
<code class="highlighter-rouge">headers（dict）</code> - 这个请求的头。dict值可以是字符串（对于单值标头）或列表（对于多值标头）。如果 None作为值传递，则不会发送HTTP头。</h2> <p><code class="highlighter-rouge">cookie（dict或list）</code> - 请求cookie。这些可以以两种形式发送。</p> </li> <li> <p>使用dict：</p> <p>request_with_cookies = Request(url=”http://www.example.com”, cookies={‘currency’: ‘USD’, ‘country’: ‘UY’})</p> <div class="highlighter-rouge"><pre class="highlight"><code>  * 使用列表：

  ```
  request_with_cookies = Request(url="http://www.example.com",
                                 cookies=[{'name': 'currency',
                                          'value': 'USD',
                                          'domain': 'example.com',
                                          'path': '/currency'}])
  ```
</code></pre></div> </li> </ul> <p>后一种形式允许定制 cookie的属性domain和path属性。这只有在保存Cookie用于以后的请求时才有用。</p> <p>当某些网站返回Cookie（在响应中）时，这些Cookie会存储在该域的Cookie中，并在将来的请求中再次发送。这是任何常规网络浏览器的典型行为。但是，如果由于某种原因，您想要避免与现有Cookie合并，您可以通过将dont_merge_cookies关键字设置为True 来指示Scrapy如此操作 Request.meta。</p> <p>不合并Cookie的请求示例：</p> <div class="highlighter-rouge"><pre class="highlight"><code>request_with_cookies = Request(url="http://www.example.com",
                               cookies={'currency': 'USD', 'country': 'UY'},
                               meta={'dont_merge_cookies': True})
</code></pre></div> <p>有关详细信息，请参阅<a href="https://doc.scrapy.org/en/1.3/topics/downloader-middleware.html#cookies-mw">CookiesMiddleware</a>。</p> <ul> <li>
<code class="highlighter-rouge">encoding（string）</code> - 此请求的编码（默认为’utf-8’）。此编码将用于对URL进行百分比编码，并将正文转换为str（如果给定unicode）。</li> <li>
<code class="highlighter-rouge">priority（int）</code> - 此请求的优先级（默认为0）。调度器使用优先级来定义用于处理请求的顺序。具有较高优先级值的请求将较早执行。允许负值以指示相对低优先级。</li> <li> <h2 id="dont_filterboolean---表示此请求不应由调度程序过滤当您想要多次执行相同的请求时忽略重复过滤器时使用小心使用它或者你会进入爬行循环默认为false">
<code class="highlighter-rouge">dont_filter（boolean）</code> - 表示此请求不应由调度程序过滤。当您想要多次执行相同的请求时忽略重复过滤器时使用。小心使用它，或者你会进入爬行循环。默认为False。</h2> <p><code class="highlighter-rouge">errback（callable）</code> - 如果在处理请求时引发任何异常，将调用的函数。这包括失败的404 HTTP错误等页面。它接收一个<a href="https://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html">Twisted Failure</a>实例作为第一个参数。有关更多信息，请参阅<a href="https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-response-ref-errbacks">使用errbacks在请求处理</a>中<a href="https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-response-ref-errbacks">捕获异常</a>。</p> </li> </ul> <p>- <code class="highlighter-rouge">url</code> 包含此请求的网址的字符串。请记住，此属性包含转义的网址，因此它可能与构造函数中传递的网址不同。</p> <p>此属性为只读。更改请求使用的URL <code class="highlighter-rouge">replace()</code>。</p> <p>- <code class="highlighter-rouge">method</code> 表示请求中的HTTP方法的字符串。这保证是大写的。例如：<code class="highlighter-rouge">"GET"，"POST"，"PUT"</code>等</p> <p>- <code class="highlighter-rouge">headers</code> 包含请求标头的类似字典的对象。</p> <p>- <code class="highlighter-rouge">body</code> 包含请求正文的str。</p> <p>此属性为只读。更改请求使用的正文 <code class="highlighter-rouge">replace()</code>。</p> <ul> <li>
<code class="highlighter-rouge">meta</code> 包含此请求的任意元数据的字典。此dict对于新请求为空，通常由不同的Scrapy组件（扩展程序，中间件等）填充。因此，此dict中包含的数据取决于您启用的扩展。</li> </ul> <p>有关<a href="https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-meta">Scrapy识别</a>的特殊元键列表，请参阅<a href="https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-meta">Request.meta特殊键</a>。</p> <p>当使用or 方法克隆请求时，此dict是<a href="https://docs.python.org/2/library/copy.html">浅复制</a>的 ，并且也可以在您的爬虫中从属性访问。copy()replace()response.meta</p> <p>- <code class="highlighter-rouge">copy（）</code> 返回一个新的请求，它是这个请求的副本。另请参见： <a href="https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-response-ref-request-callback-arguments">将附加数据传递到回调函数</a>。</p> <p>- <code class="highlighter-rouge">replace([url, method, headers, body, cookies, meta, encoding, dont_filter, callback, errback])</code> 返回具有相同成员的Request对象，但通过指定的任何关键字参数赋予新值的成员除外。该属性Request.meta是默认复制（除非新的值在给定的meta参数）。另请参见 <a href="https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-response-ref-request-callback-arguments">将附加数据传递给回调函数</a>。</p> <h3 id="将附加数据传递给回调函数">将附加数据传递给回调函数</h3> <p>请求的回调是当下载该请求的响应时将被调用的函数。将使用下载的Response对象作为其第一个参数来调用回调函数。</p> <p>例：</p> <div class="highlighter-rouge"><pre class="highlight"><code>def parse_page1(self, response):
    return scrapy.Request("http://www.example.com/some_page.html",
                          callback=self.parse_page2)

def parse_page2(self, response):
    # this would log http://www.example.com/some_page.html
    self.logger.info("Visited %s", response.url)
</code></pre></div> <p>在某些情况下，您可能有兴趣向这些回调函数传递参数，以便稍后在第二个回调中接收参数。您可以使用该<code class="highlighter-rouge">Request.meta</code>属性。</p> <p>以下是使用此机制传递项目以填充来自不同页面的不同字段的示例：</p> <div class="highlighter-rouge"><pre class="highlight"><code>def parse_page1(self, response):
    item = MyItem()
    item['main_url'] = response.url
    request = scrapy.Request("http://www.example.com/some_page.html",
                             callback=self.parse_page2)
    request.meta['item'] = item
    yield request

def parse_page2(self, response):
    item = response.meta['item']
    item['other_url'] = response.url
    yield item
</code></pre></div> <h3 id="使用errbacks在请求处理中捕获异常">使用errbacks在请求处理中捕获异常</h3> <p>请求的errback是在处理异常时被调用的函数。</p> <p>它接收一个<a href="https://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html">Twisted Failure</a>实例作为第一个参数，并可用于跟踪连接建立超时，DNS错误等。</p> <p>这里有一个示例爬虫记录所有错误，并捕获一些特定的错误，如果需要：</p> <div class="highlighter-rouge"><pre class="highlight"><code>import scrapy

from scrapy.spidermiddlewares.httperror import HttpError
from twisted.internet.error import DNSLookupError
from twisted.internet.error import TimeoutError, TCPTimedOutError

class ErrbackSpider(scrapy.Spider):
    name = "errback_example"
    start_urls = [
        "http://www.httpbin.org/",              # HTTP 200 expected
        "http://www.httpbin.org/status/404",    # Not found error
        "http://www.httpbin.org/status/500",    # server issue
        "http://www.httpbin.org:12345/",        # non-responding host, timeout expected
        "http://www.httphttpbinbin.org/",       # DNS error expected
    ]

    def start_requests(self):
        for u in self.start_urls:
            yield scrapy.Request(u, callback=self.parse_httpbin,
                                    errback=self.errback_httpbin,
                                    dont_filter=True)

    def parse_httpbin(self, response):
        self.logger.info('Got successful response from {}'.format(response.url))
        # do something useful here...

    def errback_httpbin(self, failure):
        # log all failures
        self.logger.error(repr(failure))

        # in case you want to do something special for some errors,
        # you may need the failure's type:

        if failure.check(HttpError):
            # these exceptions come from HttpError spider middleware
            # you can get the non-200 response
            response = failure.value.response
            self.logger.error('HttpError on %s', response.url)

        elif failure.check(DNSLookupError):
            # this is the original request
            request = failure.request
            self.logger.error('DNSLookupError on %s', request.url)

        elif failure.check(TimeoutError, TCPTimedOutError):
            request = failure.request
            self.logger.error('TimeoutError on %s', request.url)
</code></pre></div> <hr> <h2 id="requestmeta特殊键">Request.meta特殊键</h2> <p>该<code class="highlighter-rouge">Request.meta</code>属性可以包含任何任意数据，但有一些特殊的键由Scrapy及其内置扩展识别。</p> <p>那些是：</p> <div class="highlighter-rouge"><pre class="highlight"><code>dont_redirect
dont_retry
handle_httpstatus_list
handle_httpstatus_all
dont_merge_cookies（参见cookies构造函数的Request参数）
cookiejar
dont_cache
redirect_urls
bindaddress
dont_obey_robotstxt
download_timeout
download_maxsize
download_latency
proxy
</code></pre></div> <h4 id="bindaddress">bindaddress</h4> <p>用于执行请求的出站IP地址的IP。</p> <h4 id="download_timeout">download_timeout</h4> <p>下载器在超时前等待的时间量（以秒为单位）。参见：<code class="highlighter-rouge">DOWNLOAD_TIMEOUT</code>。</p> <h4 id="download_latency">download_latency</h4> <p>自请求已启动以来，用于获取响应的时间量，即通过网络发送的HTTP消息。此元键仅在响应已下载时可用。虽然大多数其他元键用于控制Scrapy行为，但这应该是只读的。</p> <h2 id="请求子类">请求子类</h2> <p>这里是内置子类的Request列表。您还可以将其子类化以实现您自己的自定义功能。</p> <p>FormRequest对象 FormRequest类扩展了Request具有处理HTML表单的功能的基础。它使用lxml.html表单 从Response对象的表单数据预填充表单字段。</p> <p><code class="highlighter-rouge">class scrapy.http.FormRequest(url[, formdata, ...])</code></p> <p>本<code class="highlighter-rouge">FormRequest</code>类增加了新的构造函数的参数。其余的参数与<code class="highlighter-rouge">Request</code>类相同，这里没有记录。</p> <ul> <li> <p>参数：<strong>formdata</strong>（元组的dict或iterable） - 是一个包含HTML Form数据的字典（或（key，value）元组的迭代），它将被url编码并分配给请求的主体。 该<code class="highlighter-rouge">FormRequest</code>对象支持除标准以下类方法Request的方法：</p> <p>classmethod from_response(response[, formname=None, formid=None, formnumber=0, formdata=None, formxpath=None, formcss=None, clickdata=None, dont_click=False, …])</p> </li> </ul> <p>返回一个新<code class="highlighter-rouge">FormRequest</code>对象，其中的表单字段值已预先<code class="highlighter-rouge">&lt;form&gt;</code>填充在给定响应中包含的HTML 元素中。有关示例，请参阅 <a href="https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-response-ref-request-userlogin">使用FormRequest.from_response（）来模拟用户登录</a>。</p> <p>该策略是在任何可查看的表单控件上默认自动模拟点击，如a 。即使这是相当方便，并且经常想要的行为，有时它可能导致难以调试的问题。例如，当使用使用javascript填充和/或提交的表单时，默认行为可能不是最合适的。要禁用此行为，您可以将参数设置 为。此外，如果要更改单击的控件（而不是禁用它），您还可以使用 参数。<code class="highlighter-rouge">&lt;input type="submit"&gt; from_response() dont_click True clickdata</code></p> <p><strong>参数：</strong></p> <ul> <li>response（Responseobject） - 包含将用于预填充表单字段的HTML表单的响应</li> <li>formname（string） - 如果给定，将使用name属性设置为此值的形式。</li> <li>formid（string） - 如果给定，将使用id属性设置为此值的形式。</li> <li>formxpath（string） - 如果给定，将使用匹配xpath的第一个表单。</li> <li>formcss（string） - 如果给定，将使用匹配css选择器的第一个形式。</li> <li>formnumber（integer） - 当响应包含多个表单时要使用的表单的数量。第一个（也是默认）是0。</li> <li>formdata（dict） - 要在表单数据中覆盖的字段。如果响应&lt;form&gt;元素中已存在字段，则其值将被在此参数中传递的值覆盖。</li> <li>clickdata（dict） - 查找控件被点击的属性。如果没有提供，表单数据将被提交，模拟第一个可点击元素的点击。除了html属性，控件可以通过其相对于表单中其他提交表输入的基于零的索引，通过nr属性来标识。</li> <li>dont_click（boolean） - 如果为True，表单数据将在不点击任何元素的情况下提交。</li> </ul> <blockquote> <p>这个类方法的其他参数直接传递给 FormRequest构造函数。 在新版本0.10.3：该formname参数。 在新版本0.17：该formxpath参数。 新的版本1.1.0：该formcss参数。 新的版本1.1.0：该formid参数。</p> </blockquote> <hr> <h3 id="请求使用示例">请求使用示例</h3> <h4 id="使用formrequest通过http-post发送数据">使用FormRequest通过HTTP POST发送数据</h4> <p>如果你想在你的爬虫中模拟HTML表单POST并发送几个键值字段，你可以返回一个FormRequest对象（从你的爬虫）像这样：</p> <div class="highlighter-rouge"><pre class="highlight"><code>return [FormRequest(url="http://www.example.com/post/action",
                    formdata={'name': 'John Doe', 'age': '27'},
                    callback=self.after_post)]
</code></pre></div> <h4 id="使用formrequestfrom_response来模拟用户登录">使用FormRequest.from_response（）来模拟用户登录</h4> <p>网站通常通过元素（例如会话相关数据或认证令牌（用于登录页面））提供预填充的表单字段。进行剪贴时，您需要自动预填充这些字段，并且只覆盖其中的一些，例如用户名和密码。您可以使用 此作业的方法。这里有一个使用它的爬虫示例：<code class="highlighter-rouge">&lt;input type="hidden"&gt; FormRequest.from_response()</code></p> <div class="highlighter-rouge"><pre class="highlight"><code>import scrapy

class LoginSpider(scrapy.Spider):
    name = 'example.com'
    start_urls = ['http://www.example.com/users/login.php']

    def parse(self, response):
        return scrapy.FormRequest.from_response(
            response,
            formdata={'username': 'john', 'password': 'secret'},
            callback=self.after_login
        )

    def after_login(self, response):
        # check login succeed before going on
        if "authentication failed" in response.body:
            self.logger.error("Login failed")
            return

        # continue scraping with authenticated session...
</code></pre></div> <h1 id="响应对象">响应对象</h1> <p><code class="highlighter-rouge">class scrapy.http.Response(url[, status=200, headers=None, body=b'', flags=None, request=None])</code> 一个Response对象表示的HTTP响应，这通常是下载（由下载），并供给到爬虫进行处理。</p> <p><strong>参数：</strong></p> <ul> <li>url（string） - 此响应的URL</li> <li>status（integer） - 响应的HTTP状态。默认为<strong>200</strong>。</li> <li>headers（dict） - 这个响应的头。dict值可以是字符串（对于单值标头）或列表（对于多值标头）。</li> <li>body（str） - 响应体。它必须是str，而不是unicode，除非你使用一个编码感知<a href="https://doc.scrapy.org/en/1.3/topics/request-response.html#topics-request-response-ref-response-subclasses">响应子类</a>，如 <code class="highlighter-rouge">TextResponse</code>。</li> <li>flags（<a href="https://doc.scrapy.org/en/1.3/topics/api.html#scrapy.loader.SpiderLoader.list">list</a>） - 是一个包含属性初始值的 <code class="highlighter-rouge">Response.flags</code>列表。如果给定，列表将被浅复制。</li> <li>request（Requestobject） - 属性的初始值<code class="highlighter-rouge">Response.request</code>。这代表Request生成此响应。</li> </ul> <p><strong>url</strong> 包含响应的URL的字符串。</p> <p>此属性为只读。更改响应使用的URL replace()。</p> <p><strong>status</strong> 表示响应的HTTP状态的整数。示例：200， 404。</p> <p><strong>headers</strong> 包含响应标题的类字典对象。可以使用get()返回具有指定名称的第一个标头值或getlist()返回具有指定名称的所有标头值来访问值。例如，此调用会为您提供标题中的所有Cookie：</p> <div class="highlighter-rouge"><pre class="highlight"><code>response.headers.getlist('Set-Cookie')
</code></pre></div> <p><strong>body</strong> 本回复的正文。记住Response.body总是一个字节对象。如果你想unicode版本使用 TextResponse.text（只在TextResponse 和子类中可用）。</p> <p>此属性为只读。更改响应使用的主体 replace()。</p> <p><strong>request</strong> Request生成此响应的对象。在响应和请求通过所有<a href="https://doc.scrapy.org/en/1.3/topics/downloader-middleware.html#topics-downloader-middleware">下载中间件</a>后，此属性在Scrapy引擎中分配。特别地，这意味着：</p> <p>HTTP重定向将导致将原始请求（重定向之前的URL）分配给重定向响应（重定向后具有最终URL）。 Response.request.url并不总是等于Response.url 此属性仅在爬虫程序代码和 <a href="https://doc.scrapy.org/en/1.3/topics/spider-middleware.html#topics-spider-middleware">Spider Middleware</a>中可用，但不能在Downloader Middleware中使用（尽管您有通过其他方式可用的请求）和处理程序response_downloaded。</p> <p><strong>meta</strong> 的快捷方式Request.meta的属性 Response.request对象（即self.request.meta）。</p> <p>与Response.request属性不同，Response.meta 属性沿重定向和重试传播，因此您将获得Request.meta从您的爬虫发送的原始属性。</p> <p>也可以看看</p> <p>Request.meta 属性</p> <p><strong>flags</strong> 包含此响应的标志的列表。标志是用于标记响应的标签。例如：’cached’，’redirected ‘等等。它们显示在Response（** str** 方法）的字符串表示上，它被引擎用于日志记录。</p> <p><strong>copy（）</strong> 返回一个新的响应，它是此响应的副本。</p> <p><strong>replace（[ url，status，headers，body，request，flags，cls ] ）</strong> 返回具有相同成员的Response对象，但通过指定的任何关键字参数赋予新值的成员除外。该属性Response.meta是默认复制。</p> <p><strong>urljoin（url ）</strong> 通过将响应url与可能的相对URL 组合构造绝对url。</p> <p>这是一个包装在<a href="https://docs.python.org/2/library/urlparse.html#urlparse.urljoin">urlparse.urljoin</a>，它只是一个别名，使这个调用：</p> <div class="highlighter-rouge"><pre class="highlight"><code>urlparse.urljoin(response.url, url)
</code></pre></div> <h3 id="响应子类">响应子类</h3> <p>这里是可用的内置Response子类的列表。您还可以将Response类子类化以实现您自己的功能。</p> <h4 id="textresponse对象">TextResponse对象</h4> <p><code class="highlighter-rouge">class scrapy.http.TextResponse(url[, encoding[, ...]])</code></p> <p>TextResponse对象向基Response类添加编码能力 ，这意味着仅用于二进制数据，例如图像，声音或任何媒体文件。</p> <p>TextResponse对象支持一个新的构造函数参数，除了基础Response对象。其余的功能与Response类相同，这里没有记录。</p> <p><strong>参数： encoding（string）</strong> - 是一个字符串，包含用于此响应的编码。如果你创建一个TextResponse具有unicode主体的对象，它将使用这个编码进行编码（记住body属性总是一个字符串）。如果encoding是None（默认值），则将在响应标头和正文中查找编码。 TextResponse除了标准对象之外，对象还支持以下属性Response</p> <p><strong>text</strong> 响应体，如unicode。</p> <p>同样<code class="highlighter-rouge">response.body.decode(response.encoding)</code>，但结果是在第一次调用后缓存，因此您可以访问 <code class="highlighter-rouge">response.text</code>多次，无需额外的开销。</p> <blockquote> <p><strong>注意</strong> unicode(response.body)不是一个正确的方法来将响应身体转换为unicode：您将使用系统默认编码（通常为ascii）而不是响应编码。</p> </blockquote> <p><strong>encoding</strong> 包含此响应的编码的字符串。编码通过尝试以下机制按顺序解决：</p> <ol> <li>在构造函数编码参数中传递的编码</li> <li>在Content-Type HTTP头中声明的编码。如果此编码无效（即未知），则会被忽略，并尝试下一个解析机制。</li> <li>在响应主体中声明的编码。TextResponse类不提供任何特殊功能。然而， HtmlResponse和XmlResponse类做。</li> <li>通过查看响应体来推断的编码。这是更脆弱的方法，但也是最后一个尝试。</li> </ol> <p><strong>selector</strong> 一个Selector使用响应为目标实例。选择器在第一次访问时被延迟实例化。</p> <p>TextResponse对象除了标准对象外还支持以下方法Response：</p> <p><strong>xpath（查询）</strong> 快捷方式<code class="highlighter-rouge">TextResponse.selector.xpath(query)</code>：</p> <div class="highlighter-rouge"><pre class="highlight"><code>response.xpath('//p')
</code></pre></div> <p><strong>css(query)</strong> 快捷方式 <code class="highlighter-rouge">TextResponse.selector.css(query)</code>:</p> <div class="highlighter-rouge"><pre class="highlight"><code>response.css('p')
</code></pre></div> <p><strong>body_as_unicode()</strong> 同样text，但可用作方法。保留此方法以实现向后兼容; 请喜欢response.text。</p> <h4 id="htmlresponse对象">HtmlResponse对象</h4> <p><code class="highlighter-rouge">class scrapy.http.HtmlResponse（url [，... ] ）</code> 本HtmlResponse类的子类，TextResponse 这增加了通过查看HTML编码自动发现支持META HTTP-EQUIV属性。见TextResponse.encoding。</p> <h4 id="xmlresponse对象">XmlResponse对象</h4> <p><code class="highlighter-rouge">class scrapy.http.XmlResponse（url [，... ] ）</code> 本XmlResponse类的子类，TextResponse这增加了通过查看XML声明线路编码自动发现支持。见TextResponse.encoding。</p> <div class="entry-meta"> <br> <hr> <span class="entry-tags"><a href="http://localhost:4000/tags/#Scrapy" title="Pages tagged Scrapy" class="tag"><span class="term">Scrapy</span></a></span> <span class="social-share"> <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E5%8D%81%E4%B8%80-request%E5%92%8Cresponse-%E8%AF%B7%E6%B1%82%E5%92%8C%E5%93%8D%E5%BA%94/" title="Share on Facebook" class="tag"> <span class="term"><i class="fa fa-facebook-square"></i> Share</span> </a> <a href="https://twitter.com/intent/tweet?text=http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E5%8D%81%E4%B8%80-request%E5%92%8Cresponse-%E8%AF%B7%E6%B1%82%E5%92%8C%E5%93%8D%E5%BA%94/" title="Share on Twitter" class="tag"> <span class="term"><i class="fa fa-twitter-square"></i> Tweet</span> </a> <a href="https://plus.google.com/share?url=http://localhost:4000/scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E5%8D%81%E4%B8%80-request%E5%92%8Cresponse-%E8%AF%B7%E6%B1%82%E5%92%8C%E5%93%8D%E5%BA%94/" title="Share on Google+" class="tag"> <span class="term"><i class="fa fa-google-plus-square"></i> +1</span> </a> </span> <div style="clear:both"></div> </div> </div> </div> <h101> <!-- 多说评论框 start --> <div class="ds-thread" data-thread-key="" data-title="Scrapy爬虫入门教程十一 Request和Response（请求和响应）" data-url="http://localhost:4000"></div> <!-- 多说评论框 end --> <!-- 多说公共JS代码 start (一个网页只需插入一次) --> <script type="text/javascript"> var duoshuoQuery = {short_name:"zhuio"}; (function() { var ds = document.createElement('script'); ds.type = 'text/javascript';ds.async = true; ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js'; ds.charset = 'UTF-8'; (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(ds); })(); </script> <!-- 多说公共JS代码 end --> </h101> </header> <!-- JS --> <script src="http://localhost:4000/assets/js/jquery-1.12.0.min.js"></script> <script src="http://localhost:4000/assets/js/jquery.dlmenu.min.js"></script> <script src="http://localhost:4000/assets/js/jquery.goup.min.js"></script> <script src="http://localhost:4000/assets/js/jquery.magnific-popup.min.js"></script> <script src="http://localhost:4000/assets/js/jquery.fitvid.min.js"></script> <script src="http://localhost:4000/assets/js/scripts.js"></script> <script type="text/javascript"> var disqus_shortname = 'zhuio-github-io'; (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js'; (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })(); (function () { var s = document.createElement('script'); s.async = true; s.type = 'text/javascript'; s.src = '//' + disqus_shortname + '.disqus.com/count.js'; (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s); }()); </script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a>
</noscript> <!-- MathJax --> <script async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> </body> </html>
